{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8ac2bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/storage1/shourovj/shourovj_works/Auburn/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import scipy.io as sio\n",
    "import sentencepiece\n",
    "import tiktoken\n",
    "import einops\n",
    "import wandb\n",
    "import accelerate\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29292507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['__header__', '__version__', '__globals__', 'label'])\n"
     ]
    }
   ],
   "source": [
    "data = sio.loadmat('toy_data/label.mat')\n",
    "print(data.keys())\n",
    "labels = data['label']\n",
    "# print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ea87969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !uv pip install matplotlib \n",
    "# !uv pip install seaborn \n",
    "# !uv pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299756fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f21df824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['toy_data/s_222_feature.mat', 'toy_data/s_156_feature.mat', 'toy_data/s_153_cluster_index.mat', 'toy_data/s_202_feature.mat', 'toy_data/s_396_cluster_index.mat', 'toy_data/s_193_feature.mat', 'toy_data/s_24_feature.mat', 'toy_data/s_28_feature.mat', 'toy_data/s_157_cluster_index.mat', 'toy_data/s_317_cluster_index.mat', 'toy_data/s_182_feature.mat', 'toy_data/s_481_feature.mat', 'toy_data/s_200_cluster_index.mat', 'toy_data/s_400_feature.mat', 'toy_data/s_140_feature.mat', 'toy_data/s_271_cluster_index.mat', 'toy_data/s_207_feature.mat', 'toy_data/s_387_cluster_index.mat', 'toy_data/s_430_feature.mat', 'toy_data/s_6_feature.mat', 'toy_data/s_134_feature.mat', 'toy_data/s_145_feature.mat', 'toy_data/s_126_cluster_index.mat', 'toy_data/s_66_feature.mat', 'toy_data/s_193_cluster_index.mat', 'toy_data/s_444_cluster_index.mat', 'toy_data/s_424_feature.mat', 'toy_data/s_398_cluster_index.mat', 'toy_data/s_401_feature.mat', 'toy_data/s_73_feature.mat', 'toy_data/s_358_feature.mat', 'toy_data/s_384_cluster_index.mat', 'toy_data/s_407_feature.mat', 'toy_data/s_190_feature.mat', 'toy_data/s_42_feature.mat', 'toy_data/s_497_feature.mat', 'toy_data/s_270_cluster_index.mat', 'toy_data/s_63_feature.mat', 'toy_data/s_261_feature.mat', 'toy_data/s_456_cluster_index.mat', 'toy_data/s_103_feature.mat', 'toy_data/s_118_cluster_index.mat', 'toy_data/s_10_cluster_index.mat', 'toy_data/s_116_feature.mat', 'toy_data/s_232_cluster_index.mat', 'toy_data/s_490_feature.mat', 'toy_data/s_272_cluster_index.mat', 'toy_data/s_88_cluster_index.mat', 'toy_data/s_56_feature.mat', 'toy_data/s_419_cluster_index.mat', 'toy_data/s_209_cluster_index.mat', 'toy_data/s_128_cluster_index.mat', 'toy_data/s_109_cluster_index.mat', 'toy_data/s_255_feature.mat', 'toy_data/s_206_cluster_index.mat', 'toy_data/s_459_cluster_index.mat', 'toy_data/s_490_cluster_index.mat', 'toy_data/s_493_cluster_index.mat', 'toy_data/s_144_cluster_index.mat', 'toy_data/s_110_feature.mat', 'toy_data/s_172_feature.mat', 'toy_data/s_283_cluster_index.mat', 'toy_data/s_460_cluster_index.mat', 'toy_data/s_308_feature.mat', 'toy_data/s_92_cluster_index.mat', 'toy_data/s_232_feature.mat', 'toy_data/s_2_cluster_index.mat', 'toy_data/s_370_feature.mat', 'toy_data/s_472_feature.mat', 'toy_data/s_9_cluster_index.mat', 'toy_data/s_56_cluster_index.mat', 'toy_data/s_374_cluster_index.mat', 'toy_data/s_499_cluster_index.mat', 'toy_data/s_12_feature.mat', 'toy_data/s_70_feature.mat', 'toy_data/s_208_cluster_index.mat', 'toy_data/s_406_feature.mat', 'toy_data/s_117_feature.mat', 'toy_data/s_206_feature.mat', 'toy_data/s_43_feature.mat', 'toy_data/s_44_cluster_index.mat', 'toy_data/s_27_cluster_index.mat', 'toy_data/s_234_feature.mat', 'toy_data/s_100_cluster_index.mat', 'toy_data/s_294_cluster_index.mat', 'toy_data/s_289_feature.mat', 'toy_data/s_464_cluster_index.mat', 'toy_data/s_404_feature.mat', 'toy_data/s_73_cluster_index.mat', 'toy_data/s_288_cluster_index.mat', 'toy_data/s_469_cluster_index.mat', 'toy_data/s_64_feature.mat', 'toy_data/s_443_cluster_index.mat', 'toy_data/s_320_feature.mat', 'toy_data/s_102_cluster_index.mat', 'toy_data/s_18_feature.mat', 'toy_data/s_78_cluster_index.mat', 'toy_data/s_291_feature.mat', 'toy_data/s_416_feature.mat', 'toy_data/s_437_feature.mat', 'toy_data/s_313_cluster_index.mat', 'toy_data/s_209_feature.mat', 'toy_data/s_62_feature.mat', 'toy_data/s_247_feature.mat', 'toy_data/s_340_feature.mat', 'toy_data/s_119_cluster_index.mat', 'toy_data/s_212_feature.mat', 'toy_data/s_216_cluster_index.mat', 'toy_data/s_147_cluster_index.mat', 'toy_data/s_14_feature.mat', 'toy_data/s_159_feature.mat', 'toy_data/s_20_feature.mat', 'toy_data/s_312_feature.mat', 'toy_data/s_298_feature.mat', 'toy_data/s_381_cluster_index.mat', 'toy_data/s_195_cluster_index.mat', 'toy_data/s_15_feature.mat', 'toy_data/s_345_cluster_index.mat', 'toy_data/s_207_cluster_index.mat', 'toy_data/s_72_cluster_index.mat', 'toy_data/s_147_feature.mat', 'toy_data/s_322_feature.mat', 'toy_data/s_219_cluster_index.mat', 'toy_data/s_323_feature.mat', 'toy_data/s_345_feature.mat', 'toy_data/s_192_cluster_index.mat', 'toy_data/s_424_cluster_index.mat', 'toy_data/s_304_cluster_index.mat', 'toy_data/s_221_cluster_index.mat', 'toy_data/s_344_feature.mat', 'toy_data/s_269_cluster_index.mat', 'toy_data/s_479_feature.mat', 'toy_data/s_486_cluster_index.mat', 'toy_data/s_386_cluster_index.mat', 'toy_data/s_110_cluster_index.mat', 'toy_data/s_75_feature.mat', 'toy_data/s_307_feature.mat', 'toy_data/s_123_cluster_index.mat', 'toy_data/s_441_feature.mat', 'toy_data/s_485_cluster_index.mat', 'toy_data/s_311_cluster_index.mat', 'toy_data/s_44_feature.mat', 'toy_data/s_431_cluster_index.mat', 'toy_data/s_399_feature.mat', 'toy_data/s_364_cluster_index.mat', 'toy_data/s_483_cluster_index.mat', 'toy_data/s_452_feature.mat', 'toy_data/s_402_cluster_index.mat', 'toy_data/s_351_cluster_index.mat', 'toy_data/s_334_cluster_index.mat', 'toy_data/s_396_feature.mat', 'toy_data/s_237_cluster_index.mat', 'toy_data/s_21_cluster_index.mat', 'toy_data/s_22_cluster_index.mat', 'toy_data/s_189_cluster_index.mat', 'toy_data/s_210_cluster_index.mat', 'toy_data/s_241_cluster_index.mat', 'toy_data/s_462_cluster_index.mat', 'toy_data/s_183_cluster_index.mat', 'toy_data/s_411_cluster_index.mat', 'toy_data/s_313_feature.mat', 'toy_data/s_154_feature.mat', 'toy_data/s_438_feature.mat', 'toy_data/s_327_feature.mat', 'toy_data/s_125_feature.mat', 'toy_data/s_230_cluster_index.mat', 'toy_data/s_329_cluster_index.mat', 'toy_data/s_107_cluster_index.mat', 'toy_data/s_333_feature.mat', 'toy_data/s_321_cluster_index.mat', 'toy_data/s_49_feature.mat', 'toy_data/s_144_feature.mat', 'toy_data/s_30_cluster_index.mat', 'toy_data/s_135_cluster_index.mat', 'toy_data/s_212_cluster_index.mat', 'toy_data/s_333_cluster_index.mat', 'toy_data/s_331_feature.mat', 'toy_data/s_162_cluster_index.mat', 'toy_data/s_228_feature.mat', 'toy_data/s_205_feature.mat', 'toy_data/s_174_cluster_index.mat', 'toy_data/s_426_cluster_index.mat', 'toy_data/s_55_cluster_index.mat', 'toy_data/s_239_feature.mat', 'toy_data/s_138_feature.mat', 'toy_data/s_197_cluster_index.mat', 'toy_data/s_105_cluster_index.mat', 'toy_data/s_205_cluster_index.mat', 'toy_data/s_460_feature.mat', 'toy_data/s_338_feature.mat', 'toy_data/s_376_feature.mat', 'toy_data/s_281_cluster_index.mat', 'toy_data/s_226_cluster_index.mat', 'toy_data/s_55_feature.mat', 'toy_data/s_89_feature.mat', 'toy_data/s_83_cluster_index.mat', 'toy_data/s_337_cluster_index.mat', 'toy_data/s_318_cluster_index.mat', 'toy_data/s_29_cluster_index.mat', 'toy_data/s_342_feature.mat', 'toy_data/s_489_feature.mat', 'toy_data/s_126_feature.mat', 'toy_data/s_397_cluster_index.mat', 'toy_data/s_436_cluster_index.mat', 'toy_data/s_383_feature.mat', 'toy_data/s_297_cluster_index.mat', 'toy_data/s_274_cluster_index.mat', 'toy_data/s_38_feature.mat', 'toy_data/s_332_feature.mat', 'toy_data/s_132_cluster_index.mat', 'toy_data/s_174_feature.mat', 'toy_data/s_347_feature.mat', 'toy_data/s_405_feature.mat', 'toy_data/s_263_cluster_index.mat', 'toy_data/s_67_feature.mat', 'toy_data/s_344_cluster_index.mat', 'toy_data/s_24_cluster_index.mat', 'toy_data/s_489_cluster_index.mat', 'toy_data/s_134_cluster_index.mat', 'toy_data/s_395_feature.mat', 'toy_data/s_413_feature.mat', 'toy_data/s_350_cluster_index.mat', 'toy_data/s_414_cluster_index.mat', 'toy_data/s_160_cluster_index.mat', 'toy_data/s_43_cluster_index.mat', 'toy_data/s_450_cluster_index.mat', 'toy_data/s_498_cluster_index.mat', 'toy_data/s_115_feature.mat', 'toy_data/s_406_cluster_index.mat', 'toy_data/s_324_cluster_index.mat', 'toy_data/s_420_feature.mat', 'toy_data/s_278_feature.mat', 'toy_data/s_303_feature.mat', 'toy_data/s_252_feature.mat', 'toy_data/s_76_feature.mat', 'toy_data/s_95_cluster_index.mat', 'toy_data/s_235_feature.mat', 'toy_data/s_178_feature.mat', 'toy_data/s_111_feature.mat', 'toy_data/s_375_feature.mat', 'toy_data/s_464_feature.mat', 'toy_data/s_251_feature.mat', 'toy_data/s_377_feature.mat', 'toy_data/s_129_cluster_index.mat', 'toy_data/s_260_feature.mat', 'toy_data/s_8_cluster_index.mat', 'toy_data/s_321_feature.mat', 'toy_data/s_275_feature.mat', 'toy_data/s_139_cluster_index.mat', 'toy_data/s_199_feature.mat', 'toy_data/s_298_cluster_index.mat', 'toy_data/s_31_cluster_index.mat', 'toy_data/s_123_feature.mat', 'toy_data/s_379_feature.mat', 'toy_data/s_242_cluster_index.mat', 'toy_data/s_89_cluster_index.mat', 'toy_data/s_258_feature.mat', 'toy_data/s_122_cluster_index.mat', 'toy_data/s_184_feature.mat', 'toy_data/s_68_cluster_index.mat', 'toy_data/s_332_cluster_index.mat', 'toy_data/s_301_cluster_index.mat', 'toy_data/s_358_cluster_index.mat', 'toy_data/s_19_feature.mat', 'toy_data/s_90_feature.mat', 'toy_data/s_446_feature.mat', 'toy_data/s_112_feature.mat', 'toy_data/s_386_feature.mat', 'toy_data/s_268_feature.mat', 'toy_data/s_95_feature.mat', 'toy_data/s_368_cluster_index.mat', 'toy_data/s_50_feature.mat', 'toy_data/s_37_cluster_index.mat', 'toy_data/s_62_cluster_index.mat', 'toy_data/s_87_cluster_index.mat', 'toy_data/s_17_feature.mat', 'toy_data/s_196_cluster_index.mat', 'toy_data/s_11_feature.mat', 'toy_data/s_499_feature.mat', 'toy_data/s_173_cluster_index.mat', 'toy_data/s_349_feature.mat', 'toy_data/s_180_feature.mat', 'toy_data/s_415_feature.mat', 'toy_data/s_431_feature.mat', 'toy_data/s_261_cluster_index.mat', 'toy_data/s_389_cluster_index.mat', 'toy_data/s_334_feature.mat', 'toy_data/s_364_feature.mat', 'toy_data/s_468_feature.mat', 'toy_data/s_218_cluster_index.mat', 'toy_data/s_182_cluster_index.mat', 'toy_data/s_275_cluster_index.mat', 'toy_data/s_13_feature.mat', 'toy_data/s_148_feature.mat', 'toy_data/s_305_cluster_index.mat', 'toy_data/s_395_cluster_index.mat', 'toy_data/s_141_feature.mat', 'toy_data/s_40_cluster_index.mat', 'toy_data/s_258_cluster_index.mat', 'toy_data/s_279_feature.mat', 'toy_data/s_242_feature.mat', 'toy_data/s_432_feature.mat', 'toy_data/s_7_feature.mat', 'toy_data/s_80_cluster_index.mat', 'toy_data/s_457_cluster_index.mat', 'toy_data/s_175_cluster_index.mat', 'toy_data/s_365_cluster_index.mat', 'toy_data/s_328_feature.mat', 'toy_data/s_237_feature.mat', 'toy_data/s_170_feature.mat', 'toy_data/s_294_feature.mat', 'toy_data/s_314_feature.mat', 'toy_data/s_326_cluster_index.mat', 'toy_data/s_1_cluster_index.mat', 'toy_data/s_16_cluster_index.mat', 'toy_data/s_136_feature.mat', 'toy_data/s_418_cluster_index.mat', 'toy_data/s_215_cluster_index.mat', 'toy_data/s_41_cluster_index.mat', 'toy_data/s_61_cluster_index.mat', 'toy_data/s_355_feature.mat', 'toy_data/s_365_feature.mat', 'toy_data/s_187_cluster_index.mat', 'toy_data/s_328_cluster_index.mat', 'toy_data/s_124_feature.mat', 'toy_data/s_224_feature.mat', 'toy_data/s_46_cluster_index.mat', 'toy_data/s_483_feature.mat', 'toy_data/s_15_cluster_index.mat', 'toy_data/s_377_cluster_index.mat', 'toy_data/s_417_cluster_index.mat', 'toy_data/s_228_cluster_index.mat', 'toy_data/s_29_feature.mat', 'toy_data/s_74_cluster_index.mat', 'toy_data/s_136_cluster_index.mat', 'toy_data/s_267_feature.mat', 'toy_data/s_488_cluster_index.mat', 'toy_data/s_303_cluster_index.mat', 'toy_data/s_454_cluster_index.mat', 'toy_data/s_236_cluster_index.mat', 'toy_data/s_99_feature.mat', 'toy_data/s_158_cluster_index.mat', 'toy_data/s_19_cluster_index.mat', 'toy_data/s_455_cluster_index.mat', 'toy_data/s_172_cluster_index.mat', 'toy_data/s_140_cluster_index.mat', 'toy_data/s_353_feature.mat', 'toy_data/s_271_feature.mat', 'toy_data/s_421_cluster_index.mat', 'toy_data/s_194_feature.mat', 'toy_data/s_114_cluster_index.mat', 'toy_data/s_84_feature.mat', 'toy_data/s_439_cluster_index.mat', 'toy_data/s_190_cluster_index.mat', 'toy_data/s_444_feature.mat', 'toy_data/s_434_feature.mat', 'toy_data/s_184_cluster_index.mat', 'toy_data/s_217_cluster_index.mat', 'toy_data/s_8_feature.mat', 'toy_data/s_186_cluster_index.mat', 'toy_data/s_264_feature.mat', 'toy_data/s_484_cluster_index.mat', 'toy_data/s_40_feature.mat', 'toy_data/s_478_cluster_index.mat', 'toy_data/s_265_feature.mat', 'toy_data/s_16_feature.mat', 'toy_data/s_425_cluster_index.mat', 'toy_data/s_45_cluster_index.mat', 'toy_data/s_164_cluster_index.mat', 'toy_data/s_222_cluster_index.mat', 'toy_data/s_101_feature.mat', 'toy_data/s_477_cluster_index.mat', 'toy_data/s_457_feature.mat', 'toy_data/s_131_cluster_index.mat', 'toy_data/s_459_feature.mat', 'toy_data/s_103_cluster_index.mat', 'toy_data/s_35_feature.mat', 'toy_data/s_322_cluster_index.mat', 'toy_data/s_61_feature.mat', 'toy_data/s_121_cluster_index.mat', 'toy_data/s_195_feature.mat', 'toy_data/s_214_cluster_index.mat', 'toy_data/s_263_feature.mat', 'toy_data/s_370_cluster_index.mat', 'toy_data/s_463_cluster_index.mat', 'toy_data/s_116_cluster_index.mat', 'toy_data/s_286_cluster_index.mat', 'toy_data/s_397_feature.mat', 'toy_data/s_339_cluster_index.mat', 'toy_data/s_262_feature.mat', 'toy_data/s_451_cluster_index.mat', 'toy_data/s_54_feature.mat', 'toy_data/s_202_cluster_index.mat', 'toy_data/s_282_cluster_index.mat', 'toy_data/s_380_cluster_index.mat', 'toy_data/s_249_cluster_index.mat', 'toy_data/s_454_feature.mat', 'toy_data/s_156_cluster_index.mat', 'toy_data/s_154_cluster_index.mat', 'toy_data/s_412_cluster_index.mat', 'toy_data/s_151_cluster_index.mat', 'toy_data/s_121_feature.mat', 'toy_data/s_120_cluster_index.mat', 'toy_data/s_287_feature.mat', 'toy_data/s_324_feature.mat', 'toy_data/s_83_feature.mat', 'toy_data/s_342_cluster_index.mat', 'toy_data/s_166_cluster_index.mat', 'toy_data/s_292_feature.mat', 'toy_data/s_301_feature.mat', 'toy_data/s_146_feature.mat', 'toy_data/s_380_feature.mat', 'toy_data/s_63_cluster_index.mat', 'toy_data/s_4_cluster_index.mat', 'toy_data/s_335_cluster_index.mat', 'toy_data/s_127_feature.mat', 'toy_data/s_23_feature.mat', 'toy_data/s_394_feature.mat', 'toy_data/s_436_feature.mat', 'toy_data/s_229_cluster_index.mat', 'toy_data/s_119_feature.mat', 'toy_data/s_249_feature.mat', 'toy_data/s_244_feature.mat', 'toy_data/s_485_feature.mat', 'toy_data/s_145_cluster_index.mat', 'toy_data/s_33_feature.mat', 'toy_data/s_64_cluster_index.mat', 'toy_data/s_408_cluster_index.mat', 'toy_data/s_451_feature.mat', 'toy_data/s_57_feature.mat', 'toy_data/s_423_cluster_index.mat', 'toy_data/s_169_cluster_index.mat', 'toy_data/s_401_cluster_index.mat', 'toy_data/s_97_cluster_index.mat', 'toy_data/s_445_feature.mat', 'toy_data/s_201_cluster_index.mat', 'toy_data/s_269_feature.mat', 'toy_data/s_440_feature.mat', 'toy_data/s_426_feature.mat', 'toy_data/s_77_cluster_index.mat', 'toy_data/s_138_cluster_index.mat', 'toy_data/s_477_feature.mat', 'toy_data/s_443_feature.mat', 'toy_data/s_50_cluster_index.mat', 'toy_data/s_191_feature.mat', 'toy_data/s_441_cluster_index.mat', 'toy_data/s_475_cluster_index.mat', 'toy_data/s_455_feature.mat', 'toy_data/s_79_feature.mat', 'toy_data/s_372_cluster_index.mat', 'toy_data/s_25_cluster_index.mat', 'toy_data/s_130_feature.mat', 'toy_data/s_48_cluster_index.mat', 'toy_data/s_442_cluster_index.mat', 'toy_data/s_274_feature.mat', 'toy_data/s_398_feature.mat', 'toy_data/s_113_cluster_index.mat', 'toy_data/s_108_feature.mat', 'toy_data/s_351_feature.mat', 'toy_data/s_327_cluster_index.mat', 'toy_data/s_330_cluster_index.mat', 'toy_data/s_299_feature.mat', 'toy_data/s_127_cluster_index.mat', 'toy_data/s_143_cluster_index.mat', 'toy_data/s_421_feature.mat', 'toy_data/s_230_feature.mat', 'toy_data/s_390_cluster_index.mat', 'toy_data/s_98_cluster_index.mat', 'toy_data/s_47_cluster_index.mat', 'toy_data/s_51_feature.mat', 'toy_data/s_285_cluster_index.mat', 'toy_data/s_72_feature.mat', 'toy_data/s_363_feature.mat', 'toy_data/s_47_feature.mat', 'toy_data/s_176_feature.mat', 'toy_data/s_471_feature.mat', 'toy_data/s_302_cluster_index.mat', 'toy_data/s_250_cluster_index.mat', 'toy_data/s_336_cluster_index.mat', 'toy_data/s_403_feature.mat', 'toy_data/s_229_feature.mat', 'toy_data/s_429_feature.mat', 'toy_data/s_282_feature.mat', 'toy_data/s_26_cluster_index.mat', 'toy_data/s_470_feature.mat', 'toy_data/s_188_feature.mat', 'toy_data/s_461_cluster_index.mat', 'toy_data/s_183_feature.mat', 'toy_data/s_131_feature.mat', 'toy_data/s_173_feature.mat', 'toy_data/s_168_cluster_index.mat', 'toy_data/s_30_feature.mat', 'toy_data/s_85_feature.mat', 'toy_data/s_81_feature.mat', 'toy_data/s_233_cluster_index.mat', 'toy_data/s_305_feature.mat', 'toy_data/s_143_feature.mat', 'toy_data/s_428_feature.mat', 'toy_data/s_32_feature.mat', 'toy_data/s_65_feature.mat', 'toy_data/s_295_cluster_index.mat', 'toy_data/s_196_feature.mat', 'toy_data/s_316_cluster_index.mat', 'toy_data/s_210_feature.mat', 'toy_data/s_54_cluster_index.mat', 'toy_data/s_25_feature.mat', 'toy_data/s_461_feature.mat', 'toy_data/s_486_feature.mat', 'toy_data/s_484_feature.mat', 'toy_data/s_189_feature.mat', 'toy_data/s_279_cluster_index.mat', 'toy_data/s_378_feature.mat', 'toy_data/s_96_cluster_index.mat', 'toy_data/s_203_feature.mat', 'toy_data/s_429_cluster_index.mat', 'toy_data/s_471_cluster_index.mat', 'toy_data/s_465_cluster_index.mat', 'toy_data/s_220_cluster_index.mat', 'toy_data/s_31_feature.mat', 'toy_data/s_243_cluster_index.mat', 'toy_data/s_155_feature.mat', 'toy_data/s_343_feature.mat', 'toy_data/s_387_feature.mat', 'toy_data/s_447_feature.mat', 'toy_data/s_440_cluster_index.mat', 'toy_data/s_3_feature.mat', 'toy_data/s_255_cluster_index.mat', 'toy_data/s_407_cluster_index.mat', 'toy_data/s_26_feature.mat', 'toy_data/s_198_feature.mat', 'toy_data/s_284_feature.mat', 'toy_data/s_372_feature.mat', 'toy_data/s_102_feature.mat', 'toy_data/s_37_feature.mat', 'toy_data/s_325_feature.mat', 'toy_data/s_416_cluster_index.mat', 'toy_data/s_6_cluster_index.mat', 'toy_data/s_90_cluster_index.mat', 'toy_data/s_106_feature.mat', 'toy_data/s_343_cluster_index.mat', 'toy_data/s_470_cluster_index.mat', 'toy_data/s_251_cluster_index.mat', 'toy_data/s_466_feature.mat', 'toy_data/s_340_cluster_index.mat', 'toy_data/s_307_cluster_index.mat', 'toy_data/s_290_cluster_index.mat', 'toy_data/s_246_feature.mat', 'toy_data/s_418_feature.mat', 'toy_data/s_48_feature.mat', 'toy_data/s_223_feature.mat', 'toy_data/s_133_cluster_index.mat', 'toy_data/s_36_feature.mat', 'toy_data/s_462_feature.mat', 'toy_data/s_304_feature.mat', 'toy_data/s_276_cluster_index.mat', 'toy_data/s_264_cluster_index.mat', 'toy_data/s_166_feature.mat', 'toy_data/s_466_cluster_index.mat', 'toy_data/s_319_cluster_index.mat', 'toy_data/s_4_feature.mat', 'toy_data/s_373_feature.mat', 'toy_data/s_310_feature.mat', 'toy_data/s_311_feature.mat', 'toy_data/s_465_feature.mat', 'toy_data/s_130_cluster_index.mat', 'toy_data/s_458_feature.mat', 'toy_data/s_254_feature.mat', 'toy_data/s_91_cluster_index.mat', 'toy_data/s_146_cluster_index.mat', 'toy_data/s_280_feature.mat', 'toy_data/s_323_cluster_index.mat', 'toy_data/s_128_feature.mat', 'toy_data/s_329_feature.mat', 'toy_data/s_373_cluster_index.mat', 'toy_data/s_248_feature.mat', 'toy_data/s_168_feature.mat', 'toy_data/s_112_cluster_index.mat', 'toy_data/s_71_feature.mat', 'toy_data/s_227_feature.mat', 'toy_data/s_105_feature.mat', 'toy_data/s_300_feature.mat', 'toy_data/s_442_feature.mat', 'toy_data/s_385_cluster_index.mat', 'toy_data/s_70_cluster_index.mat', 'toy_data/s_49_cluster_index.mat', 'toy_data/s_463_feature.mat', 'toy_data/s_257_feature.mat', 'toy_data/s_367_feature.mat', 'toy_data/s_208_feature.mat', 'toy_data/s_238_feature.mat', 'toy_data/s_36_cluster_index.mat', 'toy_data/s_39_cluster_index.mat', 'toy_data/s_259_cluster_index.mat', 'toy_data/s_432_cluster_index.mat', 'toy_data/s_241_feature.mat', 'toy_data/s_11_cluster_index.mat', 'toy_data/s_369_cluster_index.mat', 'toy_data/s_338_cluster_index.mat', 'toy_data/s_82_feature.mat', 'toy_data/s_240_cluster_index.mat', 'toy_data/s_266_cluster_index.mat', 'toy_data/s_106_cluster_index.mat', 'toy_data/s_374_feature.mat', 'toy_data/s_478_feature.mat', 'toy_data/s_151_feature.mat', 'toy_data/s_32_cluster_index.mat', 'toy_data/s_165_feature.mat', 'toy_data/s_233_feature.mat', 'toy_data/s_359_feature.mat', 'toy_data/s_392_feature.mat', 'toy_data/s_496_feature.mat', 'toy_data/s_52_feature.mat', 'toy_data/s_494_cluster_index.mat', 'toy_data/s_312_cluster_index.mat', 'toy_data/s_363_cluster_index.mat', 'toy_data/s_259_feature.mat', 'toy_data/s_331_cluster_index.mat', 'toy_data/s_326_feature.mat', 'toy_data/s_388_feature.mat', 'toy_data/s_185_feature.mat', 'toy_data/s_118_feature.mat', 'toy_data/s_267_cluster_index.mat', 'toy_data/s_51_cluster_index.mat', 'toy_data/s_357_cluster_index.mat', 'toy_data/s_480_feature.mat', 'toy_data/s_359_cluster_index.mat', 'toy_data/s_453_feature.mat', 'toy_data/s_356_feature.mat', 'toy_data/s_448_feature.mat', 'toy_data/s_272_feature.mat', 'toy_data/s_498_feature.mat', 'toy_data/s_254_cluster_index.mat', 'toy_data/s_352_feature.mat', 'toy_data/s_214_feature.mat', 'toy_data/s_65_cluster_index.mat', 'toy_data/s_309_cluster_index.mat', 'toy_data/s_487_feature.mat', 'toy_data/s_283_feature.mat', 'toy_data/s_493_feature.mat', 'toy_data/s_155_cluster_index.mat', 'toy_data/s_352_cluster_index.mat', 'toy_data/s_99_cluster_index.mat', 'toy_data/s_96_feature.mat', 'toy_data/s_198_cluster_index.mat', 'toy_data/s_302_feature.mat', 'toy_data/s_104_feature.mat', 'toy_data/s_341_feature.mat', 'toy_data/s_20_cluster_index.mat', 'toy_data/s_187_feature.mat', 'toy_data/s_350_feature.mat', 'toy_data/s_227_cluster_index.mat', 'toy_data/s_27_feature.mat', 'toy_data/s_467_feature.mat', 'toy_data/s_341_cluster_index.mat', 'toy_data/s_413_cluster_index.mat', 'toy_data/s_476_cluster_index.mat', 'toy_data/s_411_feature.mat', 'toy_data/s_445_cluster_index.mat', 'toy_data/s_281_feature.mat', 'toy_data/s_428_cluster_index.mat', 'toy_data/s_88_feature.mat', 'toy_data/s_162_feature.mat', 'toy_data/s_150_feature.mat', 'toy_data/s_273_cluster_index.mat', 'toy_data/s_163_cluster_index.mat', 'toy_data/s_408_feature.mat', 'toy_data/s_488_feature.mat', 'toy_data/s_188_cluster_index.mat', 'toy_data/s_59_cluster_index.mat', 'toy_data/s_306_feature.mat', 'toy_data/s_167_feature.mat', 'toy_data/s_277_feature.mat', 'toy_data/s_69_cluster_index.mat', 'toy_data/s_449_feature.mat', 'toy_data/s_97_feature.mat', 'toy_data/s_109_feature.mat', 'toy_data/s_318_feature.mat', 'toy_data/s_59_feature.mat', 'toy_data/s_180_cluster_index.mat', 'toy_data/s_366_cluster_index.mat', 'toy_data/s_18_cluster_index.mat', 'toy_data/s_35_cluster_index.mat', 'toy_data/s_171_cluster_index.mat', 'toy_data/s_417_feature.mat', 'toy_data/s_244_cluster_index.mat', 'toy_data/s_480_cluster_index.mat', 'toy_data/s_476_feature.mat', 'toy_data/s_336_feature.mat', 'toy_data/s_104_cluster_index.mat', 'toy_data/s_67_cluster_index.mat', 'toy_data/s_213_cluster_index.mat', 'toy_data/s_1_feature.mat', 'toy_data/s_369_feature.mat', 'toy_data/s_245_feature.mat', 'toy_data/s_437_cluster_index.mat', 'toy_data/s_28_cluster_index.mat', 'toy_data/s_137_feature.mat', 'toy_data/s_439_feature.mat', 'toy_data/s_382_feature.mat', 'toy_data/s_38_cluster_index.mat', 'toy_data/s_427_cluster_index.mat', 'toy_data/s_309_feature.mat', 'toy_data/s_362_feature.mat', 'toy_data/s_231_cluster_index.mat', 'toy_data/s_75_cluster_index.mat', 'toy_data/s_306_cluster_index.mat', 'toy_data/s_141_cluster_index.mat', 'toy_data/s_357_feature.mat', 'toy_data/s_260_cluster_index.mat', 'toy_data/s_201_feature.mat', 'toy_data/s_149_cluster_index.mat', 'toy_data/s_492_cluster_index.mat', 'toy_data/s_299_cluster_index.mat', 'toy_data/s_256_feature.mat', 'toy_data/s_285_feature.mat', 'toy_data/s_355_cluster_index.mat', 'toy_data/s_215_feature.mat', 'toy_data/s_262_cluster_index.mat', 'toy_data/s_491_cluster_index.mat', 'toy_data/s_76_cluster_index.mat', 'toy_data/s_192_feature.mat', 'toy_data/s_435_cluster_index.mat', 'toy_data/s_296_cluster_index.mat', 'toy_data/s_266_feature.mat', 'toy_data/s_225_cluster_index.mat', 'toy_data/s_473_cluster_index.mat', 'toy_data/s_256_cluster_index.mat', 'toy_data/s_291_cluster_index.mat', 'toy_data/s_177_feature.mat', 'toy_data/s_361_cluster_index.mat', 'toy_data/s_473_feature.mat', 'toy_data/s_57_cluster_index.mat', 'toy_data/s_474_cluster_index.mat', 'toy_data/s_360_feature.mat', 'toy_data/s_7_cluster_index.mat', 'toy_data/s_74_feature.mat', 'toy_data/s_468_cluster_index.mat', 'toy_data/s_297_feature.mat', 'toy_data/s_5_feature.mat', 'toy_data/s_495_cluster_index.mat', 'toy_data/s_296_feature.mat', 'toy_data/s_288_feature.mat', 'toy_data/s_239_cluster_index.mat', 'toy_data/s_94_cluster_index.mat', 'toy_data/s_268_cluster_index.mat', 'toy_data/s_247_cluster_index.mat', 'toy_data/s_402_feature.mat', 'toy_data/s_66_cluster_index.mat', 'toy_data/s_60_feature.mat', 'toy_data/s_403_cluster_index.mat', 'toy_data/s_446_cluster_index.mat', 'toy_data/s_423_feature.mat', 'toy_data/s_219_feature.mat', 'toy_data/s_290_feature.mat', 'toy_data/s_181_cluster_index.mat', 'toy_data/s_252_cluster_index.mat', 'toy_data/s_211_cluster_index.mat', 'toy_data/s_170_cluster_index.mat', 'toy_data/s_178_cluster_index.mat', 'toy_data/label.mat', 'toy_data/s_474_feature.mat', 'toy_data/s_314_cluster_index.mat', 'toy_data/s_10_feature.mat', 'toy_data/s_42_cluster_index.mat', 'toy_data/s_390_feature.mat', 'toy_data/s_203_cluster_index.mat', 'toy_data/s_179_feature.mat', 'toy_data/s_152_cluster_index.mat', 'toy_data/s_368_feature.mat', 'toy_data/s_354_feature.mat', 'toy_data/s_137_cluster_index.mat', 'toy_data/s_179_cluster_index.mat', 'toy_data/s_469_feature.mat', 'toy_data/s_139_feature.mat', 'toy_data/s_376_cluster_index.mat', 'toy_data/s_157_feature.mat', 'toy_data/s_245_cluster_index.mat', 'toy_data/s_409_feature.mat', 'toy_data/s_482_cluster_index.mat', 'toy_data/s_129_feature.mat', 'toy_data/s_388_cluster_index.mat', 'toy_data/s_276_feature.mat', 'toy_data/s_58_cluster_index.mat', 'toy_data/s_14_cluster_index.mat', 'toy_data/s_234_cluster_index.mat', 'toy_data/s_366_feature.mat', 'toy_data/s_68_feature.mat', 'toy_data/s_389_feature.mat', 'toy_data/s_218_feature.mat', 'toy_data/s_371_cluster_index.mat', 'toy_data/s_393_feature.mat', 'toy_data/s_325_cluster_index.mat', 'toy_data/s_34_cluster_index.mat', 'toy_data/s_100_feature.mat', 'toy_data/s_93_feature.mat', 'toy_data/s_420_cluster_index.mat', 'toy_data/s_164_feature.mat', 'toy_data/s_94_feature.mat', 'toy_data/s_60_cluster_index.mat', 'toy_data/s_39_feature.mat', 'toy_data/s_87_feature.mat', 'toy_data/s_246_cluster_index.mat', 'toy_data/s_300_cluster_index.mat', 'toy_data/s_223_cluster_index.mat', 'toy_data/s_354_cluster_index.mat', 'toy_data/s_9_feature.mat', 'toy_data/s_165_cluster_index.mat', 'toy_data/s_392_cluster_index.mat', 'toy_data/s_161_feature.mat', 'toy_data/s_414_feature.mat', 'toy_data/s_41_feature.mat', 'toy_data/s_213_feature.mat', 'toy_data/s_21_feature.mat', 'toy_data/s_22_feature.mat', 'toy_data/s_177_cluster_index.mat', 'toy_data/s_409_cluster_index.mat', 'toy_data/s_135_feature.mat', 'toy_data/s_315_cluster_index.mat', 'toy_data/s_132_feature.mat', 'toy_data/s_339_feature.mat', 'toy_data/s_360_cluster_index.mat', 'toy_data/s_265_cluster_index.mat', 'toy_data/s_33_cluster_index.mat', 'toy_data/s_293_cluster_index.mat', 'toy_data/s_171_feature.mat', 'toy_data/s_308_cluster_index.mat', 'toy_data/s_250_feature.mat', 'toy_data/s_287_cluster_index.mat', 'toy_data/s_3_cluster_index.mat', 'toy_data/s_353_cluster_index.mat', 'toy_data/s_85_cluster_index.mat', 'toy_data/s_458_cluster_index.mat', 'toy_data/s_81_cluster_index.mat', 'toy_data/s_472_cluster_index.mat', 'toy_data/s_181_feature.mat', 'toy_data/s_185_cluster_index.mat', 'toy_data/s_235_cluster_index.mat', 'toy_data/s_295_feature.mat', 'toy_data/s_400_cluster_index.mat', 'toy_data/s_371_feature.mat', 'toy_data/s_375_cluster_index.mat', 'toy_data/s_337_feature.mat', 'toy_data/s_425_feature.mat', 'toy_data/s_220_feature.mat', 'toy_data/s_384_feature.mat', 'toy_data/s_316_feature.mat', 'toy_data/s_422_cluster_index.mat', 'toy_data/s_383_cluster_index.mat', 'toy_data/s_427_feature.mat', 'toy_data/s_2_feature.mat', 'toy_data/s_186_feature.mat', 'toy_data/s_253_cluster_index.mat', 'toy_data/s_394_cluster_index.mat', 'toy_data/s_107_feature.mat', 'toy_data/s_253_feature.mat', 'toy_data/s_378_cluster_index.mat', 'toy_data/s_500_feature.mat', 'toy_data/s_278_cluster_index.mat', 'toy_data/s_13_cluster_index.mat', 'toy_data/s_391_cluster_index.mat', 'toy_data/s_379_cluster_index.mat', 'toy_data/s_211_feature.mat', 'toy_data/s_53_feature.mat', 'toy_data/s_330_feature.mat', 'toy_data/s_124_cluster_index.mat', 'toy_data/s_148_cluster_index.mat', 'toy_data/s_433_feature.mat', 'toy_data/s_410_cluster_index.mat', 'toy_data/s_348_feature.mat', 'toy_data/s_399_cluster_index.mat', 'toy_data/s_405_cluster_index.mat', 'toy_data/s_270_feature.mat', 'toy_data/s_491_feature.mat', 'toy_data/s_419_feature.mat', 'toy_data/s_362_cluster_index.mat', 'toy_data/s_12_cluster_index.mat', 'toy_data/s_385_feature.mat', 'toy_data/s_216_feature.mat', 'toy_data/s_169_feature.mat', 'toy_data/s_412_feature.mat', 'toy_data/s_58_feature.mat', 'toy_data/s_273_feature.mat', 'toy_data/s_289_cluster_index.mat', 'toy_data/s_161_cluster_index.mat', 'toy_data/s_456_feature.mat', 'toy_data/s_200_feature.mat', 'toy_data/s_125_cluster_index.mat', 'toy_data/s_77_feature.mat', 'toy_data/s_191_cluster_index.mat', 'toy_data/s_152_feature.mat', 'toy_data/s_310_cluster_index.mat', 'toy_data/s_92_feature.mat', 'toy_data/s_142_cluster_index.mat', 'toy_data/s_101_cluster_index.mat', 'toy_data/s_346_feature.mat', 'toy_data/s_82_cluster_index.mat', 'toy_data/s_17_cluster_index.mat', 'toy_data/s_381_feature.mat', 'toy_data/s_23_cluster_index.mat', 'toy_data/s_382_cluster_index.mat', 'toy_data/s_496_cluster_index.mat', 'toy_data/s_194_cluster_index.mat', 'toy_data/s_404_cluster_index.mat', 'toy_data/s_111_cluster_index.mat', 'toy_data/s_356_cluster_index.mat', 'toy_data/s_450_feature.mat', 'toy_data/s_448_cluster_index.mat', 'toy_data/s_149_feature.mat', 'toy_data/s_46_feature.mat', 'toy_data/s_204_feature.mat', 'toy_data/s_393_cluster_index.mat', 'toy_data/s_467_cluster_index.mat', 'toy_data/s_78_feature.mat', 'toy_data/s_349_cluster_index.mat', 'toy_data/s_346_cluster_index.mat', 'toy_data/s_159_cluster_index.mat', 'toy_data/s_108_cluster_index.mat', 'toy_data/s_84_cluster_index.mat', 'toy_data/s_52_cluster_index.mat', 'toy_data/s_475_feature.mat', 'toy_data/s_293_feature.mat', 'toy_data/s_236_feature.mat', 'toy_data/s_482_feature.mat', 'toy_data/s_348_cluster_index.mat', 'toy_data/s_86_cluster_index.mat', 'toy_data/s_447_cluster_index.mat', 'toy_data/s_115_cluster_index.mat', 'toy_data/s_93_cluster_index.mat', 'toy_data/s_224_cluster_index.mat', 'toy_data/s_243_feature.mat', 'toy_data/s_69_feature.mat', 'toy_data/s_240_feature.mat', 'toy_data/s_492_feature.mat', 'toy_data/s_5_cluster_index.mat', 'toy_data/s_160_feature.mat', 'toy_data/s_120_feature.mat', 'toy_data/s_280_cluster_index.mat', 'toy_data/s_142_feature.mat', 'toy_data/s_391_feature.mat', 'toy_data/s_422_feature.mat', 'toy_data/s_53_cluster_index.mat', 'toy_data/s_225_feature.mat', 'toy_data/s_347_cluster_index.mat', 'toy_data/s_500_cluster_index.mat', 'toy_data/s_238_cluster_index.mat', 'toy_data/s_452_cluster_index.mat', 'toy_data/s_494_feature.mat', 'toy_data/s_113_feature.mat', 'toy_data/s_80_feature.mat', 'toy_data/s_367_cluster_index.mat', 'toy_data/s_86_feature.mat', 'toy_data/s_430_cluster_index.mat', 'toy_data/s_150_cluster_index.mat', 'toy_data/s_34_feature.mat', 'toy_data/s_167_cluster_index.mat', 'toy_data/s_217_feature.mat', 'toy_data/s_435_feature.mat', 'toy_data/s_449_cluster_index.mat', 'toy_data/s_133_feature.mat', 'toy_data/s_153_feature.mat', 'toy_data/s_163_feature.mat', 'toy_data/s_257_cluster_index.mat', 'toy_data/s_158_feature.mat', 'toy_data/s_317_feature.mat', 'toy_data/s_284_cluster_index.mat', 'toy_data/s_91_feature.mat', 'toy_data/s_415_cluster_index.mat', 'toy_data/s_434_cluster_index.mat', 'toy_data/s_319_feature.mat', 'toy_data/s_453_cluster_index.mat', 'toy_data/s_292_cluster_index.mat', 'toy_data/s_175_feature.mat', 'toy_data/s_71_cluster_index.mat', 'toy_data/s_45_feature.mat', 'toy_data/s_433_cluster_index.mat', 'toy_data/s_204_cluster_index.mat', 'toy_data/s_226_feature.mat', 'toy_data/s_79_cluster_index.mat', 'toy_data/s_361_feature.mat', 'toy_data/s_197_feature.mat', 'toy_data/s_315_feature.mat', 'toy_data/s_495_feature.mat', 'toy_data/s_497_cluster_index.mat', 'toy_data/s_248_cluster_index.mat', 'toy_data/s_481_cluster_index.mat', 'toy_data/s_122_feature.mat', 'toy_data/s_114_feature.mat', 'toy_data/s_479_cluster_index.mat', 'toy_data/s_487_cluster_index.mat', 'toy_data/s_335_feature.mat', 'toy_data/s_277_cluster_index.mat', 'toy_data/s_98_feature.mat', 'toy_data/s_221_feature.mat', 'toy_data/s_176_cluster_index.mat', 'toy_data/s_117_cluster_index.mat', 'toy_data/s_286_feature.mat', 'toy_data/s_231_feature.mat', 'toy_data/s_438_cluster_index.mat', 'toy_data/s_199_cluster_index.mat', 'toy_data/s_410_feature.mat', 'toy_data/s_320_cluster_index.mat']\n",
      "['toy_data/s_1_feature.mat', 'toy_data/s_2_feature.mat', 'toy_data/s_3_feature.mat', 'toy_data/s_4_feature.mat', 'toy_data/s_5_feature.mat', 'toy_data/s_6_feature.mat', 'toy_data/s_7_feature.mat', 'toy_data/s_8_feature.mat', 'toy_data/s_9_feature.mat', 'toy_data/s_10_feature.mat', 'toy_data/s_11_feature.mat', 'toy_data/s_12_feature.mat', 'toy_data/s_13_feature.mat', 'toy_data/s_14_feature.mat', 'toy_data/s_15_feature.mat', 'toy_data/s_16_feature.mat', 'toy_data/s_17_feature.mat', 'toy_data/s_18_feature.mat', 'toy_data/s_19_feature.mat', 'toy_data/s_20_feature.mat', 'toy_data/s_21_feature.mat', 'toy_data/s_22_feature.mat', 'toy_data/s_23_feature.mat', 'toy_data/s_24_feature.mat', 'toy_data/s_25_feature.mat', 'toy_data/s_26_feature.mat', 'toy_data/s_27_feature.mat', 'toy_data/s_28_feature.mat', 'toy_data/s_29_feature.mat', 'toy_data/s_30_feature.mat', 'toy_data/s_31_feature.mat', 'toy_data/s_32_feature.mat', 'toy_data/s_33_feature.mat', 'toy_data/s_34_feature.mat', 'toy_data/s_35_feature.mat', 'toy_data/s_36_feature.mat', 'toy_data/s_37_feature.mat', 'toy_data/s_38_feature.mat', 'toy_data/s_39_feature.mat', 'toy_data/s_40_feature.mat', 'toy_data/s_41_feature.mat', 'toy_data/s_42_feature.mat', 'toy_data/s_43_feature.mat', 'toy_data/s_44_feature.mat', 'toy_data/s_45_feature.mat', 'toy_data/s_46_feature.mat', 'toy_data/s_47_feature.mat', 'toy_data/s_48_feature.mat', 'toy_data/s_49_feature.mat', 'toy_data/s_50_feature.mat', 'toy_data/s_51_feature.mat', 'toy_data/s_52_feature.mat', 'toy_data/s_53_feature.mat', 'toy_data/s_54_feature.mat', 'toy_data/s_55_feature.mat', 'toy_data/s_56_feature.mat', 'toy_data/s_57_feature.mat', 'toy_data/s_58_feature.mat', 'toy_data/s_59_feature.mat', 'toy_data/s_60_feature.mat', 'toy_data/s_61_feature.mat', 'toy_data/s_62_feature.mat', 'toy_data/s_63_feature.mat', 'toy_data/s_64_feature.mat', 'toy_data/s_65_feature.mat', 'toy_data/s_66_feature.mat', 'toy_data/s_67_feature.mat', 'toy_data/s_68_feature.mat', 'toy_data/s_69_feature.mat', 'toy_data/s_70_feature.mat', 'toy_data/s_71_feature.mat', 'toy_data/s_72_feature.mat', 'toy_data/s_73_feature.mat', 'toy_data/s_74_feature.mat', 'toy_data/s_75_feature.mat', 'toy_data/s_76_feature.mat', 'toy_data/s_77_feature.mat', 'toy_data/s_78_feature.mat', 'toy_data/s_79_feature.mat', 'toy_data/s_80_feature.mat', 'toy_data/s_81_feature.mat', 'toy_data/s_82_feature.mat', 'toy_data/s_83_feature.mat', 'toy_data/s_84_feature.mat', 'toy_data/s_85_feature.mat', 'toy_data/s_86_feature.mat', 'toy_data/s_87_feature.mat', 'toy_data/s_88_feature.mat', 'toy_data/s_89_feature.mat', 'toy_data/s_90_feature.mat', 'toy_data/s_91_feature.mat', 'toy_data/s_92_feature.mat', 'toy_data/s_93_feature.mat', 'toy_data/s_94_feature.mat', 'toy_data/s_95_feature.mat', 'toy_data/s_96_feature.mat', 'toy_data/s_97_feature.mat', 'toy_data/s_98_feature.mat', 'toy_data/s_99_feature.mat', 'toy_data/s_100_feature.mat', 'toy_data/s_101_feature.mat', 'toy_data/s_102_feature.mat', 'toy_data/s_103_feature.mat', 'toy_data/s_104_feature.mat', 'toy_data/s_105_feature.mat', 'toy_data/s_106_feature.mat', 'toy_data/s_107_feature.mat', 'toy_data/s_108_feature.mat', 'toy_data/s_109_feature.mat', 'toy_data/s_110_feature.mat', 'toy_data/s_111_feature.mat', 'toy_data/s_112_feature.mat', 'toy_data/s_113_feature.mat', 'toy_data/s_114_feature.mat', 'toy_data/s_115_feature.mat', 'toy_data/s_116_feature.mat', 'toy_data/s_117_feature.mat', 'toy_data/s_118_feature.mat', 'toy_data/s_119_feature.mat', 'toy_data/s_120_feature.mat', 'toy_data/s_121_feature.mat', 'toy_data/s_122_feature.mat', 'toy_data/s_123_feature.mat', 'toy_data/s_124_feature.mat', 'toy_data/s_125_feature.mat', 'toy_data/s_126_feature.mat', 'toy_data/s_127_feature.mat', 'toy_data/s_128_feature.mat', 'toy_data/s_129_feature.mat', 'toy_data/s_130_feature.mat', 'toy_data/s_131_feature.mat', 'toy_data/s_132_feature.mat', 'toy_data/s_133_feature.mat', 'toy_data/s_134_feature.mat', 'toy_data/s_135_feature.mat', 'toy_data/s_136_feature.mat', 'toy_data/s_137_feature.mat', 'toy_data/s_138_feature.mat', 'toy_data/s_139_feature.mat', 'toy_data/s_140_feature.mat', 'toy_data/s_141_feature.mat', 'toy_data/s_142_feature.mat', 'toy_data/s_143_feature.mat', 'toy_data/s_144_feature.mat', 'toy_data/s_145_feature.mat', 'toy_data/s_146_feature.mat', 'toy_data/s_147_feature.mat', 'toy_data/s_148_feature.mat', 'toy_data/s_149_feature.mat', 'toy_data/s_150_feature.mat', 'toy_data/s_151_feature.mat', 'toy_data/s_152_feature.mat', 'toy_data/s_153_feature.mat', 'toy_data/s_154_feature.mat', 'toy_data/s_155_feature.mat', 'toy_data/s_156_feature.mat', 'toy_data/s_157_feature.mat', 'toy_data/s_158_feature.mat', 'toy_data/s_159_feature.mat', 'toy_data/s_160_feature.mat', 'toy_data/s_161_feature.mat', 'toy_data/s_162_feature.mat', 'toy_data/s_163_feature.mat', 'toy_data/s_164_feature.mat', 'toy_data/s_165_feature.mat', 'toy_data/s_166_feature.mat', 'toy_data/s_167_feature.mat', 'toy_data/s_168_feature.mat', 'toy_data/s_169_feature.mat', 'toy_data/s_170_feature.mat', 'toy_data/s_171_feature.mat', 'toy_data/s_172_feature.mat', 'toy_data/s_173_feature.mat', 'toy_data/s_174_feature.mat', 'toy_data/s_175_feature.mat', 'toy_data/s_176_feature.mat', 'toy_data/s_177_feature.mat', 'toy_data/s_178_feature.mat', 'toy_data/s_179_feature.mat', 'toy_data/s_180_feature.mat', 'toy_data/s_181_feature.mat', 'toy_data/s_182_feature.mat', 'toy_data/s_183_feature.mat', 'toy_data/s_184_feature.mat', 'toy_data/s_185_feature.mat', 'toy_data/s_186_feature.mat', 'toy_data/s_187_feature.mat', 'toy_data/s_188_feature.mat', 'toy_data/s_189_feature.mat', 'toy_data/s_190_feature.mat', 'toy_data/s_191_feature.mat', 'toy_data/s_192_feature.mat', 'toy_data/s_193_feature.mat', 'toy_data/s_194_feature.mat', 'toy_data/s_195_feature.mat', 'toy_data/s_196_feature.mat', 'toy_data/s_197_feature.mat', 'toy_data/s_198_feature.mat', 'toy_data/s_199_feature.mat', 'toy_data/s_200_feature.mat', 'toy_data/s_201_feature.mat', 'toy_data/s_202_feature.mat', 'toy_data/s_203_feature.mat', 'toy_data/s_204_feature.mat', 'toy_data/s_205_feature.mat', 'toy_data/s_206_feature.mat', 'toy_data/s_207_feature.mat', 'toy_data/s_208_feature.mat', 'toy_data/s_209_feature.mat', 'toy_data/s_210_feature.mat', 'toy_data/s_211_feature.mat', 'toy_data/s_212_feature.mat', 'toy_data/s_213_feature.mat', 'toy_data/s_214_feature.mat', 'toy_data/s_215_feature.mat', 'toy_data/s_216_feature.mat', 'toy_data/s_217_feature.mat', 'toy_data/s_218_feature.mat', 'toy_data/s_219_feature.mat', 'toy_data/s_220_feature.mat', 'toy_data/s_221_feature.mat', 'toy_data/s_222_feature.mat', 'toy_data/s_223_feature.mat', 'toy_data/s_224_feature.mat', 'toy_data/s_225_feature.mat', 'toy_data/s_226_feature.mat', 'toy_data/s_227_feature.mat', 'toy_data/s_228_feature.mat', 'toy_data/s_229_feature.mat', 'toy_data/s_230_feature.mat', 'toy_data/s_231_feature.mat', 'toy_data/s_232_feature.mat', 'toy_data/s_233_feature.mat', 'toy_data/s_234_feature.mat', 'toy_data/s_235_feature.mat', 'toy_data/s_236_feature.mat', 'toy_data/s_237_feature.mat', 'toy_data/s_238_feature.mat', 'toy_data/s_239_feature.mat', 'toy_data/s_240_feature.mat', 'toy_data/s_241_feature.mat', 'toy_data/s_242_feature.mat', 'toy_data/s_243_feature.mat', 'toy_data/s_244_feature.mat', 'toy_data/s_245_feature.mat', 'toy_data/s_246_feature.mat', 'toy_data/s_247_feature.mat', 'toy_data/s_248_feature.mat', 'toy_data/s_249_feature.mat', 'toy_data/s_250_feature.mat', 'toy_data/s_251_feature.mat', 'toy_data/s_252_feature.mat', 'toy_data/s_253_feature.mat', 'toy_data/s_254_feature.mat', 'toy_data/s_255_feature.mat', 'toy_data/s_256_feature.mat', 'toy_data/s_257_feature.mat', 'toy_data/s_258_feature.mat', 'toy_data/s_259_feature.mat', 'toy_data/s_260_feature.mat', 'toy_data/s_261_feature.mat', 'toy_data/s_262_feature.mat', 'toy_data/s_263_feature.mat', 'toy_data/s_264_feature.mat', 'toy_data/s_265_feature.mat', 'toy_data/s_266_feature.mat', 'toy_data/s_267_feature.mat', 'toy_data/s_268_feature.mat', 'toy_data/s_269_feature.mat', 'toy_data/s_270_feature.mat', 'toy_data/s_271_feature.mat', 'toy_data/s_272_feature.mat', 'toy_data/s_273_feature.mat', 'toy_data/s_274_feature.mat', 'toy_data/s_275_feature.mat', 'toy_data/s_276_feature.mat', 'toy_data/s_277_feature.mat', 'toy_data/s_278_feature.mat', 'toy_data/s_279_feature.mat', 'toy_data/s_280_feature.mat', 'toy_data/s_281_feature.mat', 'toy_data/s_282_feature.mat', 'toy_data/s_283_feature.mat', 'toy_data/s_284_feature.mat', 'toy_data/s_285_feature.mat', 'toy_data/s_286_feature.mat', 'toy_data/s_287_feature.mat', 'toy_data/s_288_feature.mat', 'toy_data/s_289_feature.mat', 'toy_data/s_290_feature.mat', 'toy_data/s_291_feature.mat', 'toy_data/s_292_feature.mat', 'toy_data/s_293_feature.mat', 'toy_data/s_294_feature.mat', 'toy_data/s_295_feature.mat', 'toy_data/s_296_feature.mat', 'toy_data/s_297_feature.mat', 'toy_data/s_298_feature.mat', 'toy_data/s_299_feature.mat', 'toy_data/s_300_feature.mat', 'toy_data/s_301_feature.mat', 'toy_data/s_302_feature.mat', 'toy_data/s_303_feature.mat', 'toy_data/s_304_feature.mat', 'toy_data/s_305_feature.mat', 'toy_data/s_306_feature.mat', 'toy_data/s_307_feature.mat', 'toy_data/s_308_feature.mat', 'toy_data/s_309_feature.mat', 'toy_data/s_310_feature.mat', 'toy_data/s_311_feature.mat', 'toy_data/s_312_feature.mat', 'toy_data/s_313_feature.mat', 'toy_data/s_314_feature.mat', 'toy_data/s_315_feature.mat', 'toy_data/s_316_feature.mat', 'toy_data/s_317_feature.mat', 'toy_data/s_318_feature.mat', 'toy_data/s_319_feature.mat', 'toy_data/s_320_feature.mat', 'toy_data/s_321_feature.mat', 'toy_data/s_322_feature.mat', 'toy_data/s_323_feature.mat', 'toy_data/s_324_feature.mat', 'toy_data/s_325_feature.mat', 'toy_data/s_326_feature.mat', 'toy_data/s_327_feature.mat', 'toy_data/s_328_feature.mat', 'toy_data/s_329_feature.mat', 'toy_data/s_330_feature.mat', 'toy_data/s_331_feature.mat', 'toy_data/s_332_feature.mat', 'toy_data/s_333_feature.mat', 'toy_data/s_334_feature.mat', 'toy_data/s_335_feature.mat', 'toy_data/s_336_feature.mat', 'toy_data/s_337_feature.mat', 'toy_data/s_338_feature.mat', 'toy_data/s_339_feature.mat', 'toy_data/s_340_feature.mat', 'toy_data/s_341_feature.mat', 'toy_data/s_342_feature.mat', 'toy_data/s_343_feature.mat', 'toy_data/s_344_feature.mat', 'toy_data/s_345_feature.mat', 'toy_data/s_346_feature.mat', 'toy_data/s_347_feature.mat', 'toy_data/s_348_feature.mat', 'toy_data/s_349_feature.mat', 'toy_data/s_350_feature.mat', 'toy_data/s_351_feature.mat', 'toy_data/s_352_feature.mat', 'toy_data/s_353_feature.mat', 'toy_data/s_354_feature.mat', 'toy_data/s_355_feature.mat', 'toy_data/s_356_feature.mat', 'toy_data/s_357_feature.mat', 'toy_data/s_358_feature.mat', 'toy_data/s_359_feature.mat', 'toy_data/s_360_feature.mat', 'toy_data/s_361_feature.mat', 'toy_data/s_362_feature.mat', 'toy_data/s_363_feature.mat', 'toy_data/s_364_feature.mat', 'toy_data/s_365_feature.mat', 'toy_data/s_366_feature.mat', 'toy_data/s_367_feature.mat', 'toy_data/s_368_feature.mat', 'toy_data/s_369_feature.mat', 'toy_data/s_370_feature.mat', 'toy_data/s_371_feature.mat', 'toy_data/s_372_feature.mat', 'toy_data/s_373_feature.mat', 'toy_data/s_374_feature.mat', 'toy_data/s_375_feature.mat', 'toy_data/s_376_feature.mat', 'toy_data/s_377_feature.mat', 'toy_data/s_378_feature.mat', 'toy_data/s_379_feature.mat', 'toy_data/s_380_feature.mat', 'toy_data/s_381_feature.mat', 'toy_data/s_382_feature.mat', 'toy_data/s_383_feature.mat', 'toy_data/s_384_feature.mat', 'toy_data/s_385_feature.mat', 'toy_data/s_386_feature.mat', 'toy_data/s_387_feature.mat', 'toy_data/s_388_feature.mat', 'toy_data/s_389_feature.mat', 'toy_data/s_390_feature.mat', 'toy_data/s_391_feature.mat', 'toy_data/s_392_feature.mat', 'toy_data/s_393_feature.mat', 'toy_data/s_394_feature.mat', 'toy_data/s_395_feature.mat', 'toy_data/s_396_feature.mat', 'toy_data/s_397_feature.mat', 'toy_data/s_398_feature.mat', 'toy_data/s_399_feature.mat', 'toy_data/s_400_feature.mat', 'toy_data/s_401_feature.mat', 'toy_data/s_402_feature.mat', 'toy_data/s_403_feature.mat', 'toy_data/s_404_feature.mat', 'toy_data/s_405_feature.mat', 'toy_data/s_406_feature.mat', 'toy_data/s_407_feature.mat', 'toy_data/s_408_feature.mat', 'toy_data/s_409_feature.mat', 'toy_data/s_410_feature.mat', 'toy_data/s_411_feature.mat', 'toy_data/s_412_feature.mat', 'toy_data/s_413_feature.mat', 'toy_data/s_414_feature.mat', 'toy_data/s_415_feature.mat', 'toy_data/s_416_feature.mat', 'toy_data/s_417_feature.mat', 'toy_data/s_418_feature.mat', 'toy_data/s_419_feature.mat', 'toy_data/s_420_feature.mat', 'toy_data/s_421_feature.mat', 'toy_data/s_422_feature.mat', 'toy_data/s_423_feature.mat', 'toy_data/s_424_feature.mat', 'toy_data/s_425_feature.mat', 'toy_data/s_426_feature.mat', 'toy_data/s_427_feature.mat', 'toy_data/s_428_feature.mat', 'toy_data/s_429_feature.mat', 'toy_data/s_430_feature.mat', 'toy_data/s_431_feature.mat', 'toy_data/s_432_feature.mat', 'toy_data/s_433_feature.mat', 'toy_data/s_434_feature.mat', 'toy_data/s_435_feature.mat', 'toy_data/s_436_feature.mat', 'toy_data/s_437_feature.mat', 'toy_data/s_438_feature.mat', 'toy_data/s_439_feature.mat', 'toy_data/s_440_feature.mat', 'toy_data/s_441_feature.mat', 'toy_data/s_442_feature.mat', 'toy_data/s_443_feature.mat', 'toy_data/s_444_feature.mat', 'toy_data/s_445_feature.mat', 'toy_data/s_446_feature.mat', 'toy_data/s_447_feature.mat', 'toy_data/s_448_feature.mat', 'toy_data/s_449_feature.mat', 'toy_data/s_450_feature.mat', 'toy_data/s_451_feature.mat', 'toy_data/s_452_feature.mat', 'toy_data/s_453_feature.mat', 'toy_data/s_454_feature.mat', 'toy_data/s_455_feature.mat', 'toy_data/s_456_feature.mat', 'toy_data/s_457_feature.mat', 'toy_data/s_458_feature.mat', 'toy_data/s_459_feature.mat', 'toy_data/s_460_feature.mat', 'toy_data/s_461_feature.mat', 'toy_data/s_462_feature.mat', 'toy_data/s_463_feature.mat', 'toy_data/s_464_feature.mat', 'toy_data/s_465_feature.mat', 'toy_data/s_466_feature.mat', 'toy_data/s_467_feature.mat', 'toy_data/s_468_feature.mat', 'toy_data/s_469_feature.mat', 'toy_data/s_470_feature.mat', 'toy_data/s_471_feature.mat', 'toy_data/s_472_feature.mat', 'toy_data/s_473_feature.mat', 'toy_data/s_474_feature.mat', 'toy_data/s_475_feature.mat', 'toy_data/s_476_feature.mat', 'toy_data/s_477_feature.mat', 'toy_data/s_478_feature.mat', 'toy_data/s_479_feature.mat', 'toy_data/s_480_feature.mat', 'toy_data/s_481_feature.mat', 'toy_data/s_482_feature.mat', 'toy_data/s_483_feature.mat', 'toy_data/s_484_feature.mat', 'toy_data/s_485_feature.mat', 'toy_data/s_486_feature.mat', 'toy_data/s_487_feature.mat', 'toy_data/s_488_feature.mat', 'toy_data/s_489_feature.mat', 'toy_data/s_490_feature.mat', 'toy_data/s_491_feature.mat', 'toy_data/s_492_feature.mat', 'toy_data/s_493_feature.mat', 'toy_data/s_494_feature.mat', 'toy_data/s_495_feature.mat', 'toy_data/s_496_feature.mat', 'toy_data/s_497_feature.mat', 'toy_data/s_498_feature.mat', 'toy_data/s_499_feature.mat', 'toy_data/s_500_feature.mat']\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"toy_data/\"\n",
    "all_files = glob(data_dir +'*.mat')\n",
    "print((all_files))\n",
    "\n",
    "label_file = [file_name if file_name.split('.')[0].split('/')[-1] == 'label' else None for file_name in all_files ]\n",
    "label_file = [file_name for file_name in label_file if file_name is not None]\n",
    "\n",
    "label_file\n",
    "\n",
    "\n",
    "feature_file = [file_name if file_name.split('.')[0].split('_')[-1] == 'feature' else None for file_name in all_files ]\n",
    "feature_file = [file_name for file_name in feature_file if file_name is not None]\n",
    "sorted_feature_file = sorted(feature_file, key=lambda x: int(x.split('.')[0].split('_')[2]))\n",
    "print(sorted_feature_file)\n",
    "\n",
    "# len(feature_file)\n",
    "\n",
    "cluster_file = [file_name if file_name.split('.')[0].split('_')[-1] == 'index' else None for file_name in all_files ]\n",
    "cluster_file = [file_name for file_name in cluster_file if file_name is not None]\n",
    "sorted_cluster_file = sorted(cluster_file, key=lambda x: int(x.split('.')[0].split('_')[2]))\n",
    "# print(sorted_cluster_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d48fab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "531011f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame using dictionary - each key becomes a column\n",
    "# sorted_feature_file and sorted_cluster_file are lists of file paths (500 elements each)\n",
    "# labels is a numpy array of shape (500, 1), so we flatten it to (500,)\n",
    "df = pd.DataFrame({\n",
    "    'feature_file': sorted_feature_file,\n",
    "    'cluster_file': sorted_cluster_file,\n",
    "    'label': labels.flatten()  # Flatten from (500, 1) to (500,)\n",
    "})\n",
    "# len(df)\n",
    "# df.to_csv('data.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8ae3e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "237b384c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset: 500 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 500/500 [00:09<00:00, 55.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 2 different shapes:\n",
      "  Shape (399, 1632): 14 files\n",
      "NON-STANDARD! First 5 files:\n",
      "      - Row 3: toy_data/s_4_feature.mat\n",
      "      - Row 5: toy_data/s_6_feature.mat\n",
      "      - Row 172: toy_data/s_173_feature.mat\n",
      "      - Row 174: toy_data/s_175_feature.mat\n",
      "      - Row 210: toy_data/s_211_feature.mat\n",
      "  Shape (400, 1632): 486 files\n",
      "Cleaned dataset: 486 samples (removed 14 files)\n",
      "Original: 500  Cleaned: 486\n",
      "Train set: 340 samples (70.0%)\n",
      "Validation set: 73 samples (15.0%)\n",
      "Test set: 73 samples (15.0%)\n",
      "\n",
      "Train label distribution:\n",
      "label\n",
      "1    171\n",
      "2    169\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Val label distribution:\n",
      "label\n",
      "1    37\n",
      "2    36\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Test label distribution:\n",
      "label\n",
      "1    37\n",
      "2    36\n",
      "Name: count, dtype: int64\n",
      "\n",
      " Problematic files saved to 'toy_data/split/problematic_files.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load the original dataframe\n",
    "from tqdm import tqdm\n",
    "df = pd.read_csv('toy_data/split/data.csv')\n",
    "print(f\"Original dataset: {len(df)} samples\")\n",
    "\n",
    "shape_dict = {}\n",
    "problematic_indices = []\n",
    "expected_shape = (400, 1632)\n",
    "\n",
    "for idx in tqdm(range(len(df))):\n",
    "    row = df.iloc[idx]\n",
    "    feature_file = row['feature']\n",
    "    \n",
    "    try:\n",
    "        f_data = sio.loadmat(feature_file)\n",
    "        f_mat = f_data['feature_mat']\n",
    "        shape = f_mat.shape\n",
    "        \n",
    "        if shape not in shape_dict:\n",
    "            shape_dict[shape] = []\n",
    "        shape_dict[shape].append(idx)\n",
    "        \n",
    "        # Check if shape doesn't match expected\n",
    "        if shape[0] != expected_shape[0]:\n",
    "            problematic_indices.append(idx)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {feature_file}: {e}\")\n",
    "        problematic_indices.append(idx)\n",
    "\n",
    "print(f\"\\nFound {len(shape_dict)} different shapes:\")\n",
    "for shape, indices in sorted(shape_dict.items()):\n",
    "    print(f\"  Shape {shape}: {len(indices)} files\")\n",
    "    if shape[0] != expected_shape[0]:\n",
    "        print(f\"NON-STANDARD! First 5 files:\")\n",
    "        for i in indices[:5]:\n",
    "            print(f\"      - Row {i}: {df.iloc[i]['feature']}\")\n",
    "\n",
    "# print(f\"FILTERING OUT {len(problematic_indices)} PROBLEMATIC FILES...\")\n",
    "\n",
    "df_clean = df.drop(index=problematic_indices).reset_index(drop=True)\n",
    "print(f\"Cleaned dataset: {len(df_clean)} samples (removed {len(problematic_indices)} files)\")\n",
    "print(f\"Original: {len(df)}  Cleaned: {len(df_clean)}\")\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "train_val_df, test_df = train_test_split(\n",
    "    df_clean, \n",
    "    test_size=0.15, \n",
    "    random_state=RANDOM_SEED,\n",
    "    stratify=df_clean['label'] \n",
    ")\n",
    "\n",
    "train_df, val_df = train_test_split(\n",
    "    train_val_df,\n",
    "    test_size=15/85,  \n",
    "    random_state=RANDOM_SEED,\n",
    "    stratify=train_val_df['label']  \n",
    ")\n",
    "\n",
    "\n",
    "print(f\"Train set: {len(train_df)} samples ({len(train_df)/len(df_clean)*100:.1f}%)\")\n",
    "print(f\"Validation set: {len(val_df)} samples ({len(val_df)/len(df_clean)*100:.1f}%)\")\n",
    "print(f\"Test set: {len(test_df)} samples ({len(test_df)/len(df_clean)*100:.1f}%)\")\n",
    "print(f\"\\nTrain label distribution:\\n{train_df['label'].value_counts().sort_index()}\")\n",
    "print(f\"\\nVal label distribution:\\n{val_df['label'].value_counts().sort_index()}\")\n",
    "print(f\"\\nTest label distribution:\\n{test_df['label'].value_counts().sort_index()}\")\n",
    "\n",
    "# Save cleaned splits\n",
    "train_df.to_csv('toy_data/split/train_df.csv', index=False)\n",
    "val_df.to_csv('toy_data/split/val_df.csv', index=False)\n",
    "test_df.to_csv('toy_data/split/test_df.csv', index=False)\n",
    "\n",
    "# Save the problematic files list for reference\n",
    "if problematic_indices:\n",
    "    problematic_df = df.iloc[problematic_indices]\n",
    "    problematic_df.to_csv('toy_data/split/problematic_files.csv', index=False)\n",
    "    print(f\"\\n Problematic files saved to 'toy_data/split/problematic_files.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d4124f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>cluster</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>toy_data/s_119_feature.mat</td>\n",
       "      <td>toy_data/s_119_cluster_index.mat</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>toy_data/s_437_feature.mat</td>\n",
       "      <td>toy_data/s_437_cluster_index.mat</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427</th>\n",
       "      <td>toy_data/s_441_feature.mat</td>\n",
       "      <td>toy_data/s_441_cluster_index.mat</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>toy_data/s_416_feature.mat</td>\n",
       "      <td>toy_data/s_416_cluster_index.mat</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>toy_data/s_315_feature.mat</td>\n",
       "      <td>toy_data/s_315_cluster_index.mat</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        feature                           cluster  label\n",
       "116  toy_data/s_119_feature.mat  toy_data/s_119_cluster_index.mat      1\n",
       "423  toy_data/s_437_feature.mat  toy_data/s_437_cluster_index.mat      2\n",
       "427  toy_data/s_441_feature.mat  toy_data/s_441_cluster_index.mat      2\n",
       "403  toy_data/s_416_feature.mat  toy_data/s_416_cluster_index.mat      2\n",
       "307  toy_data/s_315_feature.mat  toy_data/s_315_cluster_index.mat      2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96e817b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature file path: toy_data/s_119_feature.mat\n",
      "(400, 1632)\n",
      "cluster file path: toy_data/s_119_cluster_index.mat\n",
      "(45, 54, 45)\n",
      "Label: 1\n"
     ]
    }
   ],
   "source": [
    "sample_feature_file = train_df.iloc[0]['feature']\n",
    "print(f\"Feature file path: {sample_feature_file}\")\n",
    "sample_feature_data = sio.loadmat(sample_feature_file)\n",
    "# print(f\"Keys in feature file: {sample_feature_data.keys()}\")\n",
    "print(sample_feature_data['feature_mat'].shape)\n",
    "\n",
    "\n",
    "sample_cluster_file = train_df.iloc[0]['cluster']\n",
    "print(f\"cluster file path: {sample_cluster_file}\")\n",
    "sample_cluster_data = sio.loadmat(sample_cluster_file)\n",
    "# print(f\"Keys in feature file: {sample_cluster_data.keys()}\")\n",
    "print(sample_cluster_data['cluster_index_mat'].shape)\n",
    "\n",
    "sample_label = train_df.iloc[0]['label']\n",
    "print(f\"Label: {sample_label}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5fbf816a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.utils import data\n",
    "\n",
    "class MRIDataset(data.Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        feature_file = row['feature']\n",
    "        cluster_file = row['cluster']\n",
    "        label = row['label']\n",
    "\n",
    "        # Load .mat files\n",
    "        f_data = sio.loadmat(feature_file)\n",
    "        f_mat = f_data['feature_mat']  # Shape: (400, 1632) - ROI features\n",
    "        c_data = sio.loadmat(cluster_file)\n",
    "        c_mat = c_data['cluster_index_mat']  # Shape: (45, 54, 45) - Cluster indices\n",
    "\n",
    "        # Convert to tensors\n",
    "        f_mat = torch.FloatTensor(f_mat)  # Feature matrix\n",
    "        c_mat = torch.LongTensor(c_mat)  # Cluster indices (integers)\n",
    "        label = torch.LongTensor([label - 1])  # Convert to 0-indexed (1,2 -> 0,1)\n",
    "\n",
    "        return f_mat, c_mat, label\n",
    "\n",
    "\n",
    "train_dataset = MRIDataset(train_df)\n",
    "val_dataset = MRIDataset(val_df)\n",
    "test_dataset = MRIDataset(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3132472e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Diagnostic: Check which files have different feature matrix shapes\n",
    "# # Check ALL 500 datasets (train + val + test)\n",
    "# print(\"Checking feature matrix shapes across ALL datasets (train + val + test)...\")\n",
    "# print(\"=\" * 70)\n",
    "\n",
    "# shape_dict = {}\n",
    "# problematic_files = []\n",
    "# all_dfs = {'train': train_df, 'val': val_df, 'test': test_df}\n",
    "\n",
    "# # Check each split\n",
    "# for split_name, split_df in all_dfs.items():\n",
    "#     print(f\"\\n[{split_name.upper()}] Checking {len(split_df)} files...\")\n",
    "    \n",
    "#     for idx in range(len(split_df)):\n",
    "#         row = split_df.iloc[idx]\n",
    "#         feature_file = row['feature']\n",
    "        \n",
    "#         try:\n",
    "#             f_data = sio.loadmat(feature_file)\n",
    "#             f_mat = f_data['feature_mat']\n",
    "#             shape = f_mat.shape\n",
    "            \n",
    "#             if shape not in shape_dict:\n",
    "#                 shape_dict[shape] = []\n",
    "#             shape_dict[shape].append((feature_file, split_name))\n",
    "            \n",
    "#             # Check if shape is not (400, 1632)\n",
    "#             if shape[0] != 400:\n",
    "#                 problematic_files.append((feature_file, shape, split_name))\n",
    "#         except Exception as e:\n",
    "#             print(f\"   Error loading {feature_file}: {e}\")\n",
    "\n",
    "# print(\"\\n\" + \"=\" * 70)\n",
    "# print(f\"\\n SUMMARY: Found {len(shape_dict)} different shapes across all datasets:\")\n",
    "# print(\"=\" * 70)\n",
    "\n",
    "# for shape, file_list in sorted(shape_dict.items()):\n",
    "#     print(f\"\\n  Shape {shape}: {len(file_list)} files\")\n",
    "#     # Count by split\n",
    "#     split_counts = {}\n",
    "#     for file, split in file_list:\n",
    "#         split_counts[split] = split_counts.get(split, 0) + 1\n",
    "#     print(f\"    Distribution: {split_counts}\")\n",
    "    \n",
    "#     # Show files if there are few (<= 10)\n",
    "#     if len(file_list) <= 10:\n",
    "#         for file, split in file_list:\n",
    "#             print(f\"      [{split}] {file}\")\n",
    "\n",
    "# if problematic_files:\n",
    "#     print(\"\\n\" + \"=\" * 70)\n",
    "#     print(f\"  FOUND {len(problematic_files)} FILES WITH NON-STANDARD SHAPES:\")\n",
    "#     print(\"=\" * 70)\n",
    "    \n",
    "#     # Group by shape\n",
    "#     by_shape = {}\n",
    "#     for file, shape, split in problematic_files:\n",
    "#         if shape not in by_shape:\n",
    "#             by_shape[shape] = []\n",
    "#         by_shape[shape].append((file, split))\n",
    "    \n",
    "#     for shape, file_list in sorted(by_shape.items()):\n",
    "#         print(f\"\\n  Shape {shape} ({len(file_list)} files):\")\n",
    "#         for file, split in file_list:\n",
    "#             print(f\"    [{split}] {file}\")\n",
    "# else:\n",
    "#     print(\"\\n All files have the expected shape (400, 1632)!\")\n",
    "\n",
    "# # Save problematic files to a list for easy access\n",
    "# if problematic_files:\n",
    "#     print(\"\\n\" + \"=\" * 70)\n",
    "#     print(\" Problematic files saved to 'problematic_files' variable\")\n",
    "#     print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "173b64b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = data.DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = data.DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "test_loader = data.DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "ex = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7844f8d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 400, 1632])\n",
      "torch.Size([16, 45, 54, 45])\n",
      "torch.Size([16, 1])\n"
     ]
    }
   ],
   "source": [
    "feature = ex[0]\n",
    "cluster = ex[1]\n",
    "label = ex[2]\n",
    "\n",
    "print(feature.shape)\n",
    "print(cluster.shape)\n",
    "print(label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441f98ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "03ba24ae",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 316\u001b[39m\n\u001b[32m    314\u001b[39m \u001b[38;5;66;03m# Create model instance\u001b[39;00m\n\u001b[32m    315\u001b[39m config = ModelConfig()\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m model = \u001b[43mBrainNet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[38;5;66;03m# Print model summary\u001b[39;00m\n\u001b[32m    319\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel created with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msum\u001b[39m(p.numel()\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mp\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mmodel.parameters())/\u001b[32m10e6\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m parameters\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 119\u001b[39m, in \u001b[36mBrainNet.__init__\u001b[39m\u001b[34m(self, config)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;28mself\u001b[39m.roi_projection = nn.Linear(config.feature_dim, config.hidden_dim)\n\u001b[32m    115\u001b[39m \u001b[38;5;66;03m# Step 2-3: Spatial Block Tokenization will be done in forward pass\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;66;03m# We'll use adaptive pooling to handle variable spatial dimensions\u001b[39;00m\n\u001b[32m    117\u001b[39m \n\u001b[32m    118\u001b[39m \u001b[38;5;66;03m# Step 4: Transformer Encoder\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m \u001b[38;5;28mself\u001b[39m.transformer_blocks = nn.ModuleList(\u001b[43m[\u001b[49m\n\u001b[32m    120\u001b[39m \u001b[43m    \u001b[49m\u001b[43mTransformerBlock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhidden_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mintermediate_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[32m    129\u001b[39m \u001b[38;5;66;03m# Step 5: Global Readout & Classification\u001b[39;00m\n\u001b[32m    130\u001b[39m \u001b[38;5;28mself\u001b[39m.global_pool = nn.AdaptiveAvgPool1d(\u001b[32m1\u001b[39m)  \u001b[38;5;66;03m# Global average pooling\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 120\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;28mself\u001b[39m.roi_projection = nn.Linear(config.feature_dim, config.hidden_dim)\n\u001b[32m    115\u001b[39m \u001b[38;5;66;03m# Step 2-3: Spatial Block Tokenization will be done in forward pass\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;66;03m# We'll use adaptive pooling to handle variable spatial dimensions\u001b[39;00m\n\u001b[32m    117\u001b[39m \n\u001b[32m    118\u001b[39m \u001b[38;5;66;03m# Step 4: Transformer Encoder\u001b[39;00m\n\u001b[32m    119\u001b[39m \u001b[38;5;28mself\u001b[39m.transformer_blocks = nn.ModuleList([\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m     \u001b[43mTransformerBlock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhidden_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mintermediate_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    126\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config.num_layers)\n\u001b[32m    127\u001b[39m ])\n\u001b[32m    129\u001b[39m \u001b[38;5;66;03m# Step 5: Global Readout & Classification\u001b[39;00m\n\u001b[32m    130\u001b[39m \u001b[38;5;28mself\u001b[39m.global_pool = nn.AdaptiveAvgPool1d(\u001b[32m1\u001b[39m)  \u001b[38;5;66;03m# Global average pooling\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 75\u001b[39m, in \u001b[36mTransformerBlock.__init__\u001b[39m\u001b[34m(self, d_model, num_heads, d_ff, dropout)\u001b[39m\n\u001b[32m     72\u001b[39m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m()\n\u001b[32m     74\u001b[39m \u001b[38;5;28mself\u001b[39m.norm1 = nn.LayerNorm(d_model)\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m \u001b[38;5;28mself\u001b[39m.attn = \u001b[43mMultiHeadAttention\u001b[49m\u001b[43m(\u001b[49m\u001b[43md_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[38;5;28mself\u001b[39m.norm2 = nn.LayerNorm(d_model)\n\u001b[32m     78\u001b[39m \u001b[38;5;28mself\u001b[39m.ffn = nn.Sequential(\n\u001b[32m     79\u001b[39m     nn.Linear(d_model, d_ff),\n\u001b[32m     80\u001b[39m     nn.GELU(),\n\u001b[32m   (...)\u001b[39m\u001b[32m     83\u001b[39m     nn.Dropout(dropout)\n\u001b[32m     84\u001b[39m )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mMultiHeadAttention.__init__\u001b[39m\u001b[34m(self, d_model, num_heads, dropout)\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, d_model, num_heads, dropout=\u001b[32m0.1\u001b[39m):\n\u001b[32m     24\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m()\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m d_model % num_heads == \u001b[32m0\u001b[39m\n\u001b[32m     27\u001b[39m     \u001b[38;5;28mself\u001b[39m.d_model = d_model\n\u001b[32m     28\u001b[39m     \u001b[38;5;28mself\u001b[39m.num_heads = num_heads\n",
      "\u001b[31mAssertionError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import einops\n",
    "\n",
    "class ModelConfig:\n",
    "    \"\"\"Configuration for Atlas-free Brain Network Transformer\"\"\"\n",
    "    feature_dim = 1632  # ROI feature dimension\n",
    "    num_rois = 400  # Number of ROIs\n",
    "    cluster_shape = (45, 54, 45)  # 3D cluster index shape\n",
    "    hidden_dim = 128  # Latent dimension V (projection dimension)\n",
    "    num_heads = 6  # Number of attention heads\n",
    "    num_layers = 12  # Number of transformer layers\n",
    "    intermediate_dim = 128*4  # Feed-forward network dimension\n",
    "    dropout = 0.1\n",
    "    block_size = 9  # Spatial block size (K x K x K)\n",
    "    block_stride = 5  # Stride for block pooling\n",
    "    output_dim = 2  # Binary classification\n",
    "\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Standard Multi-Head Self-Attention\"\"\"\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        \n",
    "        self.w_q = nn.Linear(d_model, d_model)\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model)\n",
    "        self.w_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, seq_len, d_model = x.size()\n",
    "        \n",
    "        # Linear projections\n",
    "        Q = self.w_q(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = self.w_k(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = self.w_v(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # Scaled dot-product attention\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) * self.scale\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        attn_output = torch.matmul(attn_weights, V)\n",
    "        \n",
    "        # Concatenate heads\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(\n",
    "            batch_size, seq_len, d_model\n",
    "        )\n",
    "        \n",
    "        # Final linear projection\n",
    "        output = self.w_o(attn_output)\n",
    "        return output\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"Standard Transformer Encoder Block\"\"\"\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_ff, d_model),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        # Self-attention with residual connection\n",
    "        x = x + self.dropout(self.attn(self.norm1(x), mask))\n",
    "        # Feed-forward with residual connection\n",
    "        x = x + self.ffn(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class BrainNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Atlas-free Brain Network Transformer\n",
    "    \n",
    "    Architecture:\n",
    "    1. ROI Connectivity Projection (g  q): Linear projection from feature_dim to hidden_dim\n",
    "    2. 3D Multi-channel Brain Map Construction (Q): Map ROI features to voxel space using cluster indices\n",
    "    3. Spatial Block Tokenization: Divide 3D volume into blocks and pool\n",
    "    4. Transformer Encoder: Process spatial tokens\n",
    "    5. Global Readout & Classification: Mean pool and classify\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "\n",
    "        # Step 1: ROI Connectivity Projection (g  q)\n",
    "        # Simple linear projection as per paper (can be extended to MLP if needed)\n",
    "        self.roi_projection = nn.Linear(config.feature_dim, config.hidden_dim)\n",
    "        \n",
    "        # Step 2-3: Spatial Block Tokenization will be done in forward pass\n",
    "        # We'll use adaptive pooling to handle variable spatial dimensions\n",
    "        \n",
    "        # Step 4: Transformer Encoder\n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "            TransformerBlock(\n",
    "                config.hidden_dim,\n",
    "                config.num_heads,\n",
    "                config.intermediate_dim,\n",
    "                config.dropout\n",
    "            )\n",
    "            for _ in range(config.num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Step 5: Global Readout & Classification\n",
    "        self.global_pool = nn.AdaptiveAvgPool1d(1)  # Global average pooling\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(config.hidden_dim),\n",
    "            nn.Dropout(config.dropout),\n",
    "            nn.Linear(config.hidden_dim, config.output_dim)\n",
    "        )\n",
    "    \n",
    "    def _construct_3d_brain_map(self, q, c_mat):\n",
    "        \"\"\"\n",
    "        Construct 3D Multi-channel Brain Map (Q) from ROI features and cluster indices.\n",
    "        \n",
    "        The cluster index matrix C contains:\n",
    "        - 0: background (should remain zero in output)\n",
    "        - 1-400: ROI indices (i-th ROI corresponds to (i-1)-th row of feature matrix F)\n",
    "        \n",
    "        The feature matrix F has shape [400, feature_dim], where:\n",
    "        - Row 0 corresponds to ROI 1\n",
    "        - Row 1 corresponds to ROI 2\n",
    "        - ...\n",
    "        - Row 399 corresponds to ROI 400\n",
    "        \n",
    "        Args:\n",
    "            q: Projected ROI features [batch_size, num_rois, hidden_dim]\n",
    "               q[b, i] is the projected feature for ROI (i+1) (i-th row of F)\n",
    "            c_mat: Cluster index matrix [batch_size, D, H, W]\n",
    "                   Contains ROI indices: 0 (background), 1-400 (ROI indices)\n",
    "        \n",
    "        Returns:\n",
    "            Q: 3D brain map [batch_size, hidden_dim, D, H, W]\n",
    "               Background voxels (c_mat == 0) remain zero in Q\n",
    "        \"\"\"\n",
    "        batch_size, num_rois, hidden_dim = q.shape\n",
    "        D, H, W = c_mat.shape[1], c_mat.shape[2], c_mat.shape[3]\n",
    "        \n",
    "        # Initialize output tensor (background will remain zero)\n",
    "        Q = torch.zeros(batch_size, hidden_dim, D, H, W, \n",
    "                        device=q.device, dtype=q.dtype)\n",
    "        \n",
    "        # Process each sample in the batch\n",
    "        for b in range(batch_size):\n",
    "            # Get cluster indices for this batch [D, H, W]\n",
    "            cluster_indices = c_mat[b]  # Contains: 0 (background), 1-400 (ROI indices)\n",
    "            \n",
    "            # Get projected features for this batch [num_rois, hidden_dim]\n",
    "            # q[b, i] corresponds to ROI (i+1), so q[b, 0] = ROI 1, q[b, 399] = ROI 400\n",
    "            roi_features = q[b]\n",
    "            \n",
    "            # Flatten spatial dimensions to get all voxel positions\n",
    "            cluster_indices_flat = cluster_indices.flatten()  # [D*H*W]\n",
    "            \n",
    "            # Create mask for non-background voxels (ROI indices 1-400)\n",
    "            non_bg_mask = (cluster_indices_flat > 0) & (cluster_indices_flat <= num_rois)\n",
    "            \n",
    "            # Convert ROI indices from 1-based (1-400) to 0-based (0-399) for indexing into q\n",
    "            # ROI 1 -> index 0, ROI 2 -> index 1, ..., ROI 400 -> index 399\n",
    "            roi_indices = (cluster_indices_flat[non_bg_mask] - 1).long()\n",
    "            \n",
    "            # Map non-background voxels to their corresponding ROI feature vectors\n",
    "            # Background voxels (value 0) remain zero in Q\n",
    "            voxel_features = torch.zeros(D * H * W, hidden_dim, \n",
    "                                        device=q.device, dtype=q.dtype)\n",
    "            voxel_features[non_bg_mask] = roi_features[roi_indices]  # [num_non_bg, hidden_dim]\n",
    "            \n",
    "            # Reshape back to 3D spatial structure\n",
    "            # Transpose to get [hidden_dim, D, H, W] format\n",
    "            Q[b] = voxel_features.view(D, H, W, hidden_dim).permute(3, 0, 1, 2)\n",
    "        \n",
    "        return Q\n",
    "        \n",
    "    def forward(self, f_mat, c_mat):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        \n",
    "        Args:\n",
    "            f_mat: Feature matrix [batch_size, num_rois, feature_dim] - ROI features\n",
    "            c_mat: Cluster index matrix [batch_size, D, H, W] - 3D cluster indices\n",
    "        \n",
    "        Returns:\n",
    "            logits: [batch_size, output_dim]\n",
    "        \"\"\"\n",
    "        batch_size = f_mat.size(0)\n",
    "        \n",
    "        # Step 1: ROI Connectivity Projection (g  q)\n",
    "        # f_mat: [B, num_rois, feature_dim]  [B, num_rois, hidden_dim]\n",
    "        q = self.roi_projection(f_mat)  # [B, num_rois, hidden_dim]\n",
    "        \n",
    "        # Step 2: 3D Multi-channel Brain Map Construction (Q)\n",
    "        # Map ROI features back to voxel space using cluster indices\n",
    "        # Each voxel position in c_mat contains an ROI index (1-400)\n",
    "        # This function maps the corresponding ROI feature vector to each voxel position\n",
    "        Q = self._construct_3d_brain_map(q, c_mat)  # [B, hidden_dim, D, H, W]\n",
    "        \n",
    "        D, H, W = Q.shape[2], Q.shape[3], Q.shape[4]\n",
    "        \n",
    "        # Step 3: Spatial Block Tokenization\n",
    "        # ====================================\n",
    "        # This step divides the 3D brain volume into smaller blocks and creates \"tokens\" \n",
    "        # (like words in a sentence) for the Transformer to process.\n",
    "        #\n",
    "        # INPUT: Q [B, hidden_dim, D=45, H=54, W=45] - 3D brain map with features at each voxel\n",
    "        #\n",
    "        # PROCESS:\n",
    "        # 1. Divide volume into overlapping blocks of size block_sizeblock_sizeblock_size (e.g., 999)\n",
    "        # 2. For each block, sum all the feature values within that block\n",
    "        # 3. This creates one \"token\" per block\n",
    "        #\n",
    "        # EXAMPLE (simplified 2D):\n",
    "        #   Original: [4554] voxels\n",
    "        #   Blocks: 99 blocks with stride 5\n",
    "        #   Output: ~810 blocks = 80 tokens\n",
    "        #\n",
    "        # WHY SUM-POOLING?\n",
    "        # The paper uses sum-pooling (not average) because:\n",
    "        # - It preserves the total \"activation\" in each spatial region\n",
    "        # - More sensitive to the amount of information in each block\n",
    "        # - Better for capturing spatial patterns\n",
    "        \n",
    "        # Use 3D average pooling with kernel=block_size and stride=block_stride\n",
    "        # PyTorch doesn't have direct sum-pooling, so we:\n",
    "        # 1. Use avg_pool3d to get average values in each block\n",
    "        # 2. Multiply by block_size^3 to convert to sum\n",
    "        #    (sum = average  number_of_elements_in_block)\n",
    "        #    For 999 block: sum = avg  729\n",
    "        Q_pooled = F.avg_pool3d(\n",
    "            Q,\n",
    "            kernel_size=self.config.block_size,  # e.g., 9 (creates 999 blocks)\n",
    "            stride=self.config.block_stride,     # e.g., 5 (blocks overlap)\n",
    "            padding=0  # No padding - only process valid blocks\n",
    "        ) * (self.config.block_size ** 3)  # Convert avg  sum: multiply by voxels per block\n",
    "        \n",
    "        # Get output dimensions after pooling\n",
    "        # For D=45, H=54, W=45 with block_size=9, stride=5:\n",
    "        #   D_out = (45 - 9) // 5 + 1 = 8\n",
    "        #   H_out = (54 - 9) // 5 + 1 = 10  \n",
    "        #   W_out = (45 - 9) // 5 + 1 = 8\n",
    "        #   Total tokens = 8  10  8 = 640 tokens\n",
    "        D_out, H_out, W_out = Q_pooled.shape[2], Q_pooled.shape[3], Q_pooled.shape[4]\n",
    "        \n",
    "        # Flatten spatial dimensions to create sequence of tokens for Transformer\n",
    "        # Q_pooled: [B, hidden_dim, D_out, H_out, W_out]\n",
    "        # tokens: [B, num_tokens, hidden_dim] - each token is a spatial block\n",
    "        num_tokens = D_out * H_out * W_out\n",
    "        tokens = Q_pooled.view(batch_size, self.config.hidden_dim, num_tokens)\n",
    "        tokens = tokens.transpose(1, 2)  # [B, num_tokens, hidden_dim]\n",
    "        \n",
    "        # Now we have a sequence of tokens where each token represents a spatial block\n",
    "        # The Transformer will process these tokens to learn relationships between\n",
    "        # different brain regions (spatial blocks)\n",
    "        \n",
    "        # Step 4: Transformer Encoder\n",
    "        # Process the sequence of spatial tokens through transformer blocks\n",
    "        # Each token can now \"attend\" to all other tokens to learn relationships\n",
    "        x = tokens  # [B, num_tokens, hidden_dim]\n",
    "        for transformer_block in self.transformer_blocks:\n",
    "            x = transformer_block(x)  # Still [B, num_tokens, hidden_dim]\n",
    "        \n",
    "        # Step 5: Global Readout & Classification\n",
    "        # ======================================\n",
    "        # GOAL: Convert sequence of tokens  single feature vector per sample\n",
    "        #\n",
    "        # After transformer: x = [B, num_tokens=640, hidden_dim=64]\n",
    "        # We have 640 tokens, each with 64 features\n",
    "        # We want: [B, hidden_dim=64] - one vector representing the entire brain\n",
    "        #\n",
    "        # WHY TRANSPOSE?\n",
    "        # - AdaptiveAvgPool1d pools over the LAST dimension\n",
    "        # - We want to pool over tokens (dimension 1), not features (dimension 2)\n",
    "        # - So we transpose: [B, num_tokens, hidden_dim]  [B, hidden_dim, num_tokens]\n",
    "        # - Now tokens are the last dimension, so pooling works correctly\n",
    "        #\n",
    "        # WHAT DOES GLOBAL POOLING DO?\n",
    "        # - Takes average of all 640 tokens\n",
    "        # - Result: One feature vector per sample representing the whole brain\n",
    "        # - This is the \"readout\" operation - aggregating all spatial information\n",
    "        x = x.transpose(1, 2)  # [B, hidden_dim, num_tokens] - swap dims for pooling\n",
    "        x = self.global_pool(x).squeeze(-1)  # [B, hidden_dim, 1]  [B, hidden_dim]\n",
    "        \n",
    "        # Classification\n",
    "        # Now we have one feature vector per brain, ready for classification\n",
    "        logits = self.classifier(x)  # [B, hidden_dim]  [B, output_dim=2]\n",
    "        \n",
    "        return logits\n",
    "\n",
    "\n",
    "# Create model instance\n",
    "config = ModelConfig()\n",
    "model = BrainNet(config)\n",
    "\n",
    "# Print model summary\n",
    "print(f\"Model created with {sum(p.numel() for p in model.parameters())/10e6} parameters\")\n",
    "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)/10e6}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98ffb1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Starting Training\n",
      "======================================================================\n",
      "Model parameters: 6.74M\n",
      "Training samples: 340\n",
      "Validation samples: 73\n",
      "Number of epochs: 50\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 [Train]: 100%|| 22/22 [00:14<00:00,  1.57it/s, loss=0.7856, acc=50.88%]\n",
      "Epoch 1/50 [Val]: 100%|| 5/5 [00:01<00:00,  3.08it/s, loss=0.7177, acc=52.05%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/50:\n",
      "  Train Loss: 0.7124 | Train Acc: 50.88%\n",
      "  Val Loss:   0.7140 | Val Acc:   52.05%\n",
      "   Saved best model (Val Acc: 52.05%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/50 [Train]: 100%|| 22/22 [00:11<00:00,  1.95it/s, loss=0.6281, acc=60.88%]\n",
      "Epoch 2/50 [Val]: 100%|| 5/5 [00:01<00:00,  3.07it/s, loss=0.7165, acc=52.05%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/50:\n",
      "  Train Loss: 0.6530 | Train Acc: 60.88%\n",
      "  Val Loss:   0.6983 | Val Acc:   52.05%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/50 [Train]: 100%|| 22/22 [00:11<00:00,  1.94it/s, loss=0.6283, acc=67.35%]\n",
      "Epoch 3/50 [Val]: 100%|| 5/5 [00:01<00:00,  3.12it/s, loss=0.7074, acc=54.79%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/50:\n",
      "  Train Loss: 0.6076 | Train Acc: 67.35%\n",
      "  Val Loss:   0.6984 | Val Acc:   54.79%\n",
      "   Saved best model (Val Acc: 54.79%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/50 [Train]: 100%|| 22/22 [00:11<00:00,  1.95it/s, loss=0.3564, acc=75.00%]\n",
      "Epoch 4/50 [Val]: 100%|| 5/5 [00:01<00:00,  3.13it/s, loss=0.6769, acc=54.79%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4/50:\n",
      "  Train Loss: 0.5613 | Train Acc: 75.00%\n",
      "  Val Loss:   0.6972 | Val Acc:   54.79%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/50 [Train]: 100%|| 22/22 [00:11<00:00,  1.96it/s, loss=0.4617, acc=77.35%]\n",
      "Epoch 5/50 [Val]: 100%|| 5/5 [00:01<00:00,  3.02it/s, loss=0.7370, acc=49.32%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5/50:\n",
      "  Train Loss: 0.5404 | Train Acc: 77.35%\n",
      "  Val Loss:   0.7233 | Val Acc:   49.32%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/50 [Train]: 100%|| 22/22 [00:11<00:00,  1.95it/s, loss=0.6477, acc=75.59%]\n",
      "Epoch 6/50 [Val]: 100%|| 5/5 [00:01<00:00,  3.01it/s, loss=0.6495, acc=58.90%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6/50:\n",
      "  Train Loss: 0.5330 | Train Acc: 75.59%\n",
      "  Val Loss:   0.7069 | Val Acc:   58.90%\n",
      "   Saved best model (Val Acc: 58.90%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/50 [Train]: 100%|| 22/22 [00:11<00:00,  1.91it/s, loss=0.4190, acc=77.06%]\n",
      "Epoch 7/50 [Val]: 100%|| 5/5 [00:01<00:00,  2.94it/s, loss=0.6221, acc=63.01%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7/50:\n",
      "  Train Loss: 0.4844 | Train Acc: 77.06%\n",
      "  Val Loss:   0.6977 | Val Acc:   63.01%\n",
      "   Saved best model (Val Acc: 63.01%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/50 [Train]: 100%|| 22/22 [00:11<00:00,  1.94it/s, loss=0.3051, acc=85.59%]\n",
      "Epoch 8/50 [Val]: 100%|| 5/5 [00:01<00:00,  3.12it/s, loss=0.6048, acc=63.01%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8/50:\n",
      "  Train Loss: 0.4176 | Train Acc: 85.59%\n",
      "  Val Loss:   0.7103 | Val Acc:   63.01%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/50 [Train]: 100%|| 22/22 [00:11<00:00,  1.96it/s, loss=0.0746, acc=89.41%]\n",
      "Epoch 9/50 [Val]: 100%|| 5/5 [00:01<00:00,  3.12it/s, loss=0.2774, acc=61.64%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9/50:\n",
      "  Train Loss: 0.3206 | Train Acc: 89.41%\n",
      "  Val Loss:   0.8082 | Val Acc:   61.64%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/50 [Train]: 100%|| 22/22 [00:11<00:00,  1.95it/s, loss=0.0267, acc=89.12%]\n",
      "Epoch 10/50 [Val]: 100%|| 5/5 [00:01<00:00,  3.13it/s, loss=0.4496, acc=61.64%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10/50:\n",
      "  Train Loss: 0.2403 | Train Acc: 89.12%\n",
      "  Val Loss:   0.9808 | Val Acc:   61.64%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/50 [Train]: 100%|| 22/22 [00:11<00:00,  1.94it/s, loss=0.5945, acc=91.76%]\n",
      "Epoch 11/50 [Val]: 100%|| 5/5 [00:01<00:00,  3.00it/s, loss=0.3214, acc=61.64%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11/50:\n",
      "  Train Loss: 0.2229 | Train Acc: 91.76%\n",
      "  Val Loss:   0.9760 | Val Acc:   61.64%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/50 [Train]: 100%|| 22/22 [00:11<00:00,  1.95it/s, loss=0.1081, acc=95.59%]\n",
      "Epoch 12/50 [Val]: 100%|| 5/5 [00:01<00:00,  3.08it/s, loss=0.7370, acc=57.53%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12/50:\n",
      "  Train Loss: 0.1426 | Train Acc: 95.59%\n",
      "  Val Loss:   1.1611 | Val Acc:   57.53%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/50 [Train]:  73%|  | 16/22 [00:09<00:03,  1.72it/s, loss=0.2668, acc=95.31%]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 58\u001b[39m\n\u001b[32m     55\u001b[39m loss = criterion(logits, labels)\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     59\u001b[39m optimizer.step()\n\u001b[32m     61\u001b[39m \u001b[38;5;66;03m# Calculate accuracy\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/storage1/shourovj/shourovj_works/Auburn/.venv/lib/python3.11/site-packages/torch/_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/storage1/shourovj/shourovj_works/Auburn/.venv/lib/python3.11/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/storage1/shourovj/shourovj_works/Auburn/.venv/lib/python3.11/site-packages/torch/autograd/graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "## Training\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Move model to device\n",
    "model = model.to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "\n",
    "# Training hyperparameters\n",
    "num_epochs = 50\n",
    "best_val_acc = 0.0\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accs = []\n",
    "val_accs = []\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Starting Training\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters())/1e6:.2f}M\")\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "print(f\"Number of epochs: {num_epochs}\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # ========== TRAINING PHASE ==========\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    \n",
    "    train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\")\n",
    "    for batch_idx, (f_mat, c_mat, labels) in enumerate(train_pbar):\n",
    "        # Move data to device\n",
    "        f_mat = f_mat.to(device)  # [B, 400, 1632]\n",
    "        c_mat = c_mat.to(device)  # [B, 45, 54, 45]\n",
    "        labels = labels.squeeze().to(device)  # [B] - remove extra dimension\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(f_mat, c_mat)  # [B, 2]\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(logits, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(logits.data, 1)\n",
    "        train_total += labels.size(0)\n",
    "        train_correct += (predicted == labels).sum().item()\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        # Update progress bar\n",
    "        train_pbar.set_postfix({\n",
    "            'loss': f'{loss.item():.4f}',\n",
    "            'acc': f'{100*train_correct/train_total:.2f}%'\n",
    "        })\n",
    "    \n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    train_acc = 100 * train_correct / train_total\n",
    "    train_losses.append(avg_train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    \n",
    "    # ========== VALIDATION PHASE ==========\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        val_pbar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Val]\")\n",
    "        for f_mat, c_mat, labels in val_pbar:\n",
    "            # Move data to device\n",
    "            f_mat = f_mat.to(device)\n",
    "            c_mat = c_mat.to(device)\n",
    "            labels = labels.squeeze().to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            logits = model(f_mat, c_mat)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(logits, labels)\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(logits.data, 1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            # Update progress bar\n",
    "            val_pbar.set_postfix({\n",
    "                'loss': f'{loss.item():.4f}',\n",
    "                'acc': f'{100*val_correct/val_total:.2f}%'\n",
    "            })\n",
    "    \n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_acc = 100 * val_correct / val_total\n",
    "    val_losses.append(avg_val_loss)\n",
    "    val_accs.append(val_acc)\n",
    "    \n",
    "    # Print epoch summary\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}:\")\n",
    "    print(f\"  Train Loss: {avg_train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
    "    print(f\"  Val Loss:   {avg_val_loss:.4f} | Val Acc:   {val_acc:.2f}%\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_acc': val_acc,\n",
    "            'train_acc': train_acc,\n",
    "        }, 'best_model.pth')\n",
    "        print(f\"   Saved best model (Val Acc: {val_acc:.2f}%)\\n\")\n",
    "    else:\n",
    "        print()\n",
    "    \n",
    "    # Early stopping (optional - uncomment if needed)\n",
    "    # if epoch > 10 and val_losses[-1] > val_losses[-2] and val_losses[-2] > val_losses[-3]:\n",
    "    #     print(\"Early stopping triggered!\")\n",
    "    #     break\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Training Complete!\")\n",
    "print(f\"Best Validation Accuracy: {best_val_acc:.2f}%\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53888d3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89ce465",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Understanding Training Metrics: Why Val Loss Increases While Accuracy Improves\n",
    "\n",
    "\"\"\"\n",
    "EXPLANATION OF THE PHENOMENON:\n",
    "\n",
    "You're observing CLASSIC OVERFITTING! Here's what's happening:\n",
    "\n",
    "1. TRAINING METRICS (Epoch 12):\n",
    "   - Train Loss: 0.1426 (very low - model is very confident)\n",
    "   - Train Acc: 95.59% (very high - model memorizing training data)\n",
    "\n",
    "2. VALIDATION METRICS (Epoch 12):\n",
    "   - Val Loss: 1.1611 (increasing - model less confident on unseen data)\n",
    "   - Val Acc: 57.53% (much lower than train - poor generalization)\n",
    "\n",
    "WHY THIS HAPPENS:\n",
    "\n",
    "Loss vs Accuracy measure different things:\n",
    "- LOSS (CrossEntropy): Measures how \"confident\" the model is\n",
    "  - Low loss = model is very confident in its predictions\n",
    "  - High loss = model is uncertain/confused\n",
    "  \n",
    "- ACCURACY: Just counts correct/incorrect predictions\n",
    "  - Doesn't care about confidence level\n",
    "  - A model can be \"more correct\" but less confident (or vice versa)\n",
    "\n",
    "WHAT'S HAPPENING:\n",
    "- Model is memorizing training patterns (high train acc, low train loss)\n",
    "- But when it sees new validation data, it's uncertain (high val loss)\n",
    "- However, it's still guessing correctly sometimes (val acc ~57%)\n",
    "- The gap between train (95%) and val (57%) = OVERFITTING!\n",
    "\n",
    "SOLUTIONS:\n",
    "\n",
    "1. EARLY STOPPING (already in code, just uncomment)\n",
    "2. INCREASE REGULARIZATION:\n",
    "   - Increase dropout (currently 0.1  try 0.3-0.5)\n",
    "   - Increase weight_decay (currently 1e-5  try 1e-4)\n",
    "3. REDUCE MODEL CAPACITY:\n",
    "   - Fewer transformer layers\n",
    "   - Smaller hidden_dim\n",
    "4. LEARNING RATE SCHEDULING:\n",
    "   - Reduce LR when val loss plateaus\n",
    "5. DATA AUGMENTATION (if applicable)\n",
    "6. MORE TRAINING DATA (if possible)\n",
    "\n",
    "RECOMMENDATION: Enable early stopping to prevent further overfitting!\n",
    "\"\"\"\n",
    "\n",
    "print(\"See explanation above \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5729b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Improved Training Configuration (Addressing Overfitting)\n",
    "\n",
    "\"\"\"\n",
    "QUICK FIXES FOR OVERFITTING:\n",
    "\n",
    "Based on your training results showing overfitting (train acc 95% vs val acc 57%):\n",
    "\n",
    "1. INCREASE REGULARIZATION in ModelConfig:\n",
    "   - dropout: 0.1  0.3 or 0.5\n",
    "   - weight_decay: 1e-5  1e-4\n",
    "\n",
    "2. REDUCE MODEL CAPACITY:\n",
    "   - num_layers: 8  2 or 4\n",
    "   - hidden_dim: 64  32 or 48\n",
    "\n",
    "3. ENABLE EARLY STOPPING (see code below)\n",
    "\n",
    "4. ADD LEARNING RATE SCHEDULING\n",
    "\"\"\"\n",
    "\n",
    "# Example: Update your training code with these improvements:\n",
    "\n",
    "# 1. Increase regularization\n",
    "# optimizer = Adam(model.parameters(), lr=1e-4, weight_decay=1e-4)  # Increased from 1e-5\n",
    "\n",
    "# 2. Add learning rate scheduler\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "#     optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
    "# )\n",
    "\n",
    "# 3. In training loop, after validation:\n",
    "# scheduler.step(avg_val_loss)  # Update learning rate based on val loss\n",
    "\n",
    "# 4. Better early stopping (patience-based):\n",
    "# patience = 10\n",
    "# best_val_loss = float('inf')\n",
    "# patience_counter = 0\n",
    "# \n",
    "# if avg_val_loss < best_val_loss:\n",
    "#     best_val_loss = avg_val_loss\n",
    "#     patience_counter = 0\n",
    "# else:\n",
    "#     patience_counter += 1\n",
    "#     if patience_counter >= patience:\n",
    "#         print(\"Early stopping!\")\n",
    "#         break\n",
    "\n",
    "print(\"See recommendations above \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51944e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Testing on Test Set\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "# Load best model\n",
    "checkpoint = torch.load('best_model.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(f\"Loaded best model from epoch {checkpoint['epoch']+1}\")\n",
    "print(f\"  Train Acc: {checkpoint['train_acc']:.2f}%\")\n",
    "print(f\"  Val Acc:   {checkpoint['val_acc']:.2f}%\")\n",
    "print()\n",
    "\n",
    "# Evaluate on test set\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_pbar = tqdm(test_loader, desc=\"Testing\")\n",
    "    for f_mat, c_mat, labels in test_pbar:\n",
    "        # Move data to device\n",
    "        f_mat = f_mat.to(device)\n",
    "        c_mat = c_mat.to(device)\n",
    "        labels = labels.squeeze().to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = model(f_mat, c_mat)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(logits, labels)\n",
    "        \n",
    "        # Get predictions\n",
    "        _, predicted = torch.max(logits.data, 1)\n",
    "        \n",
    "        # Store predictions and labels for detailed analysis\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        # Calculate metrics\n",
    "        test_total += labels.size(0)\n",
    "        test_correct += (predicted == labels).sum().item()\n",
    "        test_loss += loss.item()\n",
    "        \n",
    "        # Update progress bar\n",
    "        test_pbar.set_postfix({\n",
    "            'loss': f'{loss.item():.4f}',\n",
    "            'acc': f'{100*test_correct/test_total:.2f}%'\n",
    "        })\n",
    "\n",
    "avg_test_loss = test_loss / len(test_loader)\n",
    "test_acc = 100 * test_correct / test_total\n",
    "\n",
    "# Print test results\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Test Results\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Test Loss:     {avg_test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.2f}%\")\n",
    "print(f\"Correct:       {test_correct}/{test_total}\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "# Detailed classification report\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(all_labels, all_predictions, \n",
    "                            target_names=['Class 0', 'Class 1']))\n",
    "\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "cm = confusion_matrix(all_labels, all_predictions)\n",
    "print(cm)\n",
    "print(f\"\\nTrue Negatives:  {cm[0,0]}\")\n",
    "print(f\"False Positives: {cm[0,1]}\")\n",
    "print(f\"False Negatives: {cm[1,0]}\")\n",
    "print(f\"True Positives:  {cm[1,1]}\")\n",
    "\n",
    "# Calculate per-class accuracy\n",
    "class_0_acc = cm[0,0] / (cm[0,0] + cm[0,1]) * 100 if (cm[0,0] + cm[0,1]) > 0 else 0\n",
    "class_1_acc = cm[1,1] / (cm[1,0] + cm[1,1]) * 100 if (cm[1,0] + cm[1,1]) > 0 else 0\n",
    "print(f\"\\nPer-Class Accuracy:\")\n",
    "print(f\"  Class 0: {class_0_acc:.2f}%\")\n",
    "print(f\"  Class 1: {class_1_acc:.2f}%\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Testing Complete!\")\n",
    "print(f\"{'='*70}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a990638",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98584fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
