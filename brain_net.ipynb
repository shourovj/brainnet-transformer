{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e8ac2bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import scipy.io as sio\n",
    "import sentencepiece\n",
    "import tiktoken\n",
    "import einops\n",
    "import wandb\n",
    "import accelerate\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "29292507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['__header__', '__version__', '__globals__', 'label'])\n"
     ]
    }
   ],
   "source": [
    "data = sio.loadmat('toy_data/label.mat')\n",
    "print(data.keys())\n",
    "labels = data['label']\n",
    "# print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5ea87969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !uv pip install matplotlib \n",
    "# !uv pip install seaborn \n",
    "# !uv pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299756fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f21df824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['toy_data/s_222_feature.mat', 'toy_data/s_156_feature.mat', 'toy_data/s_153_cluster_index.mat', 'toy_data/s_202_feature.mat', 'toy_data/s_396_cluster_index.mat', 'toy_data/s_193_feature.mat', 'toy_data/s_24_feature.mat', 'toy_data/s_28_feature.mat', 'toy_data/s_157_cluster_index.mat', 'toy_data/s_317_cluster_index.mat', 'toy_data/s_182_feature.mat', 'toy_data/s_481_feature.mat', 'toy_data/s_200_cluster_index.mat', 'toy_data/s_400_feature.mat', 'toy_data/s_140_feature.mat', 'toy_data/s_271_cluster_index.mat', 'toy_data/s_207_feature.mat', 'toy_data/s_387_cluster_index.mat', 'toy_data/s_430_feature.mat', 'toy_data/s_6_feature.mat', 'toy_data/s_134_feature.mat', 'toy_data/s_145_feature.mat', 'toy_data/s_126_cluster_index.mat', 'toy_data/s_66_feature.mat', 'toy_data/s_193_cluster_index.mat', 'toy_data/s_444_cluster_index.mat', 'toy_data/s_424_feature.mat', 'toy_data/s_398_cluster_index.mat', 'toy_data/s_401_feature.mat', 'toy_data/s_73_feature.mat', 'toy_data/s_358_feature.mat', 'toy_data/s_384_cluster_index.mat', 'toy_data/s_407_feature.mat', 'toy_data/s_190_feature.mat', 'toy_data/s_42_feature.mat', 'toy_data/s_497_feature.mat', 'toy_data/s_270_cluster_index.mat', 'toy_data/s_63_feature.mat', 'toy_data/s_261_feature.mat', 'toy_data/s_456_cluster_index.mat', 'toy_data/s_103_feature.mat', 'toy_data/s_118_cluster_index.mat', 'toy_data/s_10_cluster_index.mat', 'toy_data/s_116_feature.mat', 'toy_data/s_232_cluster_index.mat', 'toy_data/s_490_feature.mat', 'toy_data/s_272_cluster_index.mat', 'toy_data/s_88_cluster_index.mat', 'toy_data/s_56_feature.mat', 'toy_data/s_419_cluster_index.mat', 'toy_data/s_209_cluster_index.mat', 'toy_data/s_128_cluster_index.mat', 'toy_data/s_109_cluster_index.mat', 'toy_data/s_255_feature.mat', 'toy_data/s_206_cluster_index.mat', 'toy_data/s_459_cluster_index.mat', 'toy_data/s_490_cluster_index.mat', 'toy_data/s_493_cluster_index.mat', 'toy_data/s_144_cluster_index.mat', 'toy_data/s_110_feature.mat', 'toy_data/s_172_feature.mat', 'toy_data/s_283_cluster_index.mat', 'toy_data/s_460_cluster_index.mat', 'toy_data/s_308_feature.mat', 'toy_data/s_92_cluster_index.mat', 'toy_data/s_232_feature.mat', 'toy_data/s_2_cluster_index.mat', 'toy_data/s_370_feature.mat', 'toy_data/s_472_feature.mat', 'toy_data/s_9_cluster_index.mat', 'toy_data/s_56_cluster_index.mat', 'toy_data/s_374_cluster_index.mat', 'toy_data/s_499_cluster_index.mat', 'toy_data/s_12_feature.mat', 'toy_data/s_70_feature.mat', 'toy_data/s_208_cluster_index.mat', 'toy_data/s_406_feature.mat', 'toy_data/s_117_feature.mat', 'toy_data/s_206_feature.mat', 'toy_data/s_43_feature.mat', 'toy_data/s_44_cluster_index.mat', 'toy_data/s_27_cluster_index.mat', 'toy_data/s_234_feature.mat', 'toy_data/s_100_cluster_index.mat', 'toy_data/s_294_cluster_index.mat', 'toy_data/s_289_feature.mat', 'toy_data/s_464_cluster_index.mat', 'toy_data/s_404_feature.mat', 'toy_data/s_73_cluster_index.mat', 'toy_data/s_288_cluster_index.mat', 'toy_data/s_469_cluster_index.mat', 'toy_data/s_64_feature.mat', 'toy_data/s_443_cluster_index.mat', 'toy_data/s_320_feature.mat', 'toy_data/s_102_cluster_index.mat', 'toy_data/s_18_feature.mat', 'toy_data/s_78_cluster_index.mat', 'toy_data/s_291_feature.mat', 'toy_data/s_416_feature.mat', 'toy_data/s_437_feature.mat', 'toy_data/s_313_cluster_index.mat', 'toy_data/s_209_feature.mat', 'toy_data/s_62_feature.mat', 'toy_data/s_247_feature.mat', 'toy_data/s_340_feature.mat', 'toy_data/s_119_cluster_index.mat', 'toy_data/s_212_feature.mat', 'toy_data/s_216_cluster_index.mat', 'toy_data/s_147_cluster_index.mat', 'toy_data/s_14_feature.mat', 'toy_data/s_159_feature.mat', 'toy_data/s_20_feature.mat', 'toy_data/s_312_feature.mat', 'toy_data/s_298_feature.mat', 'toy_data/s_381_cluster_index.mat', 'toy_data/s_195_cluster_index.mat', 'toy_data/s_15_feature.mat', 'toy_data/s_345_cluster_index.mat', 'toy_data/s_207_cluster_index.mat', 'toy_data/s_72_cluster_index.mat', 'toy_data/s_147_feature.mat', 'toy_data/s_322_feature.mat', 'toy_data/s_219_cluster_index.mat', 'toy_data/s_323_feature.mat', 'toy_data/s_345_feature.mat', 'toy_data/s_192_cluster_index.mat', 'toy_data/s_424_cluster_index.mat', 'toy_data/s_304_cluster_index.mat', 'toy_data/s_221_cluster_index.mat', 'toy_data/s_344_feature.mat', 'toy_data/s_269_cluster_index.mat', 'toy_data/s_479_feature.mat', 'toy_data/s_486_cluster_index.mat', 'toy_data/s_386_cluster_index.mat', 'toy_data/s_110_cluster_index.mat', 'toy_data/s_75_feature.mat', 'toy_data/s_307_feature.mat', 'toy_data/s_123_cluster_index.mat', 'toy_data/s_441_feature.mat', 'toy_data/s_485_cluster_index.mat', 'toy_data/s_311_cluster_index.mat', 'toy_data/s_44_feature.mat', 'toy_data/s_431_cluster_index.mat', 'toy_data/s_399_feature.mat', 'toy_data/s_364_cluster_index.mat', 'toy_data/s_483_cluster_index.mat', 'toy_data/s_452_feature.mat', 'toy_data/s_402_cluster_index.mat', 'toy_data/s_351_cluster_index.mat', 'toy_data/s_334_cluster_index.mat', 'toy_data/s_396_feature.mat', 'toy_data/s_237_cluster_index.mat', 'toy_data/s_21_cluster_index.mat', 'toy_data/s_22_cluster_index.mat', 'toy_data/s_189_cluster_index.mat', 'toy_data/s_210_cluster_index.mat', 'toy_data/s_241_cluster_index.mat', 'toy_data/s_462_cluster_index.mat', 'toy_data/s_183_cluster_index.mat', 'toy_data/s_411_cluster_index.mat', 'toy_data/s_313_feature.mat', 'toy_data/s_154_feature.mat', 'toy_data/s_438_feature.mat', 'toy_data/s_327_feature.mat', 'toy_data/s_125_feature.mat', 'toy_data/s_230_cluster_index.mat', 'toy_data/s_329_cluster_index.mat', 'toy_data/s_107_cluster_index.mat', 'toy_data/s_333_feature.mat', 'toy_data/s_321_cluster_index.mat', 'toy_data/s_49_feature.mat', 'toy_data/s_144_feature.mat', 'toy_data/s_30_cluster_index.mat', 'toy_data/s_135_cluster_index.mat', 'toy_data/s_212_cluster_index.mat', 'toy_data/s_333_cluster_index.mat', 'toy_data/s_331_feature.mat', 'toy_data/s_162_cluster_index.mat', 'toy_data/s_228_feature.mat', 'toy_data/s_205_feature.mat', 'toy_data/s_174_cluster_index.mat', 'toy_data/s_426_cluster_index.mat', 'toy_data/s_55_cluster_index.mat', 'toy_data/s_239_feature.mat', 'toy_data/s_138_feature.mat', 'toy_data/s_197_cluster_index.mat', 'toy_data/s_105_cluster_index.mat', 'toy_data/s_205_cluster_index.mat', 'toy_data/s_460_feature.mat', 'toy_data/s_338_feature.mat', 'toy_data/s_376_feature.mat', 'toy_data/s_281_cluster_index.mat', 'toy_data/s_226_cluster_index.mat', 'toy_data/s_55_feature.mat', 'toy_data/s_89_feature.mat', 'toy_data/s_83_cluster_index.mat', 'toy_data/s_337_cluster_index.mat', 'toy_data/s_318_cluster_index.mat', 'toy_data/s_29_cluster_index.mat', 'toy_data/s_342_feature.mat', 'toy_data/s_489_feature.mat', 'toy_data/s_126_feature.mat', 'toy_data/s_397_cluster_index.mat', 'toy_data/s_436_cluster_index.mat', 'toy_data/s_383_feature.mat', 'toy_data/s_297_cluster_index.mat', 'toy_data/s_274_cluster_index.mat', 'toy_data/s_38_feature.mat', 'toy_data/s_332_feature.mat', 'toy_data/s_132_cluster_index.mat', 'toy_data/s_174_feature.mat', 'toy_data/s_347_feature.mat', 'toy_data/s_405_feature.mat', 'toy_data/s_263_cluster_index.mat', 'toy_data/s_67_feature.mat', 'toy_data/s_344_cluster_index.mat', 'toy_data/s_24_cluster_index.mat', 'toy_data/s_489_cluster_index.mat', 'toy_data/s_134_cluster_index.mat', 'toy_data/s_395_feature.mat', 'toy_data/s_413_feature.mat', 'toy_data/s_350_cluster_index.mat', 'toy_data/s_414_cluster_index.mat', 'toy_data/s_160_cluster_index.mat', 'toy_data/s_43_cluster_index.mat', 'toy_data/s_450_cluster_index.mat', 'toy_data/s_498_cluster_index.mat', 'toy_data/s_115_feature.mat', 'toy_data/s_406_cluster_index.mat', 'toy_data/s_324_cluster_index.mat', 'toy_data/s_420_feature.mat', 'toy_data/s_278_feature.mat', 'toy_data/s_303_feature.mat', 'toy_data/s_252_feature.mat', 'toy_data/s_76_feature.mat', 'toy_data/s_95_cluster_index.mat', 'toy_data/s_235_feature.mat', 'toy_data/s_178_feature.mat', 'toy_data/s_111_feature.mat', 'toy_data/s_375_feature.mat', 'toy_data/s_464_feature.mat', 'toy_data/s_251_feature.mat', 'toy_data/s_377_feature.mat', 'toy_data/s_129_cluster_index.mat', 'toy_data/s_260_feature.mat', 'toy_data/s_8_cluster_index.mat', 'toy_data/s_321_feature.mat', 'toy_data/s_275_feature.mat', 'toy_data/s_139_cluster_index.mat', 'toy_data/s_199_feature.mat', 'toy_data/s_298_cluster_index.mat', 'toy_data/s_31_cluster_index.mat', 'toy_data/s_123_feature.mat', 'toy_data/s_379_feature.mat', 'toy_data/s_242_cluster_index.mat', 'toy_data/s_89_cluster_index.mat', 'toy_data/s_258_feature.mat', 'toy_data/s_122_cluster_index.mat', 'toy_data/s_184_feature.mat', 'toy_data/s_68_cluster_index.mat', 'toy_data/s_332_cluster_index.mat', 'toy_data/s_301_cluster_index.mat', 'toy_data/s_358_cluster_index.mat', 'toy_data/s_19_feature.mat', 'toy_data/s_90_feature.mat', 'toy_data/s_446_feature.mat', 'toy_data/s_112_feature.mat', 'toy_data/s_386_feature.mat', 'toy_data/s_268_feature.mat', 'toy_data/s_95_feature.mat', 'toy_data/s_368_cluster_index.mat', 'toy_data/s_50_feature.mat', 'toy_data/s_37_cluster_index.mat', 'toy_data/s_62_cluster_index.mat', 'toy_data/s_87_cluster_index.mat', 'toy_data/s_17_feature.mat', 'toy_data/s_196_cluster_index.mat', 'toy_data/s_11_feature.mat', 'toy_data/s_499_feature.mat', 'toy_data/s_173_cluster_index.mat', 'toy_data/s_349_feature.mat', 'toy_data/s_180_feature.mat', 'toy_data/s_415_feature.mat', 'toy_data/s_431_feature.mat', 'toy_data/s_261_cluster_index.mat', 'toy_data/s_389_cluster_index.mat', 'toy_data/s_334_feature.mat', 'toy_data/s_364_feature.mat', 'toy_data/s_468_feature.mat', 'toy_data/s_218_cluster_index.mat', 'toy_data/s_182_cluster_index.mat', 'toy_data/s_275_cluster_index.mat', 'toy_data/s_13_feature.mat', 'toy_data/s_148_feature.mat', 'toy_data/s_305_cluster_index.mat', 'toy_data/s_395_cluster_index.mat', 'toy_data/s_141_feature.mat', 'toy_data/s_40_cluster_index.mat', 'toy_data/s_258_cluster_index.mat', 'toy_data/s_279_feature.mat', 'toy_data/s_242_feature.mat', 'toy_data/s_432_feature.mat', 'toy_data/s_7_feature.mat', 'toy_data/s_80_cluster_index.mat', 'toy_data/s_457_cluster_index.mat', 'toy_data/s_175_cluster_index.mat', 'toy_data/s_365_cluster_index.mat', 'toy_data/s_328_feature.mat', 'toy_data/s_237_feature.mat', 'toy_data/s_170_feature.mat', 'toy_data/s_294_feature.mat', 'toy_data/s_314_feature.mat', 'toy_data/s_326_cluster_index.mat', 'toy_data/s_1_cluster_index.mat', 'toy_data/s_16_cluster_index.mat', 'toy_data/s_136_feature.mat', 'toy_data/s_418_cluster_index.mat', 'toy_data/s_215_cluster_index.mat', 'toy_data/s_41_cluster_index.mat', 'toy_data/s_61_cluster_index.mat', 'toy_data/s_355_feature.mat', 'toy_data/s_365_feature.mat', 'toy_data/s_187_cluster_index.mat', 'toy_data/s_328_cluster_index.mat', 'toy_data/s_124_feature.mat', 'toy_data/s_224_feature.mat', 'toy_data/s_46_cluster_index.mat', 'toy_data/s_483_feature.mat', 'toy_data/s_15_cluster_index.mat', 'toy_data/s_377_cluster_index.mat', 'toy_data/s_417_cluster_index.mat', 'toy_data/s_228_cluster_index.mat', 'toy_data/s_29_feature.mat', 'toy_data/s_74_cluster_index.mat', 'toy_data/s_136_cluster_index.mat', 'toy_data/s_267_feature.mat', 'toy_data/s_488_cluster_index.mat', 'toy_data/s_303_cluster_index.mat', 'toy_data/s_454_cluster_index.mat', 'toy_data/s_236_cluster_index.mat', 'toy_data/s_99_feature.mat', 'toy_data/s_158_cluster_index.mat', 'toy_data/s_19_cluster_index.mat', 'toy_data/s_455_cluster_index.mat', 'toy_data/s_172_cluster_index.mat', 'toy_data/s_140_cluster_index.mat', 'toy_data/s_353_feature.mat', 'toy_data/s_271_feature.mat', 'toy_data/s_421_cluster_index.mat', 'toy_data/s_194_feature.mat', 'toy_data/s_114_cluster_index.mat', 'toy_data/s_84_feature.mat', 'toy_data/s_439_cluster_index.mat', 'toy_data/s_190_cluster_index.mat', 'toy_data/s_444_feature.mat', 'toy_data/s_434_feature.mat', 'toy_data/s_184_cluster_index.mat', 'toy_data/s_217_cluster_index.mat', 'toy_data/s_8_feature.mat', 'toy_data/s_186_cluster_index.mat', 'toy_data/s_264_feature.mat', 'toy_data/s_484_cluster_index.mat', 'toy_data/s_40_feature.mat', 'toy_data/s_478_cluster_index.mat', 'toy_data/s_265_feature.mat', 'toy_data/s_16_feature.mat', 'toy_data/s_425_cluster_index.mat', 'toy_data/s_45_cluster_index.mat', 'toy_data/s_164_cluster_index.mat', 'toy_data/s_222_cluster_index.mat', 'toy_data/s_101_feature.mat', 'toy_data/s_477_cluster_index.mat', 'toy_data/s_457_feature.mat', 'toy_data/s_131_cluster_index.mat', 'toy_data/s_459_feature.mat', 'toy_data/s_103_cluster_index.mat', 'toy_data/s_35_feature.mat', 'toy_data/s_322_cluster_index.mat', 'toy_data/s_61_feature.mat', 'toy_data/s_121_cluster_index.mat', 'toy_data/s_195_feature.mat', 'toy_data/s_214_cluster_index.mat', 'toy_data/s_263_feature.mat', 'toy_data/s_370_cluster_index.mat', 'toy_data/s_463_cluster_index.mat', 'toy_data/s_116_cluster_index.mat', 'toy_data/s_286_cluster_index.mat', 'toy_data/s_397_feature.mat', 'toy_data/s_339_cluster_index.mat', 'toy_data/s_262_feature.mat', 'toy_data/s_451_cluster_index.mat', 'toy_data/s_54_feature.mat', 'toy_data/s_202_cluster_index.mat', 'toy_data/s_282_cluster_index.mat', 'toy_data/s_380_cluster_index.mat', 'toy_data/s_249_cluster_index.mat', 'toy_data/s_454_feature.mat', 'toy_data/s_156_cluster_index.mat', 'toy_data/s_154_cluster_index.mat', 'toy_data/s_412_cluster_index.mat', 'toy_data/s_151_cluster_index.mat', 'toy_data/s_121_feature.mat', 'toy_data/s_120_cluster_index.mat', 'toy_data/s_287_feature.mat', 'toy_data/s_324_feature.mat', 'toy_data/s_83_feature.mat', 'toy_data/s_342_cluster_index.mat', 'toy_data/s_166_cluster_index.mat', 'toy_data/s_292_feature.mat', 'toy_data/s_301_feature.mat', 'toy_data/s_146_feature.mat', 'toy_data/s_380_feature.mat', 'toy_data/s_63_cluster_index.mat', 'toy_data/s_4_cluster_index.mat', 'toy_data/s_335_cluster_index.mat', 'toy_data/s_127_feature.mat', 'toy_data/s_23_feature.mat', 'toy_data/s_394_feature.mat', 'toy_data/s_436_feature.mat', 'toy_data/s_229_cluster_index.mat', 'toy_data/s_119_feature.mat', 'toy_data/s_249_feature.mat', 'toy_data/s_244_feature.mat', 'toy_data/s_485_feature.mat', 'toy_data/s_145_cluster_index.mat', 'toy_data/s_33_feature.mat', 'toy_data/s_64_cluster_index.mat', 'toy_data/s_408_cluster_index.mat', 'toy_data/s_451_feature.mat', 'toy_data/s_57_feature.mat', 'toy_data/s_423_cluster_index.mat', 'toy_data/s_169_cluster_index.mat', 'toy_data/s_401_cluster_index.mat', 'toy_data/s_97_cluster_index.mat', 'toy_data/s_445_feature.mat', 'toy_data/s_201_cluster_index.mat', 'toy_data/s_269_feature.mat', 'toy_data/s_440_feature.mat', 'toy_data/s_426_feature.mat', 'toy_data/s_77_cluster_index.mat', 'toy_data/s_138_cluster_index.mat', 'toy_data/s_477_feature.mat', 'toy_data/s_443_feature.mat', 'toy_data/s_50_cluster_index.mat', 'toy_data/s_191_feature.mat', 'toy_data/s_441_cluster_index.mat', 'toy_data/s_475_cluster_index.mat', 'toy_data/s_455_feature.mat', 'toy_data/s_79_feature.mat', 'toy_data/s_372_cluster_index.mat', 'toy_data/s_25_cluster_index.mat', 'toy_data/s_130_feature.mat', 'toy_data/s_48_cluster_index.mat', 'toy_data/s_442_cluster_index.mat', 'toy_data/s_274_feature.mat', 'toy_data/s_398_feature.mat', 'toy_data/s_113_cluster_index.mat', 'toy_data/s_108_feature.mat', 'toy_data/s_351_feature.mat', 'toy_data/s_327_cluster_index.mat', 'toy_data/s_330_cluster_index.mat', 'toy_data/s_299_feature.mat', 'toy_data/s_127_cluster_index.mat', 'toy_data/s_143_cluster_index.mat', 'toy_data/s_421_feature.mat', 'toy_data/s_230_feature.mat', 'toy_data/s_390_cluster_index.mat', 'toy_data/s_98_cluster_index.mat', 'toy_data/s_47_cluster_index.mat', 'toy_data/s_51_feature.mat', 'toy_data/s_285_cluster_index.mat', 'toy_data/s_72_feature.mat', 'toy_data/s_363_feature.mat', 'toy_data/s_47_feature.mat', 'toy_data/s_176_feature.mat', 'toy_data/s_471_feature.mat', 'toy_data/s_302_cluster_index.mat', 'toy_data/s_250_cluster_index.mat', 'toy_data/s_336_cluster_index.mat', 'toy_data/s_403_feature.mat', 'toy_data/s_229_feature.mat', 'toy_data/s_429_feature.mat', 'toy_data/s_282_feature.mat', 'toy_data/s_26_cluster_index.mat', 'toy_data/s_470_feature.mat', 'toy_data/s_188_feature.mat', 'toy_data/s_461_cluster_index.mat', 'toy_data/s_183_feature.mat', 'toy_data/s_131_feature.mat', 'toy_data/s_173_feature.mat', 'toy_data/s_168_cluster_index.mat', 'toy_data/s_30_feature.mat', 'toy_data/s_85_feature.mat', 'toy_data/s_81_feature.mat', 'toy_data/s_233_cluster_index.mat', 'toy_data/s_305_feature.mat', 'toy_data/s_143_feature.mat', 'toy_data/s_428_feature.mat', 'toy_data/s_32_feature.mat', 'toy_data/s_65_feature.mat', 'toy_data/s_295_cluster_index.mat', 'toy_data/s_196_feature.mat', 'toy_data/s_316_cluster_index.mat', 'toy_data/s_210_feature.mat', 'toy_data/s_54_cluster_index.mat', 'toy_data/s_25_feature.mat', 'toy_data/s_461_feature.mat', 'toy_data/s_486_feature.mat', 'toy_data/s_484_feature.mat', 'toy_data/s_189_feature.mat', 'toy_data/s_279_cluster_index.mat', 'toy_data/s_378_feature.mat', 'toy_data/s_96_cluster_index.mat', 'toy_data/s_203_feature.mat', 'toy_data/s_429_cluster_index.mat', 'toy_data/s_471_cluster_index.mat', 'toy_data/s_465_cluster_index.mat', 'toy_data/s_220_cluster_index.mat', 'toy_data/s_31_feature.mat', 'toy_data/s_243_cluster_index.mat', 'toy_data/s_155_feature.mat', 'toy_data/s_343_feature.mat', 'toy_data/s_387_feature.mat', 'toy_data/s_447_feature.mat', 'toy_data/s_440_cluster_index.mat', 'toy_data/s_3_feature.mat', 'toy_data/s_255_cluster_index.mat', 'toy_data/s_407_cluster_index.mat', 'toy_data/s_26_feature.mat', 'toy_data/s_198_feature.mat', 'toy_data/s_284_feature.mat', 'toy_data/s_372_feature.mat', 'toy_data/s_102_feature.mat', 'toy_data/s_37_feature.mat', 'toy_data/s_325_feature.mat', 'toy_data/s_416_cluster_index.mat', 'toy_data/s_6_cluster_index.mat', 'toy_data/s_90_cluster_index.mat', 'toy_data/s_106_feature.mat', 'toy_data/s_343_cluster_index.mat', 'toy_data/s_470_cluster_index.mat', 'toy_data/s_251_cluster_index.mat', 'toy_data/s_466_feature.mat', 'toy_data/s_340_cluster_index.mat', 'toy_data/s_307_cluster_index.mat', 'toy_data/s_290_cluster_index.mat', 'toy_data/s_246_feature.mat', 'toy_data/s_418_feature.mat', 'toy_data/s_48_feature.mat', 'toy_data/s_223_feature.mat', 'toy_data/s_133_cluster_index.mat', 'toy_data/s_36_feature.mat', 'toy_data/s_462_feature.mat', 'toy_data/s_304_feature.mat', 'toy_data/s_276_cluster_index.mat', 'toy_data/s_264_cluster_index.mat', 'toy_data/s_166_feature.mat', 'toy_data/s_466_cluster_index.mat', 'toy_data/s_319_cluster_index.mat', 'toy_data/s_4_feature.mat', 'toy_data/s_373_feature.mat', 'toy_data/s_310_feature.mat', 'toy_data/s_311_feature.mat', 'toy_data/s_465_feature.mat', 'toy_data/s_130_cluster_index.mat', 'toy_data/s_458_feature.mat', 'toy_data/s_254_feature.mat', 'toy_data/s_91_cluster_index.mat', 'toy_data/s_146_cluster_index.mat', 'toy_data/s_280_feature.mat', 'toy_data/s_323_cluster_index.mat', 'toy_data/s_128_feature.mat', 'toy_data/s_329_feature.mat', 'toy_data/s_373_cluster_index.mat', 'toy_data/s_248_feature.mat', 'toy_data/s_168_feature.mat', 'toy_data/s_112_cluster_index.mat', 'toy_data/s_71_feature.mat', 'toy_data/s_227_feature.mat', 'toy_data/s_105_feature.mat', 'toy_data/s_300_feature.mat', 'toy_data/s_442_feature.mat', 'toy_data/s_385_cluster_index.mat', 'toy_data/s_70_cluster_index.mat', 'toy_data/s_49_cluster_index.mat', 'toy_data/s_463_feature.mat', 'toy_data/s_257_feature.mat', 'toy_data/s_367_feature.mat', 'toy_data/s_208_feature.mat', 'toy_data/s_238_feature.mat', 'toy_data/s_36_cluster_index.mat', 'toy_data/s_39_cluster_index.mat', 'toy_data/s_259_cluster_index.mat', 'toy_data/s_432_cluster_index.mat', 'toy_data/s_241_feature.mat', 'toy_data/s_11_cluster_index.mat', 'toy_data/s_369_cluster_index.mat', 'toy_data/s_338_cluster_index.mat', 'toy_data/s_82_feature.mat', 'toy_data/s_240_cluster_index.mat', 'toy_data/s_266_cluster_index.mat', 'toy_data/s_106_cluster_index.mat', 'toy_data/s_374_feature.mat', 'toy_data/s_478_feature.mat', 'toy_data/s_151_feature.mat', 'toy_data/s_32_cluster_index.mat', 'toy_data/s_165_feature.mat', 'toy_data/s_233_feature.mat', 'toy_data/s_359_feature.mat', 'toy_data/s_392_feature.mat', 'toy_data/s_496_feature.mat', 'toy_data/s_52_feature.mat', 'toy_data/s_494_cluster_index.mat', 'toy_data/s_312_cluster_index.mat', 'toy_data/s_363_cluster_index.mat', 'toy_data/s_259_feature.mat', 'toy_data/s_331_cluster_index.mat', 'toy_data/s_326_feature.mat', 'toy_data/s_388_feature.mat', 'toy_data/s_185_feature.mat', 'toy_data/s_118_feature.mat', 'toy_data/s_267_cluster_index.mat', 'toy_data/s_51_cluster_index.mat', 'toy_data/s_357_cluster_index.mat', 'toy_data/s_480_feature.mat', 'toy_data/s_359_cluster_index.mat', 'toy_data/s_453_feature.mat', 'toy_data/s_356_feature.mat', 'toy_data/s_448_feature.mat', 'toy_data/s_272_feature.mat', 'toy_data/s_498_feature.mat', 'toy_data/s_254_cluster_index.mat', 'toy_data/s_352_feature.mat', 'toy_data/s_214_feature.mat', 'toy_data/s_65_cluster_index.mat', 'toy_data/s_309_cluster_index.mat', 'toy_data/s_487_feature.mat', 'toy_data/s_283_feature.mat', 'toy_data/s_493_feature.mat', 'toy_data/s_155_cluster_index.mat', 'toy_data/s_352_cluster_index.mat', 'toy_data/s_99_cluster_index.mat', 'toy_data/s_96_feature.mat', 'toy_data/s_198_cluster_index.mat', 'toy_data/s_302_feature.mat', 'toy_data/s_104_feature.mat', 'toy_data/s_341_feature.mat', 'toy_data/s_20_cluster_index.mat', 'toy_data/s_187_feature.mat', 'toy_data/s_350_feature.mat', 'toy_data/s_227_cluster_index.mat', 'toy_data/s_27_feature.mat', 'toy_data/s_467_feature.mat', 'toy_data/s_341_cluster_index.mat', 'toy_data/s_413_cluster_index.mat', 'toy_data/s_476_cluster_index.mat', 'toy_data/s_411_feature.mat', 'toy_data/s_445_cluster_index.mat', 'toy_data/s_281_feature.mat', 'toy_data/s_428_cluster_index.mat', 'toy_data/s_88_feature.mat', 'toy_data/s_162_feature.mat', 'toy_data/s_150_feature.mat', 'toy_data/s_273_cluster_index.mat', 'toy_data/s_163_cluster_index.mat', 'toy_data/s_408_feature.mat', 'toy_data/s_488_feature.mat', 'toy_data/s_188_cluster_index.mat', 'toy_data/s_59_cluster_index.mat', 'toy_data/s_306_feature.mat', 'toy_data/s_167_feature.mat', 'toy_data/s_277_feature.mat', 'toy_data/s_69_cluster_index.mat', 'toy_data/s_449_feature.mat', 'toy_data/s_97_feature.mat', 'toy_data/s_109_feature.mat', 'toy_data/s_318_feature.mat', 'toy_data/s_59_feature.mat', 'toy_data/s_180_cluster_index.mat', 'toy_data/s_366_cluster_index.mat', 'toy_data/s_18_cluster_index.mat', 'toy_data/s_35_cluster_index.mat', 'toy_data/s_171_cluster_index.mat', 'toy_data/s_417_feature.mat', 'toy_data/s_244_cluster_index.mat', 'toy_data/s_480_cluster_index.mat', 'toy_data/s_476_feature.mat', 'toy_data/s_336_feature.mat', 'toy_data/s_104_cluster_index.mat', 'toy_data/s_67_cluster_index.mat', 'toy_data/s_213_cluster_index.mat', 'toy_data/s_1_feature.mat', 'toy_data/s_369_feature.mat', 'toy_data/s_245_feature.mat', 'toy_data/s_437_cluster_index.mat', 'toy_data/s_28_cluster_index.mat', 'toy_data/s_137_feature.mat', 'toy_data/s_439_feature.mat', 'toy_data/s_382_feature.mat', 'toy_data/s_38_cluster_index.mat', 'toy_data/s_427_cluster_index.mat', 'toy_data/s_309_feature.mat', 'toy_data/s_362_feature.mat', 'toy_data/s_231_cluster_index.mat', 'toy_data/s_75_cluster_index.mat', 'toy_data/s_306_cluster_index.mat', 'toy_data/s_141_cluster_index.mat', 'toy_data/s_357_feature.mat', 'toy_data/s_260_cluster_index.mat', 'toy_data/s_201_feature.mat', 'toy_data/s_149_cluster_index.mat', 'toy_data/s_492_cluster_index.mat', 'toy_data/s_299_cluster_index.mat', 'toy_data/s_256_feature.mat', 'toy_data/s_285_feature.mat', 'toy_data/s_355_cluster_index.mat', 'toy_data/s_215_feature.mat', 'toy_data/s_262_cluster_index.mat', 'toy_data/s_491_cluster_index.mat', 'toy_data/s_76_cluster_index.mat', 'toy_data/s_192_feature.mat', 'toy_data/s_435_cluster_index.mat', 'toy_data/s_296_cluster_index.mat', 'toy_data/s_266_feature.mat', 'toy_data/s_225_cluster_index.mat', 'toy_data/s_473_cluster_index.mat', 'toy_data/s_256_cluster_index.mat', 'toy_data/s_291_cluster_index.mat', 'toy_data/s_177_feature.mat', 'toy_data/s_361_cluster_index.mat', 'toy_data/s_473_feature.mat', 'toy_data/s_57_cluster_index.mat', 'toy_data/s_474_cluster_index.mat', 'toy_data/s_360_feature.mat', 'toy_data/s_7_cluster_index.mat', 'toy_data/s_74_feature.mat', 'toy_data/s_468_cluster_index.mat', 'toy_data/s_297_feature.mat', 'toy_data/s_5_feature.mat', 'toy_data/s_495_cluster_index.mat', 'toy_data/s_296_feature.mat', 'toy_data/s_288_feature.mat', 'toy_data/s_239_cluster_index.mat', 'toy_data/s_94_cluster_index.mat', 'toy_data/s_268_cluster_index.mat', 'toy_data/s_247_cluster_index.mat', 'toy_data/s_402_feature.mat', 'toy_data/s_66_cluster_index.mat', 'toy_data/s_60_feature.mat', 'toy_data/s_403_cluster_index.mat', 'toy_data/s_446_cluster_index.mat', 'toy_data/s_423_feature.mat', 'toy_data/s_219_feature.mat', 'toy_data/s_290_feature.mat', 'toy_data/s_181_cluster_index.mat', 'toy_data/s_252_cluster_index.mat', 'toy_data/s_211_cluster_index.mat', 'toy_data/s_170_cluster_index.mat', 'toy_data/s_178_cluster_index.mat', 'toy_data/label.mat', 'toy_data/s_474_feature.mat', 'toy_data/s_314_cluster_index.mat', 'toy_data/s_10_feature.mat', 'toy_data/s_42_cluster_index.mat', 'toy_data/s_390_feature.mat', 'toy_data/s_203_cluster_index.mat', 'toy_data/s_179_feature.mat', 'toy_data/s_152_cluster_index.mat', 'toy_data/s_368_feature.mat', 'toy_data/s_354_feature.mat', 'toy_data/s_137_cluster_index.mat', 'toy_data/s_179_cluster_index.mat', 'toy_data/s_469_feature.mat', 'toy_data/s_139_feature.mat', 'toy_data/s_376_cluster_index.mat', 'toy_data/s_157_feature.mat', 'toy_data/s_245_cluster_index.mat', 'toy_data/s_409_feature.mat', 'toy_data/s_482_cluster_index.mat', 'toy_data/s_129_feature.mat', 'toy_data/s_388_cluster_index.mat', 'toy_data/s_276_feature.mat', 'toy_data/s_58_cluster_index.mat', 'toy_data/s_14_cluster_index.mat', 'toy_data/s_234_cluster_index.mat', 'toy_data/s_366_feature.mat', 'toy_data/s_68_feature.mat', 'toy_data/s_389_feature.mat', 'toy_data/s_218_feature.mat', 'toy_data/s_371_cluster_index.mat', 'toy_data/s_393_feature.mat', 'toy_data/s_325_cluster_index.mat', 'toy_data/s_34_cluster_index.mat', 'toy_data/s_100_feature.mat', 'toy_data/s_93_feature.mat', 'toy_data/s_420_cluster_index.mat', 'toy_data/s_164_feature.mat', 'toy_data/s_94_feature.mat', 'toy_data/s_60_cluster_index.mat', 'toy_data/s_39_feature.mat', 'toy_data/s_87_feature.mat', 'toy_data/s_246_cluster_index.mat', 'toy_data/s_300_cluster_index.mat', 'toy_data/s_223_cluster_index.mat', 'toy_data/s_354_cluster_index.mat', 'toy_data/s_9_feature.mat', 'toy_data/s_165_cluster_index.mat', 'toy_data/s_392_cluster_index.mat', 'toy_data/s_161_feature.mat', 'toy_data/s_414_feature.mat', 'toy_data/s_41_feature.mat', 'toy_data/s_213_feature.mat', 'toy_data/s_21_feature.mat', 'toy_data/s_22_feature.mat', 'toy_data/s_177_cluster_index.mat', 'toy_data/s_409_cluster_index.mat', 'toy_data/s_135_feature.mat', 'toy_data/s_315_cluster_index.mat', 'toy_data/s_132_feature.mat', 'toy_data/s_339_feature.mat', 'toy_data/s_360_cluster_index.mat', 'toy_data/s_265_cluster_index.mat', 'toy_data/s_33_cluster_index.mat', 'toy_data/s_293_cluster_index.mat', 'toy_data/s_171_feature.mat', 'toy_data/s_308_cluster_index.mat', 'toy_data/s_250_feature.mat', 'toy_data/s_287_cluster_index.mat', 'toy_data/s_3_cluster_index.mat', 'toy_data/s_353_cluster_index.mat', 'toy_data/s_85_cluster_index.mat', 'toy_data/s_458_cluster_index.mat', 'toy_data/s_81_cluster_index.mat', 'toy_data/s_472_cluster_index.mat', 'toy_data/s_181_feature.mat', 'toy_data/s_185_cluster_index.mat', 'toy_data/s_235_cluster_index.mat', 'toy_data/s_295_feature.mat', 'toy_data/s_400_cluster_index.mat', 'toy_data/s_371_feature.mat', 'toy_data/s_375_cluster_index.mat', 'toy_data/s_337_feature.mat', 'toy_data/s_425_feature.mat', 'toy_data/s_220_feature.mat', 'toy_data/s_384_feature.mat', 'toy_data/s_316_feature.mat', 'toy_data/s_422_cluster_index.mat', 'toy_data/s_383_cluster_index.mat', 'toy_data/s_427_feature.mat', 'toy_data/s_2_feature.mat', 'toy_data/s_186_feature.mat', 'toy_data/s_253_cluster_index.mat', 'toy_data/s_394_cluster_index.mat', 'toy_data/s_107_feature.mat', 'toy_data/s_253_feature.mat', 'toy_data/s_378_cluster_index.mat', 'toy_data/s_500_feature.mat', 'toy_data/s_278_cluster_index.mat', 'toy_data/s_13_cluster_index.mat', 'toy_data/s_391_cluster_index.mat', 'toy_data/s_379_cluster_index.mat', 'toy_data/s_211_feature.mat', 'toy_data/s_53_feature.mat', 'toy_data/s_330_feature.mat', 'toy_data/s_124_cluster_index.mat', 'toy_data/s_148_cluster_index.mat', 'toy_data/s_433_feature.mat', 'toy_data/s_410_cluster_index.mat', 'toy_data/s_348_feature.mat', 'toy_data/s_399_cluster_index.mat', 'toy_data/s_405_cluster_index.mat', 'toy_data/s_270_feature.mat', 'toy_data/s_491_feature.mat', 'toy_data/s_419_feature.mat', 'toy_data/s_362_cluster_index.mat', 'toy_data/s_12_cluster_index.mat', 'toy_data/s_385_feature.mat', 'toy_data/s_216_feature.mat', 'toy_data/s_169_feature.mat', 'toy_data/s_412_feature.mat', 'toy_data/s_58_feature.mat', 'toy_data/s_273_feature.mat', 'toy_data/s_289_cluster_index.mat', 'toy_data/s_161_cluster_index.mat', 'toy_data/s_456_feature.mat', 'toy_data/s_200_feature.mat', 'toy_data/s_125_cluster_index.mat', 'toy_data/s_77_feature.mat', 'toy_data/s_191_cluster_index.mat', 'toy_data/s_152_feature.mat', 'toy_data/s_310_cluster_index.mat', 'toy_data/s_92_feature.mat', 'toy_data/s_142_cluster_index.mat', 'toy_data/s_101_cluster_index.mat', 'toy_data/s_346_feature.mat', 'toy_data/s_82_cluster_index.mat', 'toy_data/s_17_cluster_index.mat', 'toy_data/s_381_feature.mat', 'toy_data/s_23_cluster_index.mat', 'toy_data/s_382_cluster_index.mat', 'toy_data/s_496_cluster_index.mat', 'toy_data/s_194_cluster_index.mat', 'toy_data/s_404_cluster_index.mat', 'toy_data/s_111_cluster_index.mat', 'toy_data/s_356_cluster_index.mat', 'toy_data/s_450_feature.mat', 'toy_data/s_448_cluster_index.mat', 'toy_data/s_149_feature.mat', 'toy_data/s_46_feature.mat', 'toy_data/s_204_feature.mat', 'toy_data/s_393_cluster_index.mat', 'toy_data/s_467_cluster_index.mat', 'toy_data/s_78_feature.mat', 'toy_data/s_349_cluster_index.mat', 'toy_data/s_346_cluster_index.mat', 'toy_data/s_159_cluster_index.mat', 'toy_data/s_108_cluster_index.mat', 'toy_data/s_84_cluster_index.mat', 'toy_data/s_52_cluster_index.mat', 'toy_data/s_475_feature.mat', 'toy_data/s_293_feature.mat', 'toy_data/s_236_feature.mat', 'toy_data/s_482_feature.mat', 'toy_data/s_348_cluster_index.mat', 'toy_data/s_86_cluster_index.mat', 'toy_data/s_447_cluster_index.mat', 'toy_data/s_115_cluster_index.mat', 'toy_data/s_93_cluster_index.mat', 'toy_data/s_224_cluster_index.mat', 'toy_data/s_243_feature.mat', 'toy_data/s_69_feature.mat', 'toy_data/s_240_feature.mat', 'toy_data/s_492_feature.mat', 'toy_data/s_5_cluster_index.mat', 'toy_data/s_160_feature.mat', 'toy_data/s_120_feature.mat', 'toy_data/s_280_cluster_index.mat', 'toy_data/s_142_feature.mat', 'toy_data/s_391_feature.mat', 'toy_data/s_422_feature.mat', 'toy_data/s_53_cluster_index.mat', 'toy_data/s_225_feature.mat', 'toy_data/s_347_cluster_index.mat', 'toy_data/s_500_cluster_index.mat', 'toy_data/s_238_cluster_index.mat', 'toy_data/s_452_cluster_index.mat', 'toy_data/s_494_feature.mat', 'toy_data/s_113_feature.mat', 'toy_data/s_80_feature.mat', 'toy_data/s_367_cluster_index.mat', 'toy_data/s_86_feature.mat', 'toy_data/s_430_cluster_index.mat', 'toy_data/s_150_cluster_index.mat', 'toy_data/s_34_feature.mat', 'toy_data/s_167_cluster_index.mat', 'toy_data/s_217_feature.mat', 'toy_data/s_435_feature.mat', 'toy_data/s_449_cluster_index.mat', 'toy_data/s_133_feature.mat', 'toy_data/s_153_feature.mat', 'toy_data/s_163_feature.mat', 'toy_data/s_257_cluster_index.mat', 'toy_data/s_158_feature.mat', 'toy_data/s_317_feature.mat', 'toy_data/s_284_cluster_index.mat', 'toy_data/s_91_feature.mat', 'toy_data/s_415_cluster_index.mat', 'toy_data/s_434_cluster_index.mat', 'toy_data/s_319_feature.mat', 'toy_data/s_453_cluster_index.mat', 'toy_data/s_292_cluster_index.mat', 'toy_data/s_175_feature.mat', 'toy_data/s_71_cluster_index.mat', 'toy_data/s_45_feature.mat', 'toy_data/s_433_cluster_index.mat', 'toy_data/s_204_cluster_index.mat', 'toy_data/s_226_feature.mat', 'toy_data/s_79_cluster_index.mat', 'toy_data/s_361_feature.mat', 'toy_data/s_197_feature.mat', 'toy_data/s_315_feature.mat', 'toy_data/s_495_feature.mat', 'toy_data/s_497_cluster_index.mat', 'toy_data/s_248_cluster_index.mat', 'toy_data/s_481_cluster_index.mat', 'toy_data/s_122_feature.mat', 'toy_data/s_114_feature.mat', 'toy_data/s_479_cluster_index.mat', 'toy_data/s_487_cluster_index.mat', 'toy_data/s_335_feature.mat', 'toy_data/s_277_cluster_index.mat', 'toy_data/s_98_feature.mat', 'toy_data/s_221_feature.mat', 'toy_data/s_176_cluster_index.mat', 'toy_data/s_117_cluster_index.mat', 'toy_data/s_286_feature.mat', 'toy_data/s_231_feature.mat', 'toy_data/s_438_cluster_index.mat', 'toy_data/s_199_cluster_index.mat', 'toy_data/s_410_feature.mat', 'toy_data/s_320_cluster_index.mat']\n",
      "['toy_data/s_1_feature.mat', 'toy_data/s_2_feature.mat', 'toy_data/s_3_feature.mat', 'toy_data/s_4_feature.mat', 'toy_data/s_5_feature.mat', 'toy_data/s_6_feature.mat', 'toy_data/s_7_feature.mat', 'toy_data/s_8_feature.mat', 'toy_data/s_9_feature.mat', 'toy_data/s_10_feature.mat', 'toy_data/s_11_feature.mat', 'toy_data/s_12_feature.mat', 'toy_data/s_13_feature.mat', 'toy_data/s_14_feature.mat', 'toy_data/s_15_feature.mat', 'toy_data/s_16_feature.mat', 'toy_data/s_17_feature.mat', 'toy_data/s_18_feature.mat', 'toy_data/s_19_feature.mat', 'toy_data/s_20_feature.mat', 'toy_data/s_21_feature.mat', 'toy_data/s_22_feature.mat', 'toy_data/s_23_feature.mat', 'toy_data/s_24_feature.mat', 'toy_data/s_25_feature.mat', 'toy_data/s_26_feature.mat', 'toy_data/s_27_feature.mat', 'toy_data/s_28_feature.mat', 'toy_data/s_29_feature.mat', 'toy_data/s_30_feature.mat', 'toy_data/s_31_feature.mat', 'toy_data/s_32_feature.mat', 'toy_data/s_33_feature.mat', 'toy_data/s_34_feature.mat', 'toy_data/s_35_feature.mat', 'toy_data/s_36_feature.mat', 'toy_data/s_37_feature.mat', 'toy_data/s_38_feature.mat', 'toy_data/s_39_feature.mat', 'toy_data/s_40_feature.mat', 'toy_data/s_41_feature.mat', 'toy_data/s_42_feature.mat', 'toy_data/s_43_feature.mat', 'toy_data/s_44_feature.mat', 'toy_data/s_45_feature.mat', 'toy_data/s_46_feature.mat', 'toy_data/s_47_feature.mat', 'toy_data/s_48_feature.mat', 'toy_data/s_49_feature.mat', 'toy_data/s_50_feature.mat', 'toy_data/s_51_feature.mat', 'toy_data/s_52_feature.mat', 'toy_data/s_53_feature.mat', 'toy_data/s_54_feature.mat', 'toy_data/s_55_feature.mat', 'toy_data/s_56_feature.mat', 'toy_data/s_57_feature.mat', 'toy_data/s_58_feature.mat', 'toy_data/s_59_feature.mat', 'toy_data/s_60_feature.mat', 'toy_data/s_61_feature.mat', 'toy_data/s_62_feature.mat', 'toy_data/s_63_feature.mat', 'toy_data/s_64_feature.mat', 'toy_data/s_65_feature.mat', 'toy_data/s_66_feature.mat', 'toy_data/s_67_feature.mat', 'toy_data/s_68_feature.mat', 'toy_data/s_69_feature.mat', 'toy_data/s_70_feature.mat', 'toy_data/s_71_feature.mat', 'toy_data/s_72_feature.mat', 'toy_data/s_73_feature.mat', 'toy_data/s_74_feature.mat', 'toy_data/s_75_feature.mat', 'toy_data/s_76_feature.mat', 'toy_data/s_77_feature.mat', 'toy_data/s_78_feature.mat', 'toy_data/s_79_feature.mat', 'toy_data/s_80_feature.mat', 'toy_data/s_81_feature.mat', 'toy_data/s_82_feature.mat', 'toy_data/s_83_feature.mat', 'toy_data/s_84_feature.mat', 'toy_data/s_85_feature.mat', 'toy_data/s_86_feature.mat', 'toy_data/s_87_feature.mat', 'toy_data/s_88_feature.mat', 'toy_data/s_89_feature.mat', 'toy_data/s_90_feature.mat', 'toy_data/s_91_feature.mat', 'toy_data/s_92_feature.mat', 'toy_data/s_93_feature.mat', 'toy_data/s_94_feature.mat', 'toy_data/s_95_feature.mat', 'toy_data/s_96_feature.mat', 'toy_data/s_97_feature.mat', 'toy_data/s_98_feature.mat', 'toy_data/s_99_feature.mat', 'toy_data/s_100_feature.mat', 'toy_data/s_101_feature.mat', 'toy_data/s_102_feature.mat', 'toy_data/s_103_feature.mat', 'toy_data/s_104_feature.mat', 'toy_data/s_105_feature.mat', 'toy_data/s_106_feature.mat', 'toy_data/s_107_feature.mat', 'toy_data/s_108_feature.mat', 'toy_data/s_109_feature.mat', 'toy_data/s_110_feature.mat', 'toy_data/s_111_feature.mat', 'toy_data/s_112_feature.mat', 'toy_data/s_113_feature.mat', 'toy_data/s_114_feature.mat', 'toy_data/s_115_feature.mat', 'toy_data/s_116_feature.mat', 'toy_data/s_117_feature.mat', 'toy_data/s_118_feature.mat', 'toy_data/s_119_feature.mat', 'toy_data/s_120_feature.mat', 'toy_data/s_121_feature.mat', 'toy_data/s_122_feature.mat', 'toy_data/s_123_feature.mat', 'toy_data/s_124_feature.mat', 'toy_data/s_125_feature.mat', 'toy_data/s_126_feature.mat', 'toy_data/s_127_feature.mat', 'toy_data/s_128_feature.mat', 'toy_data/s_129_feature.mat', 'toy_data/s_130_feature.mat', 'toy_data/s_131_feature.mat', 'toy_data/s_132_feature.mat', 'toy_data/s_133_feature.mat', 'toy_data/s_134_feature.mat', 'toy_data/s_135_feature.mat', 'toy_data/s_136_feature.mat', 'toy_data/s_137_feature.mat', 'toy_data/s_138_feature.mat', 'toy_data/s_139_feature.mat', 'toy_data/s_140_feature.mat', 'toy_data/s_141_feature.mat', 'toy_data/s_142_feature.mat', 'toy_data/s_143_feature.mat', 'toy_data/s_144_feature.mat', 'toy_data/s_145_feature.mat', 'toy_data/s_146_feature.mat', 'toy_data/s_147_feature.mat', 'toy_data/s_148_feature.mat', 'toy_data/s_149_feature.mat', 'toy_data/s_150_feature.mat', 'toy_data/s_151_feature.mat', 'toy_data/s_152_feature.mat', 'toy_data/s_153_feature.mat', 'toy_data/s_154_feature.mat', 'toy_data/s_155_feature.mat', 'toy_data/s_156_feature.mat', 'toy_data/s_157_feature.mat', 'toy_data/s_158_feature.mat', 'toy_data/s_159_feature.mat', 'toy_data/s_160_feature.mat', 'toy_data/s_161_feature.mat', 'toy_data/s_162_feature.mat', 'toy_data/s_163_feature.mat', 'toy_data/s_164_feature.mat', 'toy_data/s_165_feature.mat', 'toy_data/s_166_feature.mat', 'toy_data/s_167_feature.mat', 'toy_data/s_168_feature.mat', 'toy_data/s_169_feature.mat', 'toy_data/s_170_feature.mat', 'toy_data/s_171_feature.mat', 'toy_data/s_172_feature.mat', 'toy_data/s_173_feature.mat', 'toy_data/s_174_feature.mat', 'toy_data/s_175_feature.mat', 'toy_data/s_176_feature.mat', 'toy_data/s_177_feature.mat', 'toy_data/s_178_feature.mat', 'toy_data/s_179_feature.mat', 'toy_data/s_180_feature.mat', 'toy_data/s_181_feature.mat', 'toy_data/s_182_feature.mat', 'toy_data/s_183_feature.mat', 'toy_data/s_184_feature.mat', 'toy_data/s_185_feature.mat', 'toy_data/s_186_feature.mat', 'toy_data/s_187_feature.mat', 'toy_data/s_188_feature.mat', 'toy_data/s_189_feature.mat', 'toy_data/s_190_feature.mat', 'toy_data/s_191_feature.mat', 'toy_data/s_192_feature.mat', 'toy_data/s_193_feature.mat', 'toy_data/s_194_feature.mat', 'toy_data/s_195_feature.mat', 'toy_data/s_196_feature.mat', 'toy_data/s_197_feature.mat', 'toy_data/s_198_feature.mat', 'toy_data/s_199_feature.mat', 'toy_data/s_200_feature.mat', 'toy_data/s_201_feature.mat', 'toy_data/s_202_feature.mat', 'toy_data/s_203_feature.mat', 'toy_data/s_204_feature.mat', 'toy_data/s_205_feature.mat', 'toy_data/s_206_feature.mat', 'toy_data/s_207_feature.mat', 'toy_data/s_208_feature.mat', 'toy_data/s_209_feature.mat', 'toy_data/s_210_feature.mat', 'toy_data/s_211_feature.mat', 'toy_data/s_212_feature.mat', 'toy_data/s_213_feature.mat', 'toy_data/s_214_feature.mat', 'toy_data/s_215_feature.mat', 'toy_data/s_216_feature.mat', 'toy_data/s_217_feature.mat', 'toy_data/s_218_feature.mat', 'toy_data/s_219_feature.mat', 'toy_data/s_220_feature.mat', 'toy_data/s_221_feature.mat', 'toy_data/s_222_feature.mat', 'toy_data/s_223_feature.mat', 'toy_data/s_224_feature.mat', 'toy_data/s_225_feature.mat', 'toy_data/s_226_feature.mat', 'toy_data/s_227_feature.mat', 'toy_data/s_228_feature.mat', 'toy_data/s_229_feature.mat', 'toy_data/s_230_feature.mat', 'toy_data/s_231_feature.mat', 'toy_data/s_232_feature.mat', 'toy_data/s_233_feature.mat', 'toy_data/s_234_feature.mat', 'toy_data/s_235_feature.mat', 'toy_data/s_236_feature.mat', 'toy_data/s_237_feature.mat', 'toy_data/s_238_feature.mat', 'toy_data/s_239_feature.mat', 'toy_data/s_240_feature.mat', 'toy_data/s_241_feature.mat', 'toy_data/s_242_feature.mat', 'toy_data/s_243_feature.mat', 'toy_data/s_244_feature.mat', 'toy_data/s_245_feature.mat', 'toy_data/s_246_feature.mat', 'toy_data/s_247_feature.mat', 'toy_data/s_248_feature.mat', 'toy_data/s_249_feature.mat', 'toy_data/s_250_feature.mat', 'toy_data/s_251_feature.mat', 'toy_data/s_252_feature.mat', 'toy_data/s_253_feature.mat', 'toy_data/s_254_feature.mat', 'toy_data/s_255_feature.mat', 'toy_data/s_256_feature.mat', 'toy_data/s_257_feature.mat', 'toy_data/s_258_feature.mat', 'toy_data/s_259_feature.mat', 'toy_data/s_260_feature.mat', 'toy_data/s_261_feature.mat', 'toy_data/s_262_feature.mat', 'toy_data/s_263_feature.mat', 'toy_data/s_264_feature.mat', 'toy_data/s_265_feature.mat', 'toy_data/s_266_feature.mat', 'toy_data/s_267_feature.mat', 'toy_data/s_268_feature.mat', 'toy_data/s_269_feature.mat', 'toy_data/s_270_feature.mat', 'toy_data/s_271_feature.mat', 'toy_data/s_272_feature.mat', 'toy_data/s_273_feature.mat', 'toy_data/s_274_feature.mat', 'toy_data/s_275_feature.mat', 'toy_data/s_276_feature.mat', 'toy_data/s_277_feature.mat', 'toy_data/s_278_feature.mat', 'toy_data/s_279_feature.mat', 'toy_data/s_280_feature.mat', 'toy_data/s_281_feature.mat', 'toy_data/s_282_feature.mat', 'toy_data/s_283_feature.mat', 'toy_data/s_284_feature.mat', 'toy_data/s_285_feature.mat', 'toy_data/s_286_feature.mat', 'toy_data/s_287_feature.mat', 'toy_data/s_288_feature.mat', 'toy_data/s_289_feature.mat', 'toy_data/s_290_feature.mat', 'toy_data/s_291_feature.mat', 'toy_data/s_292_feature.mat', 'toy_data/s_293_feature.mat', 'toy_data/s_294_feature.mat', 'toy_data/s_295_feature.mat', 'toy_data/s_296_feature.mat', 'toy_data/s_297_feature.mat', 'toy_data/s_298_feature.mat', 'toy_data/s_299_feature.mat', 'toy_data/s_300_feature.mat', 'toy_data/s_301_feature.mat', 'toy_data/s_302_feature.mat', 'toy_data/s_303_feature.mat', 'toy_data/s_304_feature.mat', 'toy_data/s_305_feature.mat', 'toy_data/s_306_feature.mat', 'toy_data/s_307_feature.mat', 'toy_data/s_308_feature.mat', 'toy_data/s_309_feature.mat', 'toy_data/s_310_feature.mat', 'toy_data/s_311_feature.mat', 'toy_data/s_312_feature.mat', 'toy_data/s_313_feature.mat', 'toy_data/s_314_feature.mat', 'toy_data/s_315_feature.mat', 'toy_data/s_316_feature.mat', 'toy_data/s_317_feature.mat', 'toy_data/s_318_feature.mat', 'toy_data/s_319_feature.mat', 'toy_data/s_320_feature.mat', 'toy_data/s_321_feature.mat', 'toy_data/s_322_feature.mat', 'toy_data/s_323_feature.mat', 'toy_data/s_324_feature.mat', 'toy_data/s_325_feature.mat', 'toy_data/s_326_feature.mat', 'toy_data/s_327_feature.mat', 'toy_data/s_328_feature.mat', 'toy_data/s_329_feature.mat', 'toy_data/s_330_feature.mat', 'toy_data/s_331_feature.mat', 'toy_data/s_332_feature.mat', 'toy_data/s_333_feature.mat', 'toy_data/s_334_feature.mat', 'toy_data/s_335_feature.mat', 'toy_data/s_336_feature.mat', 'toy_data/s_337_feature.mat', 'toy_data/s_338_feature.mat', 'toy_data/s_339_feature.mat', 'toy_data/s_340_feature.mat', 'toy_data/s_341_feature.mat', 'toy_data/s_342_feature.mat', 'toy_data/s_343_feature.mat', 'toy_data/s_344_feature.mat', 'toy_data/s_345_feature.mat', 'toy_data/s_346_feature.mat', 'toy_data/s_347_feature.mat', 'toy_data/s_348_feature.mat', 'toy_data/s_349_feature.mat', 'toy_data/s_350_feature.mat', 'toy_data/s_351_feature.mat', 'toy_data/s_352_feature.mat', 'toy_data/s_353_feature.mat', 'toy_data/s_354_feature.mat', 'toy_data/s_355_feature.mat', 'toy_data/s_356_feature.mat', 'toy_data/s_357_feature.mat', 'toy_data/s_358_feature.mat', 'toy_data/s_359_feature.mat', 'toy_data/s_360_feature.mat', 'toy_data/s_361_feature.mat', 'toy_data/s_362_feature.mat', 'toy_data/s_363_feature.mat', 'toy_data/s_364_feature.mat', 'toy_data/s_365_feature.mat', 'toy_data/s_366_feature.mat', 'toy_data/s_367_feature.mat', 'toy_data/s_368_feature.mat', 'toy_data/s_369_feature.mat', 'toy_data/s_370_feature.mat', 'toy_data/s_371_feature.mat', 'toy_data/s_372_feature.mat', 'toy_data/s_373_feature.mat', 'toy_data/s_374_feature.mat', 'toy_data/s_375_feature.mat', 'toy_data/s_376_feature.mat', 'toy_data/s_377_feature.mat', 'toy_data/s_378_feature.mat', 'toy_data/s_379_feature.mat', 'toy_data/s_380_feature.mat', 'toy_data/s_381_feature.mat', 'toy_data/s_382_feature.mat', 'toy_data/s_383_feature.mat', 'toy_data/s_384_feature.mat', 'toy_data/s_385_feature.mat', 'toy_data/s_386_feature.mat', 'toy_data/s_387_feature.mat', 'toy_data/s_388_feature.mat', 'toy_data/s_389_feature.mat', 'toy_data/s_390_feature.mat', 'toy_data/s_391_feature.mat', 'toy_data/s_392_feature.mat', 'toy_data/s_393_feature.mat', 'toy_data/s_394_feature.mat', 'toy_data/s_395_feature.mat', 'toy_data/s_396_feature.mat', 'toy_data/s_397_feature.mat', 'toy_data/s_398_feature.mat', 'toy_data/s_399_feature.mat', 'toy_data/s_400_feature.mat', 'toy_data/s_401_feature.mat', 'toy_data/s_402_feature.mat', 'toy_data/s_403_feature.mat', 'toy_data/s_404_feature.mat', 'toy_data/s_405_feature.mat', 'toy_data/s_406_feature.mat', 'toy_data/s_407_feature.mat', 'toy_data/s_408_feature.mat', 'toy_data/s_409_feature.mat', 'toy_data/s_410_feature.mat', 'toy_data/s_411_feature.mat', 'toy_data/s_412_feature.mat', 'toy_data/s_413_feature.mat', 'toy_data/s_414_feature.mat', 'toy_data/s_415_feature.mat', 'toy_data/s_416_feature.mat', 'toy_data/s_417_feature.mat', 'toy_data/s_418_feature.mat', 'toy_data/s_419_feature.mat', 'toy_data/s_420_feature.mat', 'toy_data/s_421_feature.mat', 'toy_data/s_422_feature.mat', 'toy_data/s_423_feature.mat', 'toy_data/s_424_feature.mat', 'toy_data/s_425_feature.mat', 'toy_data/s_426_feature.mat', 'toy_data/s_427_feature.mat', 'toy_data/s_428_feature.mat', 'toy_data/s_429_feature.mat', 'toy_data/s_430_feature.mat', 'toy_data/s_431_feature.mat', 'toy_data/s_432_feature.mat', 'toy_data/s_433_feature.mat', 'toy_data/s_434_feature.mat', 'toy_data/s_435_feature.mat', 'toy_data/s_436_feature.mat', 'toy_data/s_437_feature.mat', 'toy_data/s_438_feature.mat', 'toy_data/s_439_feature.mat', 'toy_data/s_440_feature.mat', 'toy_data/s_441_feature.mat', 'toy_data/s_442_feature.mat', 'toy_data/s_443_feature.mat', 'toy_data/s_444_feature.mat', 'toy_data/s_445_feature.mat', 'toy_data/s_446_feature.mat', 'toy_data/s_447_feature.mat', 'toy_data/s_448_feature.mat', 'toy_data/s_449_feature.mat', 'toy_data/s_450_feature.mat', 'toy_data/s_451_feature.mat', 'toy_data/s_452_feature.mat', 'toy_data/s_453_feature.mat', 'toy_data/s_454_feature.mat', 'toy_data/s_455_feature.mat', 'toy_data/s_456_feature.mat', 'toy_data/s_457_feature.mat', 'toy_data/s_458_feature.mat', 'toy_data/s_459_feature.mat', 'toy_data/s_460_feature.mat', 'toy_data/s_461_feature.mat', 'toy_data/s_462_feature.mat', 'toy_data/s_463_feature.mat', 'toy_data/s_464_feature.mat', 'toy_data/s_465_feature.mat', 'toy_data/s_466_feature.mat', 'toy_data/s_467_feature.mat', 'toy_data/s_468_feature.mat', 'toy_data/s_469_feature.mat', 'toy_data/s_470_feature.mat', 'toy_data/s_471_feature.mat', 'toy_data/s_472_feature.mat', 'toy_data/s_473_feature.mat', 'toy_data/s_474_feature.mat', 'toy_data/s_475_feature.mat', 'toy_data/s_476_feature.mat', 'toy_data/s_477_feature.mat', 'toy_data/s_478_feature.mat', 'toy_data/s_479_feature.mat', 'toy_data/s_480_feature.mat', 'toy_data/s_481_feature.mat', 'toy_data/s_482_feature.mat', 'toy_data/s_483_feature.mat', 'toy_data/s_484_feature.mat', 'toy_data/s_485_feature.mat', 'toy_data/s_486_feature.mat', 'toy_data/s_487_feature.mat', 'toy_data/s_488_feature.mat', 'toy_data/s_489_feature.mat', 'toy_data/s_490_feature.mat', 'toy_data/s_491_feature.mat', 'toy_data/s_492_feature.mat', 'toy_data/s_493_feature.mat', 'toy_data/s_494_feature.mat', 'toy_data/s_495_feature.mat', 'toy_data/s_496_feature.mat', 'toy_data/s_497_feature.mat', 'toy_data/s_498_feature.mat', 'toy_data/s_499_feature.mat', 'toy_data/s_500_feature.mat']\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"toy_data/\"\n",
    "all_files = glob(data_dir +'*.mat')\n",
    "print((all_files))\n",
    "\n",
    "label_file = [file_name if file_name.split('.')[0].split('/')[-1] == 'label' else None for file_name in all_files ]\n",
    "label_file = [file_name for file_name in label_file if file_name is not None]\n",
    "\n",
    "label_file\n",
    "\n",
    "\n",
    "feature_file = [file_name if file_name.split('.')[0].split('_')[-1] == 'feature' else None for file_name in all_files ]\n",
    "feature_file = [file_name for file_name in feature_file if file_name is not None]\n",
    "sorted_feature_file = sorted(feature_file, key=lambda x: int(x.split('.')[0].split('_')[2]))\n",
    "print(sorted_feature_file)\n",
    "\n",
    "# len(feature_file)\n",
    "\n",
    "cluster_file = [file_name if file_name.split('.')[0].split('_')[-1] == 'index' else None for file_name in all_files ]\n",
    "cluster_file = [file_name for file_name in cluster_file if file_name is not None]\n",
    "sorted_cluster_file = sorted(cluster_file, key=lambda x: int(x.split('.')[0].split('_')[2]))\n",
    "# print(sorted_cluster_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d48fab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "531011f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame using dictionary - each key becomes a column\n",
    "# sorted_feature_file and sorted_cluster_file are lists of file paths (500 elements each)\n",
    "# labels is a numpy array of shape (500, 1), so we flatten it to (500,)\n",
    "df = pd.DataFrame({\n",
    "    'feature_file': sorted_feature_file,\n",
    "    'cluster_file': sorted_cluster_file,\n",
    "    'label': labels.flatten()  # Flatten from (500, 1) to (500,)\n",
    "})\n",
    "# len(df)\n",
    "# df.to_csv('data.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8ae3e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237b384c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset: 500 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 500/500 [00:08<00:00, 55.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 2 different shapes:\n",
      "  Shape (399, 1632): 14 files\n",
      "NON-STANDARD! First 5 files:\n",
      "      - Row 3: toy_data/s_4_feature.mat\n",
      "      - Row 5: toy_data/s_6_feature.mat\n",
      "      - Row 172: toy_data/s_173_feature.mat\n",
      "      - Row 174: toy_data/s_175_feature.mat\n",
      "      - Row 210: toy_data/s_211_feature.mat\n",
      "  Shape (400, 1632): 486 files\n",
      "Cleaned dataset: 486 samples (removed 14 files)\n",
      "Original: 500  Cleaned: 486\n",
      "Train set: 340 samples (70.0%)\n",
      "Validation set: 73 samples (15.0%)\n",
      "Test set: 73 samples (15.0%)\n",
      "\n",
      "Train label distribution:\n",
      "label\n",
      "1    171\n",
      "2    169\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Val label distribution:\n",
      "label\n",
      "1    37\n",
      "2    36\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Test label distribution:\n",
      "label\n",
      "1    37\n",
      "2    36\n",
      "Name: count, dtype: int64\n",
      "\n",
      " Problematic files saved to 'toy_data/split/problematic_files.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# # Step 1: Load the original dataframe\n",
    "# from tqdm import tqdm\n",
    "# df = pd.read_csv('toy_data/split/data.csv')\n",
    "# print(f\"Original dataset: {len(df)} samples\")\n",
    "\n",
    "# shape_dict = {}\n",
    "# problematic_indices = []\n",
    "# expected_shape = (400, 1632)\n",
    "\n",
    "# for idx in tqdm(range(len(df))):\n",
    "#     row = df.iloc[idx]\n",
    "#     feature_file = row['feature']\n",
    "    \n",
    "#     try:\n",
    "#         f_data = sio.loadmat(feature_file)\n",
    "#         f_mat = f_data['feature_mat']\n",
    "#         shape = f_mat.shape\n",
    "        \n",
    "#         if shape not in shape_dict:\n",
    "#             shape_dict[shape] = []\n",
    "#         shape_dict[shape].append(idx)\n",
    "        \n",
    "#         # Check if shape doesn't match expected\n",
    "#         if shape[0] != expected_shape[0]:\n",
    "#             problematic_indices.append(idx)\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error loading {feature_file}: {e}\")\n",
    "#         problematic_indices.append(idx)\n",
    "\n",
    "# print(f\"\\nFound {len(shape_dict)} different shapes:\")\n",
    "# for shape, indices in sorted(shape_dict.items()):\n",
    "#     print(f\"  Shape {shape}: {len(indices)} files\")\n",
    "#     if shape[0] != expected_shape[0]:\n",
    "#         print(f\"NON-STANDARD! First 5 files:\")\n",
    "#         for i in indices[:5]:\n",
    "#             print(f\"      - Row {i}: {df.iloc[i]['feature']}\")\n",
    "\n",
    "# # print(f\"FILTERING OUT {len(problematic_indices)} PROBLEMATIC FILES...\")\n",
    "\n",
    "# df_clean = df.drop(index=problematic_indices).reset_index(drop=True)\n",
    "# print(f\"Cleaned dataset: {len(df_clean)} samples (removed {len(problematic_indices)} files)\")\n",
    "# print(f\"Original: {len(df)}  Cleaned: {len(df_clean)}\")\n",
    "\n",
    "\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# RANDOM_SEED = 42\n",
    "\n",
    "# train_val_df, test_df = train_test_split(\n",
    "#     df_clean, \n",
    "#     test_size=0.15, \n",
    "#     random_state=RANDOM_SEED,\n",
    "#     stratify=df_clean['label'] \n",
    "# )\n",
    "\n",
    "# train_df, val_df = train_test_split(\n",
    "#     train_val_df,\n",
    "#     test_size=15/85,  \n",
    "#     random_state=RANDOM_SEED,\n",
    "#     stratify=train_val_df['label']  \n",
    "# )\n",
    "\n",
    "\n",
    "# print(f\"Train set: {len(train_df)} samples ({len(train_df)/len(df_clean)*100:.1f}%)\")\n",
    "# print(f\"Validation set: {len(val_df)} samples ({len(val_df)/len(df_clean)*100:.1f}%)\")\n",
    "# print(f\"Test set: {len(test_df)} samples ({len(test_df)/len(df_clean)*100:.1f}%)\")\n",
    "# print(f\"\\nTrain label distribution:\\n{train_df['label'].value_counts().sort_index()}\")\n",
    "# print(f\"\\nVal label distribution:\\n{val_df['label'].value_counts().sort_index()}\")\n",
    "# print(f\"\\nTest label distribution:\\n{test_df['label'].value_counts().sort_index()}\")\n",
    "\n",
    "# # Save cleaned splits\n",
    "# train_df.to_csv('toy_data/split/train_df.csv', index=False)\n",
    "# val_df.to_csv('toy_data/split/val_df.csv', index=False)\n",
    "# test_df.to_csv('toy_data/split/test_df.csv', index=False)\n",
    "\n",
    "# # Save the problematic files list for reference\n",
    "# if problematic_indices:\n",
    "#     problematic_df = df.iloc[problematic_indices]\n",
    "#     problematic_df.to_csv('toy_data/split/problematic_files.csv', index=False)\n",
    "#     print(f\"\\n Problematic files saved to 'toy_data/split/problematic_files.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a2171611",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv ('/mnt/storage1/shourovj/shourovj_works/Auburn/data_split/train_df.csv')\n",
    "val_df = pd.read_csv ('/mnt/storage1/shourovj/shourovj_works/Auburn/data_split/val_df.csv')\n",
    "test_df = pd.read_csv ('/mnt/storage1/shourovj/shourovj_works/Auburn/data_split/test_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "df7c882e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "340"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "1d4124f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>cluster</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>toy_data/s_119_feature.mat</td>\n",
       "      <td>toy_data/s_119_cluster_index.mat</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>toy_data/s_437_feature.mat</td>\n",
       "      <td>toy_data/s_437_cluster_index.mat</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>toy_data/s_441_feature.mat</td>\n",
       "      <td>toy_data/s_441_cluster_index.mat</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>toy_data/s_416_feature.mat</td>\n",
       "      <td>toy_data/s_416_cluster_index.mat</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>toy_data/s_315_feature.mat</td>\n",
       "      <td>toy_data/s_315_cluster_index.mat</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      feature                           cluster  label\n",
       "0  toy_data/s_119_feature.mat  toy_data/s_119_cluster_index.mat      1\n",
       "1  toy_data/s_437_feature.mat  toy_data/s_437_cluster_index.mat      2\n",
       "2  toy_data/s_441_feature.mat  toy_data/s_441_cluster_index.mat      2\n",
       "3  toy_data/s_416_feature.mat  toy_data/s_416_cluster_index.mat      2\n",
       "4  toy_data/s_315_feature.mat  toy_data/s_315_cluster_index.mat      2"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "96e817b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature file path: toy_data/s_119_feature.mat\n",
      "(400, 1632)\n",
      "cluster file path: toy_data/s_119_cluster_index.mat\n",
      "(45, 54, 45)\n",
      "Label: 1\n"
     ]
    }
   ],
   "source": [
    "sample_feature_file = train_df.iloc[0]['feature']\n",
    "print(f\"Feature file path: {sample_feature_file}\")\n",
    "sample_feature_data = sio.loadmat(sample_feature_file)\n",
    "# print(f\"Keys in feature file: {sample_feature_data.keys()}\")\n",
    "print(sample_feature_data['feature_mat'].shape)\n",
    "\n",
    "\n",
    "sample_cluster_file = train_df.iloc[0]['cluster']\n",
    "print(f\"cluster file path: {sample_cluster_file}\")\n",
    "sample_cluster_data = sio.loadmat(sample_cluster_file)\n",
    "# print(f\"Keys in feature file: {sample_cluster_data.keys()}\")\n",
    "print(sample_cluster_data['cluster_index_mat'].shape)\n",
    "\n",
    "sample_label = train_df.iloc[0]['label']\n",
    "print(f\"Label: {sample_label}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a30a03c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Improved ModelConfig created with:\n",
      "   - hidden_dim: 64 (reduced from 128)\n",
      "   - num_layers: 3 (reduced from 6)\n",
      "   - num_heads: 4 (reduced from 8)\n",
      "   - dropout: 0.4 (increased from 0.1)\n",
      "\n",
      "  IMPORTANT: Recreate the model with this new config!\n",
      "   model = BrainNet(ModelConfig())\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8073a841",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895e7990",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c29c431",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "5fbf816a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.utils import data\n",
    "\n",
    "class MRIDataset(data.Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        feature_file = row['feature']\n",
    "        cluster_file = row['cluster']\n",
    "        label = row['label']\n",
    "\n",
    "        # Load .mat files\n",
    "        f_data = sio.loadmat(feature_file)\n",
    "        f_mat = f_data['feature_mat']  # Shape: (400, 1632) - ROI features\n",
    "        c_data = sio.loadmat(cluster_file)\n",
    "        c_mat = c_data['cluster_index_mat']  # Shape: (45, 54, 45) - Cluster indices\n",
    "\n",
    "        # Convert to tensors\n",
    "        f_mat = torch.FloatTensor(f_mat)  # Feature matrix\n",
    "        c_mat = torch.LongTensor(c_mat)  # Cluster indices (integers)\n",
    "        label = torch.LongTensor([label - 1])  # Convert to 0-indexed (1,2 -> 0,1)\n",
    "\n",
    "        return f_mat, c_mat, label\n",
    "\n",
    "\n",
    "train_dataset = MRIDataset(train_df)\n",
    "val_dataset = MRIDataset(val_df)\n",
    "test_dataset = MRIDataset(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3132472e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "173b64b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = data.DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = data.DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "test_loader = data.DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "ex = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "7844f8d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 400, 1632])\n",
      "torch.Size([16, 45, 54, 45])\n",
      "torch.Size([16, 1])\n"
     ]
    }
   ],
   "source": [
    "feature = ex[0]\n",
    "cluster = ex[1]\n",
    "label = ex[2]\n",
    "\n",
    "print(feature.shape)\n",
    "print(cluster.shape)\n",
    "print(label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441f98ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1d4b0e66",
   "metadata": {},
   "source": [
    "Without positional encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "03ba24ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# import einops\n",
    "\n",
    "# class ModelConfig:\n",
    "#     \"\"\"Configuration for Atlas-free Brain Network Transformer\"\"\"\n",
    "#     feature_dim = 1632  # ROI feature dimension\n",
    "#     num_rois = 400  # Number of ROIs\n",
    "#     cluster_shape = (45, 54, 45)  # 3D cluster index shape\n",
    "#     hidden_dim = 16  # Latent dimension V (projection dimension)\n",
    "#     num_heads = 4  # Number of attention heads\n",
    "#     num_layers = 1  # Number of transformer layers\n",
    "#     intermediate_dim = 32*4  # Feed-forward network dimension\n",
    "#     dropout = 0.4\n",
    "#     block_size = 9  # Spatial block size (K x K x K)\n",
    "#     block_stride = 5  # Stride for block pooling\n",
    "#     output_dim = 2  # Binary classification\n",
    "\n",
    "\n",
    "\n",
    "# class MultiHeadAttention(nn.Module):\n",
    "#     \"\"\"Standard Multi-Head Self-Attention\"\"\"\n",
    "#     def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "#         super().__init__()\n",
    "#         assert d_model % num_heads == 0\n",
    "        \n",
    "#         self.d_model = d_model\n",
    "#         self.num_heads = num_heads\n",
    "#         self.head_dim = d_model // num_heads\n",
    "        \n",
    "#         self.w_q = nn.Linear(d_model, d_model)\n",
    "#         self.w_k = nn.Linear(d_model, d_model)\n",
    "#         self.w_v = nn.Linear(d_model, d_model)\n",
    "#         self.w_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "#         self.scale = self.head_dim ** -0.5\n",
    "        \n",
    "#     def forward(self, x, mask=None):\n",
    "#         batch_size, seq_len, d_model = x.size()\n",
    "        \n",
    "#         # Linear projections\n",
    "#         Q = self.w_q(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "#         K = self.w_k(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "#         V = self.w_v(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "#         # Scaled dot-product attention\n",
    "#         scores = torch.matmul(Q, K.transpose(-2, -1)) * self.scale\n",
    "        \n",
    "#         if mask is not None:\n",
    "#             scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "#         attn_weights = F.softmax(scores, dim=-1)\n",
    "#         attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "#         # Apply attention to values\n",
    "#         attn_output = torch.matmul(attn_weights, V)\n",
    "        \n",
    "#         # Concatenate heads\n",
    "#         attn_output = attn_output.transpose(1, 2).contiguous().view(\n",
    "#             batch_size, seq_len, d_model\n",
    "#         )\n",
    "        \n",
    "#         # Final linear projection\n",
    "#         output = self.w_o(attn_output)\n",
    "#         return output\n",
    "\n",
    "\n",
    "# class TransformerBlock(nn.Module):\n",
    "#     \"\"\"Standard Transformer Encoder Block\"\"\"\n",
    "#     def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "#         super().__init__()\n",
    "\n",
    "#         self.norm1 = nn.LayerNorm(d_model)\n",
    "#         self.attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "\n",
    "#         self.norm2 = nn.LayerNorm(d_model)\n",
    "#         self.ffn = nn.Sequential(\n",
    "#             nn.Linear(d_model, d_ff),\n",
    "#             nn.GELU(),\n",
    "#             nn.Dropout(dropout),\n",
    "#             nn.Linear(d_ff, d_model),\n",
    "#             nn.Dropout(dropout)\n",
    "#         )\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "#     def forward(self, x, mask=None):\n",
    "#         # Self-attention with residual connection\n",
    "#         x = x + self.dropout(self.attn(self.norm1(x), mask))\n",
    "#         # Feed-forward with residual connection\n",
    "#         x = x + self.ffn(self.norm2(x))\n",
    "#         return x\n",
    "\n",
    "\n",
    "# class BrainNet(nn.Module):\n",
    "#     \"\"\"\n",
    "#     Atlas-free Brain Network Transformer\n",
    "    \n",
    "#     Architecture:\n",
    "#     1. ROI Connectivity Projection (g  q): Linear projection from feature_dim to hidden_dim\n",
    "#     2. 3D Multi-channel Brain Map Construction (Q): Map ROI features to voxel space using cluster indices\n",
    "#     3. Spatial Block Tokenization: Divide 3D volume into blocks and pool\n",
    "#     4. Transformer Encoder: Process spatial tokens\n",
    "#     5. Global Readout & Classification: Mean pool and classify\n",
    "#     \"\"\"\n",
    "#     def __init__(self, config):\n",
    "#         super().__init__()\n",
    "#         self.config = config\n",
    "        \n",
    "\n",
    "#         # Step 1: ROI Connectivity Projection (g  q)\n",
    "#         # Simple linear projection as per paper (can be extended to MLP if needed)\n",
    "#         self.roi_projection = nn.Linear(config.feature_dim, config.hidden_dim)\n",
    "#         # self.roi_projection = nn.Sequential(\n",
    "#         #     nn.Linear(1632, 16), \n",
    "#         #     # nn.BatchNorm1d(400), # Standardize across ROIs\n",
    "#         #     nn.GELU(),\n",
    "#         #     nn.Dropout(0.5),     # Force the model to not rely on specific input features\n",
    "#         #     nn.Linear(16, 32)    \n",
    "#         # )\n",
    "        \n",
    "#         # Step 2-3: Spatial Block Tokenization will be done in forward pass\n",
    "#         # We'll use adaptive pooling to handle variable spatial dimensions\n",
    "        \n",
    "#         # Step 4: Transformer Encoder\n",
    "#         self.transformer_blocks = nn.ModuleList([\n",
    "#             TransformerBlock(\n",
    "#                 config.hidden_dim,\n",
    "#                 config.num_heads,\n",
    "#                 config.intermediate_dim,\n",
    "#                 config.dropout\n",
    "#             )\n",
    "#             for _ in range(config.num_layers)\n",
    "#         ])\n",
    "        \n",
    "#         # Step 5: Global Readout & Classification\n",
    "#         self.global_pool = nn.AdaptiveAvgPool1d(1)  # Global average pooling\n",
    "#         self.classifier = nn.Sequential(\n",
    "#             nn.LayerNorm(config.hidden_dim),\n",
    "#             nn.Dropout(config.dropout),\n",
    "#             nn.Linear(config.hidden_dim, config.output_dim)\n",
    "#         )\n",
    "    \n",
    "#     def _construct_3d_brain_map(self, q, c_mat):\n",
    "#         \"\"\"\n",
    "#         Construct 3D Multi-channel Brain Map (Q) from ROI features and cluster indices.\n",
    "        \n",
    "#         The cluster index matrix C contains:\n",
    "#         - 0: background (should remain zero in output)\n",
    "#         - 1-400: ROI indices (i-th ROI corresponds to (i-1)-th row of feature matrix F)\n",
    "        \n",
    "#         The feature matrix F has shape [400, feature_dim], where:\n",
    "#         - Row 0 corresponds to ROI 1\n",
    "#         - Row 1 corresponds to ROI 2\n",
    "#         - ...\n",
    "#         - Row 399 corresponds to ROI 400\n",
    "        \n",
    "#         Args:\n",
    "#             q: Projected ROI features [batch_size, num_rois, hidden_dim]\n",
    "#                q[b, i] is the projected feature for ROI (i+1) (i-th row of F)\n",
    "#             c_mat: Cluster index matrix [batch_size, D, H, W]\n",
    "#                    Contains ROI indices: 0 (background), 1-400 (ROI indices)\n",
    "        \n",
    "#         Returns:\n",
    "#             Q: 3D brain map [batch_size, hidden_dim, D, H, W]\n",
    "#                Background voxels (c_mat == 0) remain zero in Q\n",
    "#         \"\"\"\n",
    "#         batch_size, num_rois, hidden_dim = q.shape\n",
    "#         D, H, W = c_mat.shape[1], c_mat.shape[2], c_mat.shape[3]\n",
    "        \n",
    "#         # Initialize output tensor (background will remain zero)\n",
    "#         Q = torch.zeros(batch_size, hidden_dim, D, H, W, \n",
    "#                         device=q.device, dtype=q.dtype)\n",
    "        \n",
    "#         # Process each sample in the batch\n",
    "#         for b in range(batch_size):\n",
    "#             # Get cluster indices for this batch [D, H, W]\n",
    "#             cluster_indices = c_mat[b]  # Contains: 0 (background), 1-400 (ROI indices)\n",
    "            \n",
    "#             # Get projected features for this batch [num_rois, hidden_dim]\n",
    "#             # q[b, i] corresponds to ROI (i+1), so q[b, 0] = ROI 1, q[b, 399] = ROI 400\n",
    "#             roi_features = q[b]\n",
    "            \n",
    "#             # Flatten spatial dimensions to get all voxel positions\n",
    "#             cluster_indices_flat = cluster_indices.flatten()  # [D*H*W]\n",
    "            \n",
    "#             # Create mask for non-background voxels (ROI indices 1-400)\n",
    "#             non_bg_mask = (cluster_indices_flat > 0) & (cluster_indices_flat <= num_rois)\n",
    "            \n",
    "#             # Convert ROI indices from 1-based (1-400) to 0-based (0-399) for indexing into q\n",
    "#             # ROI 1 -> index 0, ROI 2 -> index 1, ..., ROI 400 -> index 399\n",
    "#             roi_indices = (cluster_indices_flat[non_bg_mask] - 1).long()\n",
    "            \n",
    "#             # Map non-background voxels to their corresponding ROI feature vectors\n",
    "#             # Background voxels (value 0) remain zero in Q\n",
    "#             voxel_features = torch.zeros(D * H * W, hidden_dim, \n",
    "#                                         device=q.device, dtype=q.dtype)\n",
    "#             voxel_features[non_bg_mask] = roi_features[roi_indices]  # [num_non_bg, hidden_dim]\n",
    "            \n",
    "#             # Reshape back to 3D spatial structure\n",
    "#             # Transpose to get [hidden_dim, D, H, W] format\n",
    "#             Q[b] = voxel_features.view(D, H, W, hidden_dim).permute(3, 0, 1, 2)\n",
    "        \n",
    "#         return Q\n",
    "        \n",
    "#     def forward(self, f_mat, c_mat):\n",
    "#         \"\"\"\n",
    "#         Forward pass\n",
    "        \n",
    "#         Args:\n",
    "#             f_mat: Feature matrix [batch_size, num_rois, feature_dim] - ROI features\n",
    "#             c_mat: Cluster index matrix [batch_size, D, H, W] - 3D cluster indices\n",
    "        \n",
    "#         Returns:\n",
    "#             logits: [batch_size, output_dim]\n",
    "#         \"\"\"\n",
    "#         batch_size = f_mat.size(0)\n",
    "        \n",
    "#         # Step 1: ROI Connectivity Projection (g  q)\n",
    "#         # f_mat: [B, num_rois, feature_dim]  [B, num_rois, hidden_dim]\n",
    "#         q = self.roi_projection(f_mat)  # [B, num_rois, hidden_dim]\n",
    "        \n",
    "#         # Step 2: 3D Multi-channel Brain Map Construction (Q)\n",
    "#         # Map ROI features back to voxel space using cluster indices\n",
    "#         # Each voxel position in c_mat contains an ROI index (1-400)\n",
    "#         # This function maps the corresponding ROI feature vector to each voxel position\n",
    "#         Q = self._construct_3d_brain_map(q, c_mat)  # [B, hidden_dim, D, H, W]\n",
    "        \n",
    "#         D, H, W = Q.shape[2], Q.shape[3], Q.shape[4]\n",
    "        \n",
    "#         # Step 3: Spatial Block Tokenization\n",
    "#         # ====================================\n",
    "#         # This step divides the 3D brain volume into smaller blocks and creates \"tokens\" \n",
    "#         # (like words in a sentence) for the Transformer to process.\n",
    "#         #\n",
    "#         # INPUT: Q [B, hidden_dim, D=45, H=54, W=45] - 3D brain map with features at each voxel\n",
    "#         #\n",
    "#         # PROCESS:\n",
    "#         # 1. Divide volume into overlapping blocks of size block_sizeblock_sizeblock_size (e.g., 999)\n",
    "#         # 2. For each block, sum all the feature values within that block\n",
    "#         # 3. This creates one \"token\" per block\n",
    "#         #\n",
    "#         # EXAMPLE (simplified 2D):\n",
    "#         #   Original: [4554] voxels\n",
    "#         #   Blocks: 99 blocks with stride 5\n",
    "#         #   Output: ~810 blocks = 80 tokens\n",
    "#         #\n",
    "#         # WHY SUM-POOLING?\n",
    "#         # The paper uses sum-pooling (not average) because:\n",
    "#         # - It preserves the total \"activation\" in each spatial region\n",
    "#         # - More sensitive to the amount of information in each block\n",
    "#         # - Better for capturing spatial patterns\n",
    "        \n",
    "#         # Use 3D average pooling with kernel=block_size and stride=block_stride\n",
    "#         # PyTorch doesn't have direct sum-pooling, so we:\n",
    "#         # 1. Use avg_pool3d to get average values in each block\n",
    "#         # 2. Multiply by block_size^3 to convert to sum\n",
    "#         #    (sum = average  number_of_elements_in_block)\n",
    "#         #    For 999 block: sum = avg  729\n",
    "#         Q_pooled = F.avg_pool3d(\n",
    "#             Q,\n",
    "#             kernel_size=self.config.block_size,  # e.g., 9 (creates 999 blocks)\n",
    "#             stride=self.config.block_stride,     # e.g., 5 (blocks overlap)\n",
    "#             padding=0  # No padding - only process valid blocks\n",
    "#         ) * (self.config.block_size ** 3)  # Convert avg  sum: multiply by voxels per block\n",
    "        \n",
    "#         # Get output dimensions after pooling\n",
    "#         # For D=45, H=54, W=45 with block_size=9, stride=5:\n",
    "#         #   D_out = (45 - 9) // 5 + 1 = 8\n",
    "#         #   H_out = (54 - 9) // 5 + 1 = 10  \n",
    "#         #   W_out = (45 - 9) // 5 + 1 = 8\n",
    "#         #   Total tokens = 8  10  8 = 640 tokens\n",
    "#         D_out, H_out, W_out = Q_pooled.shape[2], Q_pooled.shape[3], Q_pooled.shape[4]\n",
    "        \n",
    "#         # Flatten spatial dimensions to create sequence of tokens for Transformer\n",
    "#         # Q_pooled: [B, hidden_dim, D_out, H_out, W_out]\n",
    "#         # tokens: [B, num_tokens, hidden_dim] - each token is a spatial block\n",
    "#         num_tokens = D_out * H_out * W_out\n",
    "#         tokens = Q_pooled.view(batch_size, self.config.hidden_dim, num_tokens)\n",
    "#         tokens = tokens.transpose(1, 2)  # [B, num_tokens, hidden_dim]\n",
    "        \n",
    "#         # Now we have a sequence of tokens where each token represents a spatial block\n",
    "#         # The Transformer will process these tokens to learn relationships between\n",
    "#         # different brain regions (spatial blocks)\n",
    "        \n",
    "#         # Step 4: Transformer Encoder\n",
    "#         # Process the sequence of spatial tokens through transformer blocks\n",
    "#         # Each token can now \"attend\" to all other tokens to learn relationships\n",
    "#         x = tokens  # [B, num_tokens, hidden_dim]\n",
    "#         for transformer_block in self.transformer_blocks:\n",
    "#             x = transformer_block(x)  # Still [B, num_tokens, hidden_dim]\n",
    "        \n",
    "#         # Step 5: Global Readout & Classification\n",
    "#         # ======================================\n",
    "#         # GOAL: Convert sequence of tokens  single feature vector per sample\n",
    "#         #\n",
    "#         # After transformer: x = [B, num_tokens=640, hidden_dim=64]\n",
    "#         # We have 640 tokens, each with 64 features\n",
    "#         # We want: [B, hidden_dim=64] - one vector representing the entire brain\n",
    "#         #\n",
    "#         # WHY TRANSPOSE?\n",
    "#         # - AdaptiveAvgPool1d pools over the LAST dimension\n",
    "#         # - We want to pool over tokens (dimension 1), not features (dimension 2)\n",
    "#         # - So we transpose: [B, num_tokens, hidden_dim]  [B, hidden_dim, num_tokens]\n",
    "#         # - Now tokens are the last dimension, so pooling works correctly\n",
    "#         #\n",
    "#         # WHAT DOES GLOBAL POOLING DO?\n",
    "#         # - Takes average of all 640 tokens\n",
    "#         # - Result: One feature vector per sample representing the whole brain\n",
    "#         # - This is the \"readout\" operation - aggregating all spatial information\n",
    "#         x = x.transpose(1, 2)  # [B, hidden_dim, num_tokens] - swap dims for pooling\n",
    "#         x = self.global_pool(x).squeeze(-1)  # [B, hidden_dim, 1]  [B, hidden_dim]\n",
    "        \n",
    "#         # Classification\n",
    "#         # Now we have one feature vector per brain, ready for classification\n",
    "#         logits = self.classifier(x)  # [B, hidden_dim]  [B, output_dim=2]\n",
    "        \n",
    "#         return logits\n",
    "\n",
    "\n",
    "# # Create model instance\n",
    "# config = ModelConfig()\n",
    "# model = BrainNet(config)\n",
    "\n",
    "# # Print model summary\n",
    "# print(f\"Model created with {sum(p.numel() for p in model.parameters())/10e6} parameters\")\n",
    "# print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)/10e6}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d228670d",
   "metadata": {},
   "source": [
    "With positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11f0c0e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " FIXED MODEL CREATED!\n",
      "   Parameters: 0.119M\n",
      "   hidden_dim: 16\n",
      "   num_layers: 1\n",
      "   intermediate_dim: 64 (fixed: hidden_dim * 4)\n",
      "\n",
      " All bugs fixed:\n",
      "   1. BatchNorm1d  LayerNorm\n",
      "   2. intermediate_dim consistent\n",
      "   3. _construct_3d_brain_map fixed\n",
      "   4. Balanced model size\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ModelConfig:\n",
    "    \"\"\"Configuration for Atlas-free Brain Network Transformer (FIXED)\"\"\"\n",
    "    feature_dim = 1632  # ROI feature dimension\n",
    "    num_rois = 400  # Number of ROIs\n",
    "    cluster_shape = (45, 54, 45)  # 3D cluster index shape\n",
    "    hidden_dim = 16  # Balanced size (was 16 - too small, 64 - might overfit)\n",
    "    num_heads = 4  # Number of attention heads\n",
    "    num_layers = 1  # Balanced (was 1 - too small, 3 - might overfit)\n",
    "    intermediate_dim = hidden_dim * 4\n",
    "    dropout = 0.5\n",
    "    block_size = 9  # Spatial block size (K x K x K)\n",
    "    block_stride = 5  # Stride for block pooling\n",
    "    output_dim = 2  # Binary classification\n",
    "    \n",
    "    def __init__(self):\n",
    "        if self.intermediate_dim is None:\n",
    "            self.intermediate_dim = self.hidden_dim * 4  # FIX: Consistent with hidden_dim\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Standard Multi-Head Self-Attention\"\"\"\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        \n",
    "        self.w_q = nn.Linear(d_model, d_model)\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model)\n",
    "        self.w_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, seq_len, d_model = x.size()\n",
    "        \n",
    "        # Linear projections\n",
    "        Q = self.w_q(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = self.w_k(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = self.w_v(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # Scaled dot-product attention\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) * self.scale\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        attn_output = torch.matmul(attn_weights, V)\n",
    "        \n",
    "        # Concatenate heads\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(\n",
    "            batch_size, seq_len, d_model\n",
    "        )\n",
    "        \n",
    "        # Final linear projection\n",
    "        output = self.w_o(attn_output)\n",
    "        return output\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"Standard Transformer Encoder Block\"\"\"\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_ff, d_model),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        # Self-attention with residual connection\n",
    "        x = x + self.dropout(self.attn(self.norm1(x), mask))\n",
    "        # Feed-forward with residual connection\n",
    "        x = x + self.ffn(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "class BrainNet(nn.Module):\n",
    "    \"\"\"Atlas-free Brain Network Transformer (FIXED VERSION)\"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # Step 1: ROI Connectivity Projection (FIXED: BatchNorm1d  LayerNorm)\n",
    "        self.roi_projection = nn.Sequential(\n",
    "            nn.Linear(config.feature_dim, 64),  # Intermediate dimension\n",
    "            nn.LayerNorm(64),  # FIX: LayerNorm works on last dimension [B, num_rois, 64]\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(64, config.hidden_dim)\n",
    "        )\n",
    "\n",
    "        # # Step 1: ROI Connectivity Projection (FIXED: BatchNorm1d  LayerNorm)\n",
    "        # self.roi_projection = nn.Sequential(\n",
    "        #     nn.Linear(config.feature_dim, config.hidden_dim),  # Intermediate dimension\n",
    "        #     nn.LayerNorm(64),  # FIX: LayerNorm works on last dimension [B, num_rois, 64]\n",
    "        #     nn.GELU(),\n",
    "        #     nn.Dropout(0.5),\n",
    "        # )\n",
    "        \n",
    "        # 2. DYNAMIC CALCULATION of Token Count\n",
    "        # Extract dimensions from config\n",
    "        D_in, H_in, W_in = config.cluster_shape\n",
    "        K = config.block_size\n",
    "        S = config.block_stride\n",
    "        \n",
    "        # Calculate the number of nodes after 3D pooling and sum-pooling\n",
    "        self.D_out = (D_in - K) // S + 1\n",
    "        self.H_out = (H_in - K) // S + 1\n",
    "        self.W_out = (W_in - K) // S + 1\n",
    "        \n",
    "        self.num_tokens = self.D_out * self.H_out * self.W_out\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, self.num_tokens, config.hidden_dim))\n",
    "        \n",
    "        # Step 4: Transformer Encoder\n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "            TransformerBlock(\n",
    "                config.hidden_dim,\n",
    "                config.num_heads,\n",
    "                config.intermediate_dim,\n",
    "                config.dropout\n",
    "            )\n",
    "            for _ in range(config.num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Step 5: Global Readout & Classification\n",
    "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(config.hidden_dim),\n",
    "            nn.Dropout(config.dropout),\n",
    "            nn.Linear(config.hidden_dim, config.output_dim)\n",
    "        )\n",
    "    \n",
    "    def _construct_3d_brain_map(self, q, c_mat):\n",
    "\n",
    "        batch_size, num_rois, hidden_dim = q.shape\n",
    "        D, H, W = c_mat.shape[1], c_mat.shape[2], c_mat.shape[3]\n",
    "        \n",
    "        # Initialize output tensor (background will remain zero)\n",
    "        Q = torch.zeros(batch_size, hidden_dim, D, H, W, \n",
    "                        device=q.device, dtype=q.dtype)\n",
    "        \n",
    "        # Process each sample in the batch\n",
    "        for b in range(batch_size):\n",
    "            # Get cluster indices for this batch [D, H, W]\n",
    "            cluster_indices = c_mat[b]  # Contains: 0 (background), 1-400 (ROI indices)\n",
    "            \n",
    "            # Get projected features for this batch [num_rois, hidden_dim]\n",
    "            # q[b, i] corresponds to ROI (i+1), so q[b, 0] = ROI 1, q[b, 399] = ROI 400\n",
    "            roi_features = q[b]\n",
    "            \n",
    "            # Flatten spatial dimensions to get all voxel positions\n",
    "            cluster_indices_flat = cluster_indices.flatten()  # [D*H*W]\n",
    "            \n",
    "            # Create mask for non-background voxels (ROI indices 1-400)\n",
    "            non_bg_mask = (cluster_indices_flat > 0) & (cluster_indices_flat <= num_rois)\n",
    "            \n",
    "            # Convert ROI indices from 1-based (1-400) to 0-based (0-399) for indexing into q\n",
    "            # ROI 1 -> index 0, ROI 2 -> index 1, ..., ROI 400 -> index 399\n",
    "            roi_indices = (cluster_indices_flat[non_bg_mask] - 1).long()\n",
    "            \n",
    "            # Map non-background voxels to their corresponding ROI feature vectors\n",
    "            # Background voxels (value 0) remain zero in Q\n",
    "            voxel_features = torch.zeros(D * H * W, hidden_dim, \n",
    "                                        device=q.device, dtype=q.dtype)\n",
    "            voxel_features[non_bg_mask] = roi_features[roi_indices]  # [num_non_bg, hidden_dim]\n",
    "            \n",
    "            # Reshape back to 3D spatial structure\n",
    "            # Transpose to get [hidden_dim, D, H, W] format\n",
    "            Q[b] = voxel_features.view(D, H, W, hidden_dim).permute(3, 0, 1, 2)\n",
    "        \n",
    "        return Q\n",
    "\n",
    "    \n",
    "    def forward(self, f_mat, c_mat):\n",
    "        batch_size = f_mat.size(0)\n",
    "        \n",
    "        # Step 1: ROI Connectivity Projection\n",
    "        q = self.roi_projection(f_mat)  # [B, num_rois, hidden_dim]\n",
    "        \n",
    "        # Step 2: 3D Multi-channel Brain Map Construction\n",
    "        Q = self._construct_3d_brain_map(q, c_mat)  # [B, hidden_dim, D, H, W]\n",
    "        \n",
    "        # Step 3: Spatial Block Tokenization\n",
    "        Q_pooled = F.avg_pool3d(\n",
    "            Q,\n",
    "            kernel_size=self.config.block_size,\n",
    "            stride=self.config.block_stride,\n",
    "            padding=0\n",
    "        ) * (self.config.block_size ** 3)  # Convert avg  sum\n",
    "        \n",
    "        D_out, H_out, W_out = Q_pooled.shape[2], Q_pooled.shape[3], Q_pooled.shape[4]\n",
    "        num_tokens = D_out * H_out * W_out\n",
    "        \n",
    "        # Reshape if num_tokens changed\n",
    "        if num_tokens != self.num_tokens:\n",
    "            # Recreate positional embedding if needed\n",
    "            self.pos_embedding = nn.Parameter(\n",
    "                torch.randn(1, num_tokens, self.config.hidden_dim, device=Q_pooled.device)\n",
    "            ).to(Q_pooled.device)\n",
    "            self.num_tokens = num_tokens\n",
    "        \n",
    "        tokens = Q_pooled.view(batch_size, self.config.hidden_dim, num_tokens)\n",
    "        tokens = tokens.transpose(1, 2)  # [B, num_tokens, hidden_dim]\n",
    "        \n",
    "        # Step 4: Transformer Encoder\n",
    "        x = tokens + self.pos_embedding\n",
    "        for transformer_block in self.transformer_blocks:\n",
    "            x = transformer_block(x)\n",
    "        \n",
    "        # Step 5: Global Readout & Classification\n",
    "        x = x.transpose(1, 2)  # [B, hidden_dim, num_tokens]\n",
    "        x = self.global_pool(x).squeeze(-1)  # [B, hidden_dim]\n",
    "        logits = self.classifier(x)  # [B, output_dim]\n",
    "        \n",
    "        return logits\n",
    "\n",
    "# Create fixed model\n",
    "config = ModelConfig()\n",
    "model_fixed = BrainNet(config)\n",
    "\n",
    "print(\" FIXED MODEL CREATED!\")\n",
    "print(f\"   Parameters: {sum(p.numel() for p in model_fixed.parameters())/1e6:.3f}M\")\n",
    "print(f\"   hidden_dim: {config.hidden_dim}\")\n",
    "print(f\"   num_layers: {config.num_layers}\")\n",
    "print(f\"   intermediate_dim: {config.intermediate_dim} (fixed: hidden_dim * 4)\")\n",
    "print(\"\\n All bugs fixed:\")\n",
    "print(\"   1. BatchNorm1d  LayerNorm\")\n",
    "print(\"   2. intermediate_dim consistent\")\n",
    "print(\"   3. _construct_3d_brain_map fixed\")\n",
    "print(\"   4. Balanced model size\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778b4416",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e98ffb1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "======================================================================\n",
      "Starting Training\n",
      "======================================================================\n",
      "Model parameters: 0.14M\n",
      "Training samples: 340\n",
      "Validation samples: 73\n",
      "Number of epochs: 50\n",
      "Weight decay: 1e-3 (strong regularization)\n",
      "Early stopping patience: 10\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 [Train]:   5%|         | 1/22 [00:00<00:06,  3.03it/s, loss=1.0993, acc=31.25%]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 [Train]: 100%|| 22/22 [00:07<00:00,  3.11it/s, loss=0.9419, acc=46.76%]\n",
      "Epoch 1/50 [Val]: 100%|| 5/5 [00:01<00:00,  3.50it/s, loss=0.6517, acc=47.95%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/50:\n",
      "  Train Loss: 0.8201 | Train Acc: 46.76%\n",
      "  Val Loss:   0.6855 | Val Acc:   47.95%\n",
      "  LR: 1.00e-04\n",
      "   Saved best model (Val Acc: 47.95%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/50 [Train]: 100%|| 22/22 [00:07<00:00,  3.08it/s, loss=0.5473, acc=55.59%]\n",
      "Epoch 2/50 [Val]: 100%|| 5/5 [00:01<00:00,  3.54it/s, loss=0.6550, acc=47.95%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/50:\n",
      "  Train Loss: 0.6977 | Train Acc: 55.59%\n",
      "  Val Loss:   0.6847 | Val Acc:   47.95%\n",
      "  LR: 1.00e-04\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/50 [Train]: 100%|| 22/22 [00:07<00:00,  3.11it/s, loss=0.7379, acc=56.18%]\n",
      "Epoch 3/50 [Val]: 100%|| 5/5 [00:01<00:00,  3.51it/s, loss=0.6454, acc=53.42%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/50:\n",
      "  Train Loss: 0.6894 | Train Acc: 56.18%\n",
      "  Val Loss:   0.6826 | Val Acc:   53.42%\n",
      "  LR: 1.00e-04\n",
      "   Saved best model (Val Acc: 53.42%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/50 [Train]: 100%|| 22/22 [00:07<00:00,  3.06it/s, loss=0.4628, acc=55.88%]\n",
      "Epoch 4/50 [Val]: 100%|| 5/5 [00:01<00:00,  3.46it/s, loss=0.6473, acc=50.68%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4/50:\n",
      "  Train Loss: 0.6709 | Train Acc: 55.88%\n",
      "  Val Loss:   0.6799 | Val Acc:   50.68%\n",
      "  LR: 1.00e-04\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/50 [Train]: 100%|| 22/22 [00:07<00:00,  3.11it/s, loss=0.4639, acc=65.00%]\n",
      "Epoch 5/50 [Val]: 100%|| 5/5 [00:01<00:00,  3.31it/s, loss=0.6392, acc=50.68%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5/50:\n",
      "  Train Loss: 0.6287 | Train Acc: 65.00%\n",
      "  Val Loss:   0.6763 | Val Acc:   50.68%\n",
      "  LR: 1.00e-04\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/50 [Train]: 100%|| 22/22 [00:07<00:00,  3.07it/s, loss=0.7258, acc=67.94%]\n",
      "Epoch 6/50 [Val]: 100%|| 5/5 [00:01<00:00,  3.48it/s, loss=0.6317, acc=58.90%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6/50:\n",
      "  Train Loss: 0.6210 | Train Acc: 67.94%\n",
      "  Val Loss:   0.6724 | Val Acc:   58.90%\n",
      "  LR: 1.00e-04\n",
      "   Saved best model (Val Acc: 58.90%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/50 [Train]: 100%|| 22/22 [00:07<00:00,  3.09it/s, loss=0.4994, acc=74.12%]\n",
      "Epoch 7/50 [Val]: 100%|| 5/5 [00:01<00:00,  3.45it/s, loss=0.6214, acc=58.90%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7/50:\n",
      "  Train Loss: 0.5768 | Train Acc: 74.12%\n",
      "  Val Loss:   0.6633 | Val Acc:   58.90%\n",
      "  LR: 1.00e-04\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/50 [Train]: 100%|| 22/22 [00:07<00:00,  3.08it/s, loss=0.4524, acc=77.06%]\n",
      "Epoch 8/50 [Val]: 100%|| 5/5 [00:01<00:00,  3.52it/s, loss=0.6165, acc=57.53%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8/50:\n",
      "  Train Loss: 0.5310 | Train Acc: 77.06%\n",
      "  Val Loss:   0.6559 | Val Acc:   57.53%\n",
      "  LR: 1.00e-04\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/50 [Train]: 100%|| 22/22 [00:07<00:00,  3.06it/s, loss=0.3709, acc=89.12%]\n",
      "Epoch 9/50 [Val]: 100%|| 5/5 [00:01<00:00,  3.41it/s, loss=0.5601, acc=64.38%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9/50:\n",
      "  Train Loss: 0.4221 | Train Acc: 89.12%\n",
      "  Val Loss:   0.6339 | Val Acc:   64.38%\n",
      "  LR: 1.00e-04\n",
      "   Saved best model (Val Acc: 64.38%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/50 [Train]: 100%|| 22/22 [00:07<00:00,  3.06it/s, loss=0.4093, acc=96.18%]\n",
      "Epoch 10/50 [Val]: 100%|| 5/5 [00:01<00:00,  3.50it/s, loss=0.5506, acc=64.38%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10/50:\n",
      "  Train Loss: 0.2694 | Train Acc: 96.18%\n",
      "  Val Loss:   0.6360 | Val Acc:   64.38%\n",
      "  LR: 1.00e-04\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/50 [Train]: 100%|| 22/22 [00:07<00:00,  3.11it/s, loss=0.1196, acc=97.06%]\n",
      "Epoch 11/50 [Val]: 100%|| 5/5 [00:01<00:00,  3.51it/s, loss=0.6201, acc=60.27%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11/50:\n",
      "  Train Loss: 0.1761 | Train Acc: 97.06%\n",
      "  Val Loss:   0.7007 | Val Acc:   60.27%\n",
      "  LR: 1.00e-04\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/50 [Train]: 100%|| 22/22 [00:07<00:00,  3.09it/s, loss=0.1970, acc=99.12%]\n",
      "Epoch 12/50 [Val]: 100%|| 5/5 [00:01<00:00,  3.50it/s, loss=0.5207, acc=64.38%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12/50:\n",
      "  Train Loss: 0.1325 | Train Acc: 99.12%\n",
      "  Val Loss:   0.6560 | Val Acc:   64.38%\n",
      "  LR: 1.00e-04\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/50 [Train]: 100%|| 22/22 [00:07<00:00,  3.09it/s, loss=0.0560, acc=99.71%] \n",
      "Epoch 13/50 [Val]: 100%|| 5/5 [00:01<00:00,  3.47it/s, loss=0.5370, acc=68.49%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 13/50:\n",
      "  Train Loss: 0.1004 | Train Acc: 99.71%\n",
      "  Val Loss:   0.6504 | Val Acc:   68.49%\n",
      "  LR: 1.00e-04\n",
      "   Saved best model (Val Acc: 68.49%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/50 [Train]: 100%|| 22/22 [00:07<00:00,  2.99it/s, loss=0.1147, acc=99.71%] \n",
      "Epoch 14/50 [Val]: 100%|| 5/5 [00:01<00:00,  3.46it/s, loss=0.5363, acc=68.49%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14/50:\n",
      "  Train Loss: 0.0740 | Train Acc: 99.71%\n",
      "  Val Loss:   0.6537 | Val Acc:   68.49%\n",
      "  LR: 1.00e-04\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/50 [Train]: 100%|| 22/22 [00:07<00:00,  3.09it/s, loss=0.0357, acc=99.71%]\n",
      "Epoch 15/50 [Val]: 100%|| 5/5 [00:01<00:00,  3.48it/s, loss=0.5611, acc=64.38%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15/50:\n",
      "  Train Loss: 0.0657 | Train Acc: 99.71%\n",
      "  Val Loss:   0.6812 | Val Acc:   64.38%\n",
      "  LR: 5.00e-05\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/50 [Train]: 100%|| 22/22 [00:07<00:00,  3.12it/s, loss=0.0349, acc=100.00%]\n",
      "Epoch 16/50 [Val]: 100%|| 5/5 [00:01<00:00,  3.43it/s, loss=0.5424, acc=69.86%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 16/50:\n",
      "  Train Loss: 0.0579 | Train Acc: 100.00%\n",
      "  Val Loss:   0.6590 | Val Acc:   69.86%\n",
      "  LR: 5.00e-05\n",
      "   Saved best model (Val Acc: 69.86%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/50 [Train]: 100%|| 22/22 [00:07<00:00,  3.13it/s, loss=0.1103, acc=100.00%]\n",
      "Epoch 17/50 [Val]: 100%|| 5/5 [00:01<00:00,  3.49it/s, loss=0.5380, acc=69.86%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 17/50:\n",
      "  Train Loss: 0.0554 | Train Acc: 100.00%\n",
      "  Val Loss:   0.6587 | Val Acc:   69.86%\n",
      "  LR: 5.00e-05\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/50 [Train]: 100%|| 22/22 [00:07<00:00,  3.01it/s, loss=0.0243, acc=100.00%]\n",
      "Epoch 18/50 [Val]: 100%|| 5/5 [00:01<00:00,  3.44it/s, loss=0.5495, acc=69.86%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 18/50:\n",
      "  Train Loss: 0.0532 | Train Acc: 100.00%\n",
      "  Val Loss:   0.6638 | Val Acc:   69.86%\n",
      "  LR: 5.00e-05\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/50 [Train]: 100%|| 22/22 [00:07<00:00,  3.09it/s, loss=0.0209, acc=100.00%]\n",
      "Epoch 19/50 [Val]: 100%|| 5/5 [00:01<00:00,  3.48it/s, loss=0.5593, acc=67.12%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 19/50:\n",
      "  Train Loss: 0.0421 | Train Acc: 100.00%\n",
      "  Val Loss:   0.6735 | Val Acc:   67.12%\n",
      "  LR: 5.00e-05\n",
      "\n",
      "\n",
      "  Early stopping triggered!\n",
      "   Validation loss hasn't improved for 10 epochs\n",
      "   Best validation accuracy: 69.86%\n",
      "   Best validation loss: 0.6339\n",
      "\n",
      "======================================================================\n",
      "Training Complete!\n",
      "Best Validation Accuracy: 69.86%\n",
      "Best Validation Loss: 0.6339\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## Training\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "config = ModelConfig()  # Uses the improved config from previous cell\n",
    "model = BrainNet(config)\n",
    "\n",
    "# Move model to device\n",
    "model = model.to(device)\n",
    "\n",
    "# Loss function and optimizer with STRONGER REGULARIZATION\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=1e-4, weight_decay=1e-3)  # INCREASED from 1e-5 (100x stronger!)\n",
    "\n",
    "# Learning rate scheduler - reduces LR when validation loss plateaus\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, \n",
    "    mode='min', \n",
    "    factor=0.5, \n",
    "    patience=5, \n",
    "    min_lr=1e-6\n",
    ")\n",
    "\n",
    "# Training hyperparameters\n",
    "num_epochs = 50\n",
    "best_val_acc = 0.0\n",
    "best_val_loss = float('inf')\n",
    "patience = 10  # Early stopping patience\n",
    "patience_counter = 0\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accs = []\n",
    "val_accs = []\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Starting Training\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters())/1e6:.2f}M\")\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "print(f\"Number of epochs: {num_epochs}\")\n",
    "print(f\"Weight decay: 1e-3 (strong regularization)\")\n",
    "print(f\"Early stopping patience: {patience}\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # ========== TRAINING PHASE ==========\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    \n",
    "    train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\")\n",
    "    for batch_idx, (f_mat, c_mat, labels) in enumerate(train_pbar):\n",
    "        # Move data to device\n",
    "        f_mat = f_mat.to(device)  # [B, 400, 1632]\n",
    "        c_mat = c_mat.to(device)  # [B, 45, 54, 45]\n",
    "        labels = labels.squeeze().to(device)  # [B] - remove extra dimension\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(f_mat, c_mat)  # [B, 2]\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(logits, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(logits.data, 1)\n",
    "        train_total += labels.size(0)\n",
    "        train_correct += (predicted == labels).sum().item()\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        # Update progress bar\n",
    "        train_pbar.set_postfix({\n",
    "            'loss': f'{loss.item():.4f}',\n",
    "            'acc': f'{100*train_correct/train_total:.2f}%'\n",
    "        })\n",
    "    \n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    train_acc = 100 * train_correct / train_total\n",
    "    train_losses.append(avg_train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    \n",
    "    # ========== VALIDATION PHASE ==========\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        val_pbar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Val]\")\n",
    "        for f_mat, c_mat, labels in val_pbar:\n",
    "            # Move data to device\n",
    "            f_mat = f_mat.to(device)\n",
    "            c_mat = c_mat.to(device)\n",
    "            labels = labels.squeeze().to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            logits = model(f_mat, c_mat)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(logits, labels)\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(logits.data, 1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            # Update progress bar\n",
    "            val_pbar.set_postfix({\n",
    "                'loss': f'{loss.item():.4f}',\n",
    "                'acc': f'{100*val_correct/val_total:.2f}%'\n",
    "            })\n",
    "    \n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_acc = 100 * val_correct / val_total\n",
    "    val_losses.append(avg_val_loss)\n",
    "    val_accs.append(val_acc)\n",
    "    \n",
    "    # Update learning rate scheduler\n",
    "    scheduler.step(avg_val_loss)\n",
    "    \n",
    "    # Print epoch summary\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}:\")\n",
    "    print(f\"  Train Loss: {avg_train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
    "    print(f\"  Val Loss:   {avg_val_loss:.4f} | Val Acc:   {val_acc:.2f}%\")\n",
    "    print(f\"  LR: {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_acc': val_acc,\n",
    "            'train_acc': train_acc,\n",
    "        }, 'best_model.pth')\n",
    "        print(f\"   Saved best model (Val Acc: {val_acc:.2f}%)\\n\")\n",
    "    else:\n",
    "        print()\n",
    "    \n",
    "    # Early stopping based on validation loss\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"\\n  Early stopping triggered!\")\n",
    "            print(f\"   Validation loss hasn't improved for {patience} epochs\")\n",
    "            print(f\"   Best validation accuracy: {best_val_acc:.2f}%\")\n",
    "            print(f\"   Best validation loss: {best_val_loss:.4f}\")\n",
    "            break\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Training Complete!\")\n",
    "print(f\"Best Validation Accuracy: {best_val_acc:.2f}%\")\n",
    "print(f\"Best Validation Loss: {best_val_loss:.4f}\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e0acc8fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x79e09ada2050>]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses)\n",
    "plt.plot(val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "50527052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BrainNet(\n",
      "  (roi_projection): Sequential(\n",
      "    (0): Linear(in_features=1632, out_features=64, bias=True)\n",
      "    (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    (2): GELU(approximate='none')\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=64, out_features=32, bias=True)\n",
      "  )\n",
      "  (transformer_blocks): ModuleList(\n",
      "    (0): TransformerBlock(\n",
      "      (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): MultiHeadAttention(\n",
      "        (w_q): Linear(in_features=32, out_features=32, bias=True)\n",
      "        (w_k): Linear(in_features=32, out_features=32, bias=True)\n",
      "        (w_v): Linear(in_features=32, out_features=32, bias=True)\n",
      "        (w_o): Linear(in_features=32, out_features=32, bias=True)\n",
      "        (dropout): Dropout(p=0.4, inplace=False)\n",
      "      )\n",
      "      (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "      (ffn): Sequential(\n",
      "        (0): Linear(in_features=32, out_features=128, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.4, inplace=False)\n",
      "        (3): Linear(in_features=128, out_features=32, bias=True)\n",
      "        (4): Dropout(p=0.4, inplace=False)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.4, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (global_pool): AdaptiveAvgPool1d(output_size=1)\n",
      "  (classifier): Sequential(\n",
      "    (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "    (1): Dropout(p=0.4, inplace=False)\n",
      "    (2): Linear(in_features=32, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "f51944e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Testing on Test Set\n",
      "======================================================================\n",
      "Loaded best model from epoch 16\n",
      "  Train Acc: 100.00%\n",
      "  Val Acc:   69.86%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|| 5/5 [00:01<00:00,  3.41it/s, loss=0.7451, acc=69.86%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Test Results\n",
      "======================================================================\n",
      "Test Loss:     0.6224\n",
      "Test Accuracy: 69.86%\n",
      "Correct:       51/73\n",
      "======================================================================\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.74      0.62      0.68        37\n",
      "     Class 1       0.67      0.78      0.72        36\n",
      "\n",
      "    accuracy                           0.70        73\n",
      "   macro avg       0.70      0.70      0.70        73\n",
      "weighted avg       0.70      0.70      0.70        73\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[23 14]\n",
      " [ 8 28]]\n",
      "\n",
      "True Negatives:  23\n",
      "False Positives: 14\n",
      "False Negatives: 8\n",
      "True Positives:  28\n",
      "\n",
      "Per-Class Accuracy:\n",
      "  Class 0: 62.16%\n",
      "  Class 1: 77.78%\n",
      "\n",
      "======================================================================\n",
      "Testing Complete!\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## Testing\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Testing on Test Set\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "# Load best model\n",
    "checkpoint = torch.load('best_model.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(f\"Loaded best model from epoch {checkpoint['epoch']+1}\")\n",
    "print(f\"  Train Acc: {checkpoint['train_acc']:.2f}%\")\n",
    "print(f\"  Val Acc:   {checkpoint['val_acc']:.2f}%\")\n",
    "print()\n",
    "\n",
    "# Evaluate on test set\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_pbar = tqdm(test_loader, desc=\"Testing\")\n",
    "    for f_mat, c_mat, labels in test_pbar:\n",
    "        # Move data to device\n",
    "        f_mat = f_mat.to(device)\n",
    "        c_mat = c_mat.to(device)\n",
    "        labels = labels.squeeze().to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = model(f_mat, c_mat)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(logits, labels)\n",
    "        \n",
    "        # Get predictions\n",
    "        _, predicted = torch.max(logits.data, 1)\n",
    "        \n",
    "        # Store predictions and labels for detailed analysis\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        # Calculate metrics\n",
    "        test_total += labels.size(0)\n",
    "        test_correct += (predicted == labels).sum().item()\n",
    "        test_loss += loss.item()\n",
    "        \n",
    "        # Update progress bar\n",
    "        test_pbar.set_postfix({\n",
    "            'loss': f'{loss.item():.4f}',\n",
    "            'acc': f'{100*test_correct/test_total:.2f}%'\n",
    "        })\n",
    "\n",
    "avg_test_loss = test_loss / len(test_loader)\n",
    "test_acc = 100 * test_correct / test_total\n",
    "\n",
    "# Print test results\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Test Results\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Test Loss:     {avg_test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.2f}%\")\n",
    "print(f\"Correct:       {test_correct}/{test_total}\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "# Detailed classification report\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(all_labels, all_predictions, \n",
    "                            target_names=['Class 0', 'Class 1']))\n",
    "\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "cm = confusion_matrix(all_labels, all_predictions)\n",
    "print(cm)\n",
    "print(f\"\\nTrue Negatives:  {cm[0,0]}\")\n",
    "print(f\"False Positives: {cm[0,1]}\")\n",
    "print(f\"False Negatives: {cm[1,0]}\")\n",
    "print(f\"True Positives:  {cm[1,1]}\")\n",
    "\n",
    "# Calculate per-class accuracy\n",
    "class_0_acc = cm[0,0] / (cm[0,0] + cm[0,1]) * 100 if (cm[0,0] + cm[0,1]) > 0 else 0\n",
    "class_1_acc = cm[1,1] / (cm[1,0] + cm[1,1]) * 100 if (cm[1,0] + cm[1,1]) > 0 else 0\n",
    "print(f\"\\nPer-Class Accuracy:\")\n",
    "print(f\"  Class 0: {class_0_acc:.2f}%\")\n",
    "print(f\"  Class 1: {class_1_acc:.2f}%\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Testing Complete!\")\n",
    "print(f\"{'='*70}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a990638",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98584fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
